{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## vanilla-DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Justin Tan\n",
    "\n",
    "Vanilla neural network. Do anything from MNIST to signal classification.\n",
    "\n",
    "Update 20/03: Added batch normalization, TensorBoard visualization\n",
    "\n",
    "Update 19/06: Added cosine annealing, exponential moving average\n",
    "\n",
    "Update 22/09: Moved input pipeline to Tensorflow dataset API - placeholder comparison w/ tfrecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available GPUs:\n",
      "['/device:GPU:0', '/device:GPU:1', '/device:GPU:2', '/device:GPU:3']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import glob, time, os\n",
    "import selu\n",
    "from diagnostics import *\n",
    "\n",
    "class config(object):\n",
    "    # Set network parameters\n",
    "    mode = 'cfd-selu'\n",
    "    channel = 'B2Xdy'\n",
    "    keep_prob = 0.7\n",
    "    num_epochs = 128\n",
    "    batch_size = 256\n",
    "    n_layers = 6\n",
    "    hidden_layer_nodes = [1024, 1024, 512, 512, 256, 256]\n",
    "    ema_decay = 0.999\n",
    "    learning_rate = 2e-5\n",
    "    cycles = 3 # Number of annealing cycles\n",
    "    n_classes = 2\n",
    "    builder = 'selu'\n",
    "\n",
    "class directories(object):\n",
    "    train = '/data/projects/punim0011/jtan/spark/spark2tf/cfd_b2dy_By_train.parquet'# '/home/jtan/gpu/jtan/spark/spark2tf/example_train.tfrecords'\n",
    "    test = '/data/projects/punim0011/jtan/spark/spark2tf/cfd_b2dy_By_test.parquet'#'/home/jtan/gpu/jtan/spark/spark2tf/example_test.tfrecords'\n",
    "    val = '/data/projects/punim0011/jtan/spark/spark2tf/cfd_b2dy_By_val.parquet'#'/home/jtan/gpu/jtan/spark/spark2tf/example_test.tfrecords'\n",
    "    tensorboard = 'tensorboard'\n",
    "    checkpoints = 'b2dy_cfd'\n",
    "\n",
    "def balanced_sample(df, uspl=True, seed=42):\n",
    "    features_l = [df[df['labels']==l].copy() for l in list(set(df['labels'].values))]\n",
    "    lsz = [f.shape[0] for f in features_l]\n",
    "    return pd.concat([f.sample(n = (min(lsz) if uspl else max(lsz)), replace = (not uspl)).copy() for f in features_l], axis=0 ).sample(frac=1, random_state=seed) \n",
    "\n",
    "architecture = '{} - {} | Layers: {} | Dropout: {} | Base LR: {} | Epochs: {}'.format(\n",
    "    config.channel, config.mode, config.n_layers, config.keep_prob, config.learning_rate, config.num_epochs)\n",
    "get_available_gpus()\n",
    "\n",
    "def load_parquet(datasetName, subsample=False, dropFrac=0.75):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    excludeFeatures = ['labels', 'mbc', 'deltae', 'daughterInvM', 'nCands', 'evtNum', 'MCtype', 'channel']\n",
    "    dataset = pq.ParquetDataset(datasetName)\n",
    "    pdf = dataset.read(nthreads=4).to_pandas()\n",
    "    pdf = pdf.drop(pdf[pdf['labels']==1].sample(frac=dropFrac).index) if subsample else pdf.sample(frac=1)\n",
    "    features = pdf.drop(excludeFeatures, axis=1)\n",
    "    labels = pdf['labels'].astype(np.int32)\n",
    "    \n",
    "    return features.values.astype(np.float32), labels.values, pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels, pdf = load_parquet(directories.train, subsample=True)\n",
    "featuresTest, labelsTest, pdf1 = load_parquet(directories.test)\n",
    "config.nTrainExamples, config.nFeatures = features.shape[0], features.shape[-1]\n",
    "config.steps_per_epoch = features.shape[0] // config.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_placeholder(features_placeholder, labels_placeholder, batchSize, numEpochs, training=True):  \n",
    "    dataset = tf.contrib.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))\n",
    "    dataset = dataset.shuffle(buffer_size=16384)\n",
    "    dataset = dataset.batch(batchSize)\n",
    "    dataset = dataset.repeat(numEpochs) if training else dataset\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dataset_train(dataDirectory, batchSize, numEpochs, nFeatures, training=True):\n",
    "    filenames = glob.glob('{}/part*'.format(dataDirectory))\n",
    "    dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "    # Extract data from `tf.Example` protocol buffer\n",
    "    def parser(record, batchSize=128):\n",
    "        keys_to_features = {\n",
    "            \"features\": tf.FixedLenFeature([nFeatures], tf.float32),\n",
    "            \"labels\": tf.FixedLenFeature((), tf.float32,\n",
    "            default_value=tf.zeros([], dtype=tf.float32)),\n",
    "        }\n",
    "        parsed = tf.parse_single_example(record, keys_to_features)\n",
    "        label = tf.cast(parsed['labels'], tf.int32)\n",
    "\n",
    "        return parsed['features'], label\n",
    "\n",
    "    # Transform into feature, label tensor pair\n",
    "    dataset = dataset.map(parser)\n",
    "    dataset = dataset.shuffle(buffer_size=16384)\n",
    "    dataset = dataset.batch(batchSize)\n",
    "    dataset = dataset.repeat(numEpochs) if training else dataset\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def dense_builder(x, shape, name, keep_prob, training=True, actv=tf.nn.relu):\n",
    "    init=tf.contrib.layers.xavier_initializer()\n",
    "    kwargs = {'center': True, 'scale': True, 'training': training, 'fused': True, 'renorm': True}\n",
    "\n",
    "    with tf.variable_scope(name, initializer=init) as scope:\n",
    "        layer = tf.layers.dense(x, units=shape[1], activation=actv)\n",
    "        bn = tf.layers.batch_normalization(layer, **kwargs)\n",
    "        layer_out = tf.layers.dropout(bn, keep_prob, training=training)\n",
    "\n",
    "    return layer_out\n",
    "\n",
    "def selu_builder(x, shape, name, keep_prob, training=True):\n",
    "    init = tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        W = tf.get_variable(\"weights\", shape = shape, initializer=init)\n",
    "        b = tf.get_variable(\"biases\", shape = [shape[1]], initializer=tf.random_normal_initializer(stddev=0.1))\n",
    "        actv = selu.selu(tf.add(tf.matmul(x, W), b))\n",
    "        layer_output = selu.dropout_selu(actv, rate=1-keep_prob, training=training)\n",
    "\n",
    "    return layer_output\n",
    "\n",
    "def dense_model(x, n_layers, hidden_layer_nodes, keep_prob, builder=selu_builder, reuse=False, training=True):\n",
    "    # Extensible dense model\n",
    "    SELU_initializer = tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')\n",
    "    init = SELU_initializer if builder==selu_builder else tf.contrib.layers.xavier_initializer()\n",
    "    assert n_layers == len(hidden_layer_nodes), 'Specified layer nodes and number of layers do not correspond.'\n",
    "    layers = [x]\n",
    "\n",
    "    with tf.variable_scope('dense_model', reuse=reuse):\n",
    "        hidden_0 = builder(x, shape=[config.nFeatures, hidden_layer_nodes[0]], name='hidden0',\n",
    "                                keep_prob = keep_prob, training=training)\n",
    "        layers.append(hidden_0)\n",
    "        for n in range(0,n_layers-1):\n",
    "            hidden_n = builder(layers[-1], shape=[hidden_layer_nodes[n], hidden_layer_nodes[n+1]], name='hidden{}'.format(n+1),\n",
    "                                keep_prob=keep_prob, training=training)\n",
    "            layers.append(hidden_n)\n",
    "\n",
    "        readout = tf.layers.dense(hidden_n, units=config.n_classes, kernel_initializer=init)\n",
    "\n",
    "    return readout\n",
    "\n",
    "def dense_SELU(x, n_layers, hidden_layer_nodes, keep_prob, reuse=False,\n",
    "    training=True, actv=selu.selu):\n",
    "    SELU_initializer = tf.contrib.layers.variance_scaling_initializer(factor=1.0, mode='FAN_IN')\n",
    "\n",
    "    with tf.variable_scope('seluNet', reuse=reuse):\n",
    "        l0 = tf.layers.dense(x, units=hidden_layer_nodes[0], activation=actv,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "        d0 = selu.dropout_selu(l0, rate=1-keep_prob, training=training)\n",
    "\n",
    "        l1 = tf.layers.dense(d0, units=hidden_layer_nodes[1], activation=actv,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "        d1 = selu.dropout_selu(l1, rate=1-keep_prob, training=training)\n",
    "\n",
    "        l2 = tf.layers.dense(d1, units=hidden_layer_nodes[2], activation=actv,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "        d2 = selu.dropout_selu(l2, rate=1-keep_prob, training=training)\n",
    "\n",
    "        l3 = tf.layers.dense(d2, units=hidden_layer_nodes[3], activation=actv,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "        d3 = selu.dropout_selu(l3, rate=1-keep_prob, training=training)\n",
    "\n",
    "        l4 = tf.layers.dense(d3, units=hidden_layer_nodes[4], activation=actv,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "        d4 = selu.dropout_selu(l4, rate=1-keep_prob, training=training)\n",
    "\n",
    "        # Readout layer\n",
    "        readout = tf.layers.dense(d4, units=config.n_classes,\n",
    "        kernel_initializer=SELU_initializer)\n",
    "\n",
    "    return readout\n",
    "\n",
    "def cosine_anneal(initial_lr, t, T, M):\n",
    "    from math import ceil\n",
    "    beta = initial_lr/2 * (np.cos(np.pi* (t % ceil(T/M))/ceil(T/M)) + 1)\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class vanillaDNN():\n",
    "    # Builds the computational graph\n",
    "    def __init__(self, config, training=True, cyclical=False):\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.handle = tf.placeholder(tf.string, shape=[])\n",
    "        self.training_phase = tf.placeholder(tf.bool)\n",
    "        self.beta = tf.placeholder(tf.float32) if cyclical else config.learning_rate\n",
    "#         self.beta = tf.train.exponential_decay(config.learning_rate, self.global_step, \n",
    "#                                                decay_steps = config.steps_per_epoch, decay_rate = config.lr_epoch_decay, staircase=True)\n",
    "        self.features_placeholder = tf.placeholder(features.dtype, features.shape)\n",
    "        self.labels_placeholder = tf.placeholder(labels.dtype, labels.shape)\n",
    "        self.featuresTest_placeholder = tf.placeholder(features.dtype, featuresTest.shape)\n",
    "        self.labelsTest_placeholder = tf.placeholder(labels.dtype, labelsTest.shape)\n",
    "\n",
    "        trainDataset = dataset_placeholder(self.features_placeholder, self.labels_placeholder, \n",
    "                                           config.batch_size, config.num_epochs, training=True)\n",
    "        testDataset = dataset_placeholder(self.featuresTest_placeholder, self.labelsTest_placeholder, \n",
    "                                          config.batch_size, config.num_epochs, training=True)\n",
    "                        \n",
    "        self.iterator = tf.contrib.data.Iterator.from_string_handle(self.handle, trainDataset.output_types, \n",
    "                                                               trainDataset.output_shapes)\n",
    "\n",
    "        self.train_iterator = trainDataset.make_initializable_iterator()\n",
    "        self.test_iterator = testDataset.make_initializable_iterator()\n",
    "\n",
    "        self.example, self.label = self.iterator.get_next()\n",
    "        # self.readout = dense_SELU(self.example, config.n_layers, [1024, 1024, 512, 512, 256], config.keep_prob, training=self.training_phase)\n",
    "        self.readout = dense_model(self.example, config.n_layers, config.hidden_layer_nodes, config.keep_prob, training=self.training_phase)\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = self.readout, labels = self.label))\n",
    "\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            # Ensures that we execute the update_ops before performing the train_step\n",
    "            self.opt_op = tf.train.AdamOptimizer(self.beta).minimize(self.cross_entropy, name = 'optimizer',\n",
    "                                                                     global_step = self.global_step)\n",
    "\n",
    "\n",
    "        self.ema = tf.train.ExponentialMovingAverage(decay = config.ema_decay, num_updates = self.global_step)\n",
    "        maintain_averages_op = self.ema.apply(tf.trainable_variables())\n",
    "        \n",
    "        with tf.control_dependencies([self.opt_op]):\n",
    "            self.train_op = tf.group(maintain_averages_op)\n",
    "\n",
    "        # Evaluation metrics\n",
    "        self.p = tf.nn.softmax(self.readout)\n",
    "        correct_prediction = tf.equal(tf.cast(tf.argmax(self.readout, 1), tf.int32), self.label)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        _, self.auc_op = tf.metrics.auc(predictions = tf.argmax(self.readout,1), labels = self.label, num_thresholds = 1024)\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        tf.summary.scalar('auc', self.auc_op)\n",
    "        tf.summary.scalar('learning_rate', self.beta)\n",
    "        tf.summary.scalar('cross_entropy', self.cross_entropy)\n",
    "        \n",
    "        self.merge_op = tf.summary.merge_all()\n",
    "        self.train_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'train_{}'.format(time.strftime('%d-%m_%I:%M'))), graph = tf.get_default_graph())\n",
    "        self.test_writer = tf.summary.FileWriter(\n",
    "            os.path.join(directories.tensorboard, 'test_{}'.format(time.strftime('%d-%m_%I:%M'))))\n",
    "\n",
    "    def predict(self, ckpt):\n",
    "        pin_cpu = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True, device_count = {'GPU':0})\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Restore the moving average version of the learned variables for eval.\n",
    "        variables_to_restore = self.ema.variables_to_restore()\n",
    "        saver = tf.train.Saver(variables_to_restore)\n",
    "        valDataset = dataset_placeholder(self.featuresTest_placeholder, self.labelsTest_placeholder, \n",
    "                                          config.batch_size, config.num_epochs, training=False)\n",
    "        val_iterator = valDataset.make_initializable_iterator()\n",
    "        concatLabels = tf.cast(self.label, tf.int32)\n",
    "        concatPreds = tf.cast(tf.argmax(self.readout,1), tf.int32)\n",
    "        concatOutput = self.p[:,1]\n",
    "\n",
    "        with tf.Session(config=pin_cpu) as sess:\n",
    "            # Initialize variables\n",
    "            init_op = tf.global_variables_initializer()\n",
    "            sess.run(init_op)\n",
    "            sess.run(tf.local_variables_initializer())\n",
    "            assert (ckpt.model_checkpoint_path), 'Missing checkpoint file!'    \n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "            val_handle = sess.run(val_iterator.string_handle())\n",
    "            labels, preds, outputs = [], [], []\n",
    "            sess.run(val_iterator.initializer, feed_dict={vDNN.featuresTest_placeholder: featuresTest,\n",
    "                                                          vDNN.labelsTest_placeholder: labelsTest})\n",
    "            while True:\n",
    "                try:\n",
    "                    l, p, o = sess.run([concatLabels, concatPreds, concatOutput], \n",
    "                                       feed_dict = {vDNN.training_phase: False, vDNN.handle: val_handle})\n",
    "                    labels.append(l), preds.append(p), outputs.append(o)\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    labels, preds, outputs = np.concatenate(labels), np.concatenate(preds), np.concatenate(outputs)\n",
    "                    break\n",
    "            acc = np.mean(np.equal(labels,preds))\n",
    "            print(\"Validation accuracy: {:.3f}\".format(acc))\n",
    "            \n",
    "            plot_ROC_curve(network_output=outputs, y_true=labels, identifier=config.mode+config.channel,\n",
    "                           meta=architecture + ' | Test accuracy: {:.3f}'.format(acc))\n",
    "            delta_t = time.time() - start_time\n",
    "            print(\"Inference complete. Duration: %g s\" %(delta_t))\n",
    "            \n",
    "            return labels, preds, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(config, restore = False):\n",
    "    # Executes training operations\n",
    "    print('Architecture: {}'.format(architecture))\n",
    "    vDNN = vanillaDNN(config, training=True)\n",
    "    start_time = time.time()\n",
    "    global_step, n_checkpoints, v_auc_best = 0, 0, 0.\n",
    "    saver = tf.train.Saver()\n",
    "    ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "\n",
    "    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)) as sess:\n",
    "        # Initialize variables\n",
    "        init_op = tf.global_variables_initializer()\n",
    "        sess.run(init_op)\n",
    "        train_handle = sess.run(vDNN.train_iterator.string_handle())\n",
    "        test_handle = sess.run(vDNN.test_iterator.string_handle())\n",
    "        \n",
    "        if restore and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            print('{} restored.'.format(ckpt.model_checkpoint_path))\n",
    "        \n",
    "        sess.run(vDNN.train_iterator.initializer, feed_dict={vDNN.features_placeholder: features,\n",
    "                                                             vDNN.labels_placeholder: labels})\n",
    "        sess.run(vDNN.test_iterator.initializer, feed_dict={vDNN.featuresTest_placeholder: featuresTest,\n",
    "                                                             vDNN.labelsTest_placeholder: labelsTest})\n",
    "        while True:\n",
    "            try:\n",
    "                # Run X steps on training dataset\n",
    "                sess.run(vDNN.train_op, feed_dict={vDNN.training_phase: True, vDNN.handle: train_handle})\n",
    "                global_step+=1\n",
    "                \n",
    "                if global_step % (config.steps_per_epoch // 5) == 0:\n",
    "                    epoch, v_auc_best = run_diagnostics(vDNN, config, directories, sess, saver, train_handle, test_handle, \n",
    "                                                        global_step, config.nTrainExamples, start_time, v_auc_best, n_checkpoints)\n",
    "                    \n",
    "            except tf.errors.OutOfRangeError:\n",
    "                break\n",
    "\n",
    "        save_path = saver.save(sess, os.path.join(directories.checkpoints, 'vDNN_{}_{}_end.ckpt'.format(config.mode, config.channel)), \n",
    "                               global_step=epoch)\n",
    "    \n",
    "    print(\"Training Complete. Model saved to file: {} Time elapsed: {:.3f} s\".format(save_path, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: B2Xdy - cfd-selu | Layers: 6 | Dropout: 0.7 | Base LR: 2e-05 | Epochs: 128\n",
      "Epoch 0, Step 600832 | Training Acc: 0.961 | Test Acc: 0.711 | Test Loss: 3.625 | Test AUC: 0.611 | Rate: 2554 examples/s (21.66 s) [*]\n",
      "Epoch 0, Step 1201664 | Training Acc: 0.898 | Test Acc: 0.770 | Test Loss: 1.814 | Test AUC: 0.737 | Rate: 4559 examples/s (41.51 s) [*]\n",
      "Epoch 0, Step 1802496 | Training Acc: 0.973 | Test Acc: 0.750 | Test Loss: 2.753 | Test AUC: 0.652 | Rate: 3809 examples/s (61.36 s) \n",
      "Epoch 0, Step 2403328 | Training Acc: 0.988 | Test Acc: 0.762 | Test Loss: 3.313 | Test AUC: 0.631 | Rate: 4279 examples/s (81.24 s) \n",
      "Epoch 0, Step 3004160 | Training Acc: 0.816 | Test Acc: 0.773 | Test Loss: 0.995 | Test AUC: 0.722 | Rate: 4657 examples/s (101.07 s) \n",
      "Epoch 1, Step 600564 | Training Acc: 0.941 | Test Acc: 0.801 | Test Loss: 2.287 | Test AUC: 0.737 | Rate: 4283 examples/s (121.00 s) [*]\n",
      "Epoch 1, Step 1201396 | Training Acc: 0.879 | Test Acc: 0.801 | Test Loss: 1.551 | Test AUC: 0.778 | Rate: 4641 examples/s (140.95 s) [*]\n",
      "Epoch 1, Step 1802228 | Training Acc: 0.941 | Test Acc: 0.844 | Test Loss: 1.634 | Test AUC: 0.761 | Rate: 4645 examples/s (160.93 s) \n",
      "Epoch 1, Step 2403060 | Training Acc: 0.977 | Test Acc: 0.805 | Test Loss: 3.360 | Test AUC: 0.718 | Rate: 4546 examples/s (180.92 s) \n",
      "Epoch 1, Step 3003892 | Training Acc: 0.715 | Test Acc: 0.758 | Test Loss: 1.353 | Test AUC: 0.718 | Rate: 4590 examples/s (200.91 s) \n",
      "Epoch 2, Step 600296 | Training Acc: 0.941 | Test Acc: 0.812 | Test Loss: 2.038 | Test AUC: 0.766 | Rate: 4225 examples/s (220.81 s) \n",
      "Epoch 2, Step 1201128 | Training Acc: 0.863 | Test Acc: 0.770 | Test Loss: 1.854 | Test AUC: 0.740 | Rate: 4588 examples/s (240.73 s) \n",
      "Epoch 2, Step 1801960 | Training Acc: 0.941 | Test Acc: 0.832 | Test Loss: 2.157 | Test AUC: 0.733 | Rate: 4554 examples/s (260.68 s) \n",
      "Epoch 2, Step 2402792 | Training Acc: 0.977 | Test Acc: 0.730 | Test Loss: 5.507 | Test AUC: 0.630 | Rate: 4194 examples/s (280.67 s) \n",
      "Epoch 2, Step 3003624 | Training Acc: 0.867 | Test Acc: 0.844 | Test Loss: 1.221 | Test AUC: 0.821 | Rate: 4291 examples/s (300.60 s) [*]\n",
      "Epoch 3, Step 600028 | Training Acc: 0.965 | Test Acc: 0.781 | Test Loss: 4.616 | Test AUC: 0.703 | Rate: 4304 examples/s (320.56 s) \n",
      "Epoch 3, Step 1200860 | Training Acc: 0.906 | Test Acc: 0.789 | Test Loss: 2.606 | Test AUC: 0.727 | Rate: 4608 examples/s (340.54 s) \n",
      "Epoch 3, Step 1801692 | Training Acc: 0.992 | Test Acc: 0.785 | Test Loss: 4.674 | Test AUC: 0.703 | Rate: 4588 examples/s (360.55 s) \n",
      "Epoch 3, Step 2402524 | Training Acc: 0.992 | Test Acc: 0.766 | Test Loss: 6.703 | Test AUC: 0.675 | Rate: 4632 examples/s (380.54 s) \n",
      "Epoch 3, Step 3003356 | Training Acc: 0.910 | Test Acc: 0.844 | Test Loss: 1.806 | Test AUC: 0.805 | Rate: 4235 examples/s (400.50 s) \n",
      "Epoch 4, Step 599760 | Training Acc: 0.961 | Test Acc: 0.820 | Test Loss: 4.781 | Test AUC: 0.693 | Rate: 4408 examples/s (420.51 s) \n",
      "Epoch 4, Step 1200592 | Training Acc: 0.934 | Test Acc: 0.852 | Test Loss: 2.449 | Test AUC: 0.794 | Rate: 4532 examples/s (440.49 s) \n",
      "Epoch 4, Step 1801424 | Training Acc: 0.984 | Test Acc: 0.738 | Test Loss: 7.343 | Test AUC: 0.642 | Rate: 4504 examples/s (460.49 s) \n",
      "Epoch 4, Step 2402256 | Training Acc: 0.996 | Test Acc: 0.672 | Test Loss: 11.056 | Test AUC: 0.637 | Rate: 4506 examples/s (480.48 s) \n",
      "Graph saved to file: b2dy_cfd/vDNN_cfd-selu_B2Xdy_epoch4.ckpt-4\n",
      "Epoch 4, Step 3003088 | Training Acc: 0.945 | Test Acc: 0.871 | Test Loss: 1.366 | Test AUC: 0.837 | Rate: 797 examples/s (500.72 s) [*]\n",
      "Epoch 5, Step 599492 | Training Acc: 0.980 | Test Acc: 0.766 | Test Loss: 5.833 | Test AUC: 0.668 | Rate: 4494 examples/s (520.70 s) \n",
      "Epoch 5, Step 1200324 | Training Acc: 0.938 | Test Acc: 0.781 | Test Loss: 3.938 | Test AUC: 0.736 | Rate: 4363 examples/s (540.68 s) \n",
      "Epoch 5, Step 1801156 | Training Acc: 0.980 | Test Acc: 0.730 | Test Loss: 7.895 | Test AUC: 0.649 | Rate: 4484 examples/s (560.70 s) \n",
      "Epoch 5, Step 2401988 | Training Acc: 0.984 | Test Acc: 0.703 | Test Loss: 12.826 | Test AUC: 0.636 | Rate: 4401 examples/s (580.71 s) \n",
      "Epoch 5, Step 3002820 | Training Acc: 0.973 | Test Acc: 0.832 | Test Loss: 2.210 | Test AUC: 0.770 | Rate: 4539 examples/s (600.72 s) \n",
      "Epoch 6, Step 599224 | Training Acc: 0.984 | Test Acc: 0.781 | Test Loss: 6.316 | Test AUC: 0.673 | Rate: 4166 examples/s (620.68 s) \n",
      "Epoch 6, Step 1200056 | Training Acc: 0.957 | Test Acc: 0.781 | Test Loss: 4.057 | Test AUC: 0.755 | Rate: 4507 examples/s (640.67 s) \n",
      "Epoch 6, Step 1800888 | Training Acc: 0.984 | Test Acc: 0.793 | Test Loss: 6.270 | Test AUC: 0.693 | Rate: 4438 examples/s (660.66 s) \n",
      "Epoch 6, Step 2401720 | Training Acc: 0.996 | Test Acc: 0.762 | Test Loss: 8.844 | Test AUC: 0.640 | Rate: 4609 examples/s (680.66 s) \n",
      "Epoch 6, Step 3002552 | Training Acc: 0.957 | Test Acc: 0.855 | Test Loss: 1.794 | Test AUC: 0.806 | Rate: 4501 examples/s (700.65 s) \n",
      "Epoch 7, Step 598956 | Training Acc: 1.000 | Test Acc: 0.766 | Test Loss: 6.039 | Test AUC: 0.668 | Rate: 4564 examples/s (720.62 s) \n",
      "Epoch 7, Step 1199788 | Training Acc: 0.945 | Test Acc: 0.785 | Test Loss: 5.269 | Test AUC: 0.741 | Rate: 4515 examples/s (740.62 s) \n",
      "Epoch 7, Step 1800620 | Training Acc: 1.000 | Test Acc: 0.828 | Test Loss: 5.286 | Test AUC: 0.752 | Rate: 4589 examples/s (760.60 s) \n",
      "Epoch 7, Step 2401452 | Training Acc: 0.996 | Test Acc: 0.766 | Test Loss: 11.525 | Test AUC: 0.628 | Rate: 4478 examples/s (780.60 s) \n",
      "Epoch 7, Step 3002284 | Training Acc: 0.965 | Test Acc: 0.852 | Test Loss: 2.011 | Test AUC: 0.772 | Rate: 4406 examples/s (800.63 s) \n",
      "Epoch 8, Step 598688 | Training Acc: 0.992 | Test Acc: 0.762 | Test Loss: 6.323 | Test AUC: 0.669 | Rate: 4449 examples/s (820.61 s) \n",
      "Epoch 8, Step 1199520 | Training Acc: 0.961 | Test Acc: 0.801 | Test Loss: 3.406 | Test AUC: 0.743 | Rate: 4268 examples/s (840.64 s) \n",
      "Epoch 8, Step 1800352 | Training Acc: 0.984 | Test Acc: 0.766 | Test Loss: 7.190 | Test AUC: 0.680 | Rate: 4565 examples/s (860.63 s) \n",
      "Epoch 8, Step 2401184 | Training Acc: 1.000 | Test Acc: 0.715 | Test Loss: 11.622 | Test AUC: 0.646 | Rate: 4463 examples/s (880.62 s) \n",
      "Epoch 8, Step 3002016 | Training Acc: 0.969 | Test Acc: 0.863 | Test Loss: 1.873 | Test AUC: 0.824 | Rate: 4445 examples/s (900.63 s) \n",
      "Epoch 9, Step 598420 | Training Acc: 0.988 | Test Acc: 0.820 | Test Loss: 6.216 | Test AUC: 0.741 | Rate: 4441 examples/s (920.65 s) \n",
      "Epoch 9, Step 1199252 | Training Acc: 0.922 | Test Acc: 0.785 | Test Loss: 3.541 | Test AUC: 0.716 | Rate: 4201 examples/s (940.64 s) \n",
      "Epoch 9, Step 1800084 | Training Acc: 0.977 | Test Acc: 0.770 | Test Loss: 6.367 | Test AUC: 0.681 | Rate: 4424 examples/s (960.65 s) \n",
      "Epoch 9, Step 2400916 | Training Acc: 1.000 | Test Acc: 0.703 | Test Loss: 11.853 | Test AUC: 0.620 | Rate: 4470 examples/s (980.68 s) \n",
      "Epoch 9, Step 3001748 | Training Acc: 0.957 | Test Acc: 0.852 | Test Loss: 2.422 | Test AUC: 0.781 | Rate: 4454 examples/s (1000.67 s) \n",
      "Epoch 10, Step 598152 | Training Acc: 0.980 | Test Acc: 0.777 | Test Loss: 7.677 | Test AUC: 0.697 | Rate: 4396 examples/s (1020.68 s) \n",
      "Epoch 10, Step 1198984 | Training Acc: 0.949 | Test Acc: 0.836 | Test Loss: 2.825 | Test AUC: 0.747 | Rate: 4474 examples/s (1040.70 s) \n",
      "Epoch 10, Step 1799816 | Training Acc: 0.977 | Test Acc: 0.754 | Test Loss: 8.612 | Test AUC: 0.672 | Rate: 4339 examples/s (1060.68 s) \n",
      "Epoch 10, Step 2400648 | Training Acc: 0.996 | Test Acc: 0.746 | Test Loss: 10.559 | Test AUC: 0.612 | Rate: 4337 examples/s (1080.69 s) \n",
      "Epoch 10, Step 3001480 | Training Acc: 0.996 | Test Acc: 0.793 | Test Loss: 2.997 | Test AUC: 0.732 | Rate: 4489 examples/s (1100.71 s) \n",
      "Epoch 11, Step 597884 | Training Acc: 0.977 | Test Acc: 0.770 | Test Loss: 9.475 | Test AUC: 0.694 | Rate: 4436 examples/s (1120.72 s) \n",
      "Epoch 11, Step 1198716 | Training Acc: 0.934 | Test Acc: 0.816 | Test Loss: 4.328 | Test AUC: 0.741 | Rate: 4128 examples/s (1140.71 s) \n",
      "Epoch 11, Step 1799548 | Training Acc: 0.977 | Test Acc: 0.754 | Test Loss: 6.783 | Test AUC: 0.648 | Rate: 4300 examples/s (1160.74 s) \n",
      "Epoch 11, Step 2400380 | Training Acc: 0.996 | Test Acc: 0.758 | Test Loss: 9.978 | Test AUC: 0.651 | Rate: 4274 examples/s (1180.76 s) \n",
      "Epoch 11, Step 3001212 | Training Acc: 0.977 | Test Acc: 0.820 | Test Loss: 3.258 | Test AUC: 0.756 | Rate: 4454 examples/s (1200.76 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Step 597616 | Training Acc: 0.977 | Test Acc: 0.746 | Test Loss: 8.387 | Test AUC: 0.668 | Rate: 4447 examples/s (1220.75 s) \n",
      "Epoch 12, Step 1198448 | Training Acc: 0.953 | Test Acc: 0.840 | Test Loss: 3.074 | Test AUC: 0.760 | Rate: 4082 examples/s (1240.78 s) \n",
      "Epoch 12, Step 1799280 | Training Acc: 0.988 | Test Acc: 0.840 | Test Loss: 4.499 | Test AUC: 0.769 | Rate: 4305 examples/s (1260.79 s) \n",
      "Epoch 12, Step 2400112 | Training Acc: 0.996 | Test Acc: 0.742 | Test Loss: 9.452 | Test AUC: 0.637 | Rate: 4428 examples/s (1280.81 s) \n",
      "Epoch 12, Step 3000944 | Training Acc: 0.988 | Test Acc: 0.805 | Test Loss: 4.084 | Test AUC: 0.751 | Rate: 4075 examples/s (1300.80 s) \n",
      "Epoch 13, Step 597348 | Training Acc: 0.973 | Test Acc: 0.785 | Test Loss: 6.202 | Test AUC: 0.718 | Rate: 4447 examples/s (1320.77 s) \n",
      "Epoch 13, Step 1198180 | Training Acc: 0.953 | Test Acc: 0.824 | Test Loss: 2.926 | Test AUC: 0.762 | Rate: 4163 examples/s (1340.77 s) \n",
      "Epoch 13, Step 1799012 | Training Acc: 0.992 | Test Acc: 0.746 | Test Loss: 5.050 | Test AUC: 0.642 | Rate: 4244 examples/s (1360.77 s) \n",
      "Epoch 13, Step 2399844 | Training Acc: 0.988 | Test Acc: 0.758 | Test Loss: 10.309 | Test AUC: 0.623 | Rate: 4296 examples/s (1380.75 s) \n",
      "Epoch 13, Step 3000676 | Training Acc: 0.965 | Test Acc: 0.844 | Test Loss: 2.534 | Test AUC: 0.751 | Rate: 4323 examples/s (1400.74 s) \n",
      "Epoch 14, Step 597080 | Training Acc: 0.965 | Test Acc: 0.785 | Test Loss: 7.123 | Test AUC: 0.688 | Rate: 4379 examples/s (1420.73 s) \n",
      "Epoch 14, Step 1197912 | Training Acc: 0.965 | Test Acc: 0.762 | Test Loss: 4.722 | Test AUC: 0.702 | Rate: 4035 examples/s (1440.74 s) \n",
      "Epoch 14, Step 1798744 | Training Acc: 0.992 | Test Acc: 0.773 | Test Loss: 7.197 | Test AUC: 0.702 | Rate: 4396 examples/s (1460.73 s) \n",
      "Epoch 14, Step 2399576 | Training Acc: 0.996 | Test Acc: 0.758 | Test Loss: 8.978 | Test AUC: 0.630 | Rate: 4345 examples/s (1480.73 s) \n",
      "Epoch 14, Step 3000408 | Training Acc: 0.973 | Test Acc: 0.824 | Test Loss: 2.931 | Test AUC: 0.732 | Rate: 4156 examples/s (1500.71 s) \n",
      "Epoch 15, Step 596812 | Training Acc: 0.945 | Test Acc: 0.781 | Test Loss: 7.068 | Test AUC: 0.701 | Rate: 4194 examples/s (1520.70 s) \n",
      "Epoch 15, Step 1197644 | Training Acc: 0.953 | Test Acc: 0.828 | Test Loss: 2.655 | Test AUC: 0.760 | Rate: 4423 examples/s (1540.73 s) \n",
      "Epoch 15, Step 1798476 | Training Acc: 0.992 | Test Acc: 0.770 | Test Loss: 6.776 | Test AUC: 0.688 | Rate: 4198 examples/s (1560.73 s) \n",
      "Epoch 15, Step 2399308 | Training Acc: 0.996 | Test Acc: 0.762 | Test Loss: 9.234 | Test AUC: 0.649 | Rate: 4243 examples/s (1580.72 s) \n",
      "Epoch 15, Step 3000140 | Training Acc: 0.961 | Test Acc: 0.824 | Test Loss: 3.310 | Test AUC: 0.770 | Rate: 4337 examples/s (1600.71 s) \n",
      "Epoch 16, Step 596544 | Training Acc: 0.922 | Test Acc: 0.773 | Test Loss: 7.612 | Test AUC: 0.692 | Rate: 3993 examples/s (1620.72 s) \n",
      "Epoch 16, Step 1197376 | Training Acc: 0.941 | Test Acc: 0.824 | Test Loss: 4.187 | Test AUC: 0.729 | Rate: 4235 examples/s (1640.72 s) \n",
      "Epoch 16, Step 1798208 | Training Acc: 1.000 | Test Acc: 0.781 | Test Loss: 6.863 | Test AUC: 0.700 | Rate: 4264 examples/s (1660.74 s) \n",
      "Epoch 16, Step 2399040 | Training Acc: 0.996 | Test Acc: 0.770 | Test Loss: 11.655 | Test AUC: 0.660 | Rate: 4107 examples/s (1680.76 s) \n",
      "Epoch 16, Step 2999872 | Training Acc: 0.980 | Test Acc: 0.836 | Test Loss: 3.307 | Test AUC: 0.771 | Rate: 3953 examples/s (1700.76 s) \n",
      "Epoch 17, Step 596276 | Training Acc: 0.891 | Test Acc: 0.789 | Test Loss: 7.066 | Test AUC: 0.708 | Rate: 4241 examples/s (1720.76 s) \n",
      "Epoch 17, Step 1197108 | Training Acc: 0.949 | Test Acc: 0.785 | Test Loss: 3.387 | Test AUC: 0.699 | Rate: 4253 examples/s (1740.78 s) \n",
      "Epoch 17, Step 1797940 | Training Acc: 0.988 | Test Acc: 0.750 | Test Loss: 6.865 | Test AUC: 0.684 | Rate: 4112 examples/s (1760.82 s) \n",
      "Epoch 17, Step 2398772 | Training Acc: 1.000 | Test Acc: 0.742 | Test Loss: 8.637 | Test AUC: 0.637 | Rate: 4142 examples/s (1780.85 s) \n",
      "Epoch 17, Step 2999604 | Training Acc: 0.984 | Test Acc: 0.840 | Test Loss: 3.900 | Test AUC: 0.781 | Rate: 4317 examples/s (1800.86 s) \n",
      "Epoch 18, Step 596008 | Training Acc: 0.891 | Test Acc: 0.801 | Test Loss: 5.858 | Test AUC: 0.688 | Rate: 4228 examples/s (1820.92 s) \n",
      "Epoch 18, Step 1196840 | Training Acc: 0.965 | Test Acc: 0.801 | Test Loss: 3.683 | Test AUC: 0.743 | Rate: 4249 examples/s (1840.93 s) \n",
      "Epoch 18, Step 1797672 | Training Acc: 0.992 | Test Acc: 0.711 | Test Loss: 9.592 | Test AUC: 0.661 | Rate: 4202 examples/s (1860.92 s) \n",
      "Epoch 18, Step 2398504 | Training Acc: 0.996 | Test Acc: 0.699 | Test Loss: 9.300 | Test AUC: 0.632 | Rate: 4210 examples/s (1880.94 s) \n",
      "Epoch 18, Step 2999336 | Training Acc: 0.988 | Test Acc: 0.793 | Test Loss: 4.275 | Test AUC: 0.757 | Rate: 4130 examples/s (1900.94 s) \n",
      "Epoch 19, Step 595740 | Training Acc: 0.828 | Test Acc: 0.773 | Test Loss: 7.630 | Test AUC: 0.684 | Rate: 4135 examples/s (1920.96 s) \n",
      "Epoch 19, Step 1196572 | Training Acc: 0.961 | Test Acc: 0.824 | Test Loss: 3.288 | Test AUC: 0.751 | Rate: 3873 examples/s (1940.95 s) \n",
      "Epoch 19, Step 1797404 | Training Acc: 0.988 | Test Acc: 0.746 | Test Loss: 8.029 | Test AUC: 0.693 | Rate: 4210 examples/s (1960.94 s) \n",
      "Epoch 19, Step 2398236 | Training Acc: 0.988 | Test Acc: 0.762 | Test Loss: 9.584 | Test AUC: 0.650 | Rate: 4229 examples/s (1980.92 s) \n",
      "Epoch 19, Step 2999068 | Training Acc: 0.984 | Test Acc: 0.801 | Test Loss: 2.100 | Test AUC: 0.698 | Rate: 4065 examples/s (2000.91 s) \n",
      "Epoch 20, Step 595472 | Training Acc: 0.840 | Test Acc: 0.738 | Test Loss: 6.833 | Test AUC: 0.693 | Rate: 4240 examples/s (2020.77 s) \n",
      "Epoch 20, Step 1196304 | Training Acc: 0.984 | Test Acc: 0.793 | Test Loss: 2.962 | Test AUC: 0.732 | Rate: 4193 examples/s (2040.65 s) \n",
      "Epoch 20, Step 1797136 | Training Acc: 0.996 | Test Acc: 0.727 | Test Loss: 8.823 | Test AUC: 0.656 | Rate: 4176 examples/s (2060.64 s) \n",
      "Epoch 20, Step 2397968 | Training Acc: 0.996 | Test Acc: 0.699 | Test Loss: 11.592 | Test AUC: 0.642 | Rate: 4066 examples/s (2080.62 s) \n",
      "Epoch 20, Step 2998800 | Training Acc: 0.988 | Test Acc: 0.809 | Test Loss: 3.429 | Test AUC: 0.724 | Rate: 4068 examples/s (2100.61 s) \n",
      "Epoch 21, Step 595204 | Training Acc: 0.816 | Test Acc: 0.785 | Test Loss: 5.222 | Test AUC: 0.682 | Rate: 4115 examples/s (2120.61 s) \n",
      "Epoch 21, Step 1196036 | Training Acc: 0.984 | Test Acc: 0.797 | Test Loss: 3.961 | Test AUC: 0.713 | Rate: 3966 examples/s (2140.63 s) \n",
      "Epoch 21, Step 1796868 | Training Acc: 0.992 | Test Acc: 0.816 | Test Loss: 5.608 | Test AUC: 0.711 | Rate: 4158 examples/s (2160.59 s) \n",
      "Epoch 21, Step 2397700 | Training Acc: 0.988 | Test Acc: 0.711 | Test Loss: 11.406 | Test AUC: 0.634 | Rate: 4115 examples/s (2180.58 s) \n",
      "Epoch 21, Step 2998532 | Training Acc: 0.996 | Test Acc: 0.805 | Test Loss: 3.955 | Test AUC: 0.731 | Rate: 4242 examples/s (2200.53 s) \n",
      "Epoch 22, Step 594936 | Training Acc: 0.789 | Test Acc: 0.793 | Test Loss: 3.895 | Test AUC: 0.703 | Rate: 3995 examples/s (2220.53 s) \n",
      "Epoch 22, Step 1195768 | Training Acc: 0.957 | Test Acc: 0.824 | Test Loss: 3.110 | Test AUC: 0.741 | Rate: 4276 examples/s (2240.53 s) \n",
      "Epoch 22, Step 1796600 | Training Acc: 0.984 | Test Acc: 0.715 | Test Loss: 8.530 | Test AUC: 0.630 | Rate: 4228 examples/s (2260.55 s) \n",
      "Epoch 22, Step 2397432 | Training Acc: 0.992 | Test Acc: 0.762 | Test Loss: 10.456 | Test AUC: 0.639 | Rate: 4133 examples/s (2280.56 s) \n",
      "Epoch 22, Step 2998264 | Training Acc: 0.988 | Test Acc: 0.816 | Test Loss: 2.736 | Test AUC: 0.748 | Rate: 4158 examples/s (2300.57 s) \n",
      "Epoch 23, Step 594668 | Training Acc: 0.719 | Test Acc: 0.809 | Test Loss: 4.886 | Test AUC: 0.695 | Rate: 4182 examples/s (2320.57 s) \n",
      "Epoch 23, Step 1195500 | Training Acc: 0.980 | Test Acc: 0.812 | Test Loss: 3.693 | Test AUC: 0.709 | Rate: 3966 examples/s (2340.59 s) \n",
      "Epoch 23, Step 1796332 | Training Acc: 1.000 | Test Acc: 0.773 | Test Loss: 6.971 | Test AUC: 0.651 | Rate: 4187 examples/s (2360.58 s) \n",
      "Epoch 23, Step 2397164 | Training Acc: 0.992 | Test Acc: 0.711 | Test Loss: 11.680 | Test AUC: 0.624 | Rate: 4190 examples/s (2380.61 s) \n",
      "Epoch 23, Step 2997996 | Training Acc: 0.980 | Test Acc: 0.824 | Test Loss: 2.802 | Test AUC: 0.778 | Rate: 4112 examples/s (2400.59 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Step 594400 | Training Acc: 0.730 | Test Acc: 0.789 | Test Loss: 3.834 | Test AUC: 0.699 | Rate: 4136 examples/s (2420.60 s) \n",
      "Epoch 24, Step 1195232 | Training Acc: 0.977 | Test Acc: 0.781 | Test Loss: 3.419 | Test AUC: 0.716 | Rate: 4155 examples/s (2440.58 s) \n",
      "Epoch 24, Step 1796064 | Training Acc: 0.992 | Test Acc: 0.781 | Test Loss: 3.484 | Test AUC: 0.690 | Rate: 4122 examples/s (2460.61 s) \n",
      "Epoch 24, Step 2396896 | Training Acc: 1.000 | Test Acc: 0.711 | Test Loss: 11.551 | Test AUC: 0.623 | Rate: 3888 examples/s (2480.61 s) \n",
      "Epoch 24, Step 2997728 | Training Acc: 0.996 | Test Acc: 0.805 | Test Loss: 3.985 | Test AUC: 0.721 | Rate: 3973 examples/s (2500.60 s) \n",
      "Epoch 25, Step 594132 | Training Acc: 0.785 | Test Acc: 0.812 | Test Loss: 4.621 | Test AUC: 0.725 | Rate: 4088 examples/s (2520.62 s) \n",
      "Epoch 25, Step 1194964 | Training Acc: 0.965 | Test Acc: 0.801 | Test Loss: 3.606 | Test AUC: 0.718 | Rate: 4100 examples/s (2540.65 s) \n",
      "Epoch 25, Step 1795796 | Training Acc: 1.000 | Test Acc: 0.809 | Test Loss: 4.346 | Test AUC: 0.690 | Rate: 4055 examples/s (2560.63 s) \n",
      "Epoch 25, Step 2396628 | Training Acc: 0.992 | Test Acc: 0.738 | Test Loss: 9.394 | Test AUC: 0.622 | Rate: 4126 examples/s (2580.65 s) \n",
      "Epoch 25, Step 2997460 | Training Acc: 0.992 | Test Acc: 0.828 | Test Loss: 3.353 | Test AUC: 0.769 | Rate: 4127 examples/s (2600.65 s) \n",
      "Epoch 26, Step 593864 | Training Acc: 0.812 | Test Acc: 0.801 | Test Loss: 3.102 | Test AUC: 0.735 | Rate: 4129 examples/s (2620.60 s) \n",
      "Epoch 26, Step 1194696 | Training Acc: 0.965 | Test Acc: 0.828 | Test Loss: 3.577 | Test AUC: 0.681 | Rate: 4148 examples/s (2640.59 s) \n",
      "Epoch 26, Step 1795528 | Training Acc: 0.977 | Test Acc: 0.746 | Test Loss: 6.732 | Test AUC: 0.652 | Rate: 4146 examples/s (2660.56 s) \n",
      "Epoch 26, Step 2396360 | Training Acc: 0.996 | Test Acc: 0.734 | Test Loss: 11.016 | Test AUC: 0.641 | Rate: 3987 examples/s (2680.56 s) \n",
      "Epoch 26, Step 2997192 | Training Acc: 0.996 | Test Acc: 0.832 | Test Loss: 3.726 | Test AUC: 0.778 | Rate: 3946 examples/s (2700.55 s) \n",
      "Epoch 27, Step 593596 | Training Acc: 0.801 | Test Acc: 0.828 | Test Loss: 3.060 | Test AUC: 0.749 | Rate: 4090 examples/s (2720.55 s) \n",
      "Epoch 27, Step 1194428 | Training Acc: 0.973 | Test Acc: 0.785 | Test Loss: 4.113 | Test AUC: 0.744 | Rate: 4159 examples/s (2740.57 s) \n",
      "Epoch 27, Step 1795260 | Training Acc: 0.996 | Test Acc: 0.766 | Test Loss: 5.188 | Test AUC: 0.702 | Rate: 4037 examples/s (2760.57 s) \n",
      "Epoch 27, Step 2396092 | Training Acc: 0.992 | Test Acc: 0.730 | Test Loss: 8.743 | Test AUC: 0.623 | Rate: 4116 examples/s (2780.61 s) \n",
      "Epoch 27, Step 2996924 | Training Acc: 0.984 | Test Acc: 0.836 | Test Loss: 3.305 | Test AUC: 0.741 | Rate: 4098 examples/s (2800.63 s) \n",
      "Epoch 28, Step 593328 | Training Acc: 0.805 | Test Acc: 0.777 | Test Loss: 3.639 | Test AUC: 0.709 | Rate: 3893 examples/s (2820.63 s) \n",
      "Epoch 28, Step 1194160 | Training Acc: 0.969 | Test Acc: 0.809 | Test Loss: 3.297 | Test AUC: 0.688 | Rate: 3760 examples/s (2840.65 s) \n",
      "Epoch 28, Step 1794992 | Training Acc: 0.992 | Test Acc: 0.758 | Test Loss: 7.031 | Test AUC: 0.672 | Rate: 4014 examples/s (2860.61 s) \n",
      "Epoch 28, Step 2395824 | Training Acc: 0.988 | Test Acc: 0.719 | Test Loss: 9.657 | Test AUC: 0.614 | Rate: 4047 examples/s (2880.60 s) \n",
      "Epoch 28, Step 2996656 | Training Acc: 0.992 | Test Acc: 0.809 | Test Loss: 3.396 | Test AUC: 0.748 | Rate: 4067 examples/s (2900.58 s) \n",
      "Epoch 29, Step 593060 | Training Acc: 0.797 | Test Acc: 0.832 | Test Loss: 3.121 | Test AUC: 0.738 | Rate: 3980 examples/s (2920.60 s) \n",
      "Epoch 29, Step 1193892 | Training Acc: 0.973 | Test Acc: 0.789 | Test Loss: 4.323 | Test AUC: 0.727 | Rate: 4053 examples/s (2940.58 s) \n",
      "Epoch 29, Step 1794724 | Training Acc: 0.988 | Test Acc: 0.766 | Test Loss: 5.940 | Test AUC: 0.687 | Rate: 3708 examples/s (2960.56 s) \n",
      "Epoch 29, Step 2395556 | Training Acc: 0.992 | Test Acc: 0.742 | Test Loss: 8.065 | Test AUC: 0.605 | Rate: 4061 examples/s (2980.53 s) \n",
      "Epoch 29, Step 2996388 | Training Acc: 0.984 | Test Acc: 0.777 | Test Loss: 4.833 | Test AUC: 0.729 | Rate: 4069 examples/s (3000.54 s) \n",
      "Epoch 30, Step 592792 | Training Acc: 0.824 | Test Acc: 0.758 | Test Loss: 3.592 | Test AUC: 0.715 | Rate: 4022 examples/s (3020.55 s) \n",
      "Epoch 30, Step 1193624 | Training Acc: 0.965 | Test Acc: 0.816 | Test Loss: 3.067 | Test AUC: 0.709 | Rate: 3842 examples/s (3040.54 s) \n",
      "Epoch 30, Step 1794456 | Training Acc: 0.992 | Test Acc: 0.793 | Test Loss: 5.363 | Test AUC: 0.681 | Rate: 4064 examples/s (3060.54 s) \n",
      "Epoch 30, Step 2395288 | Training Acc: 0.988 | Test Acc: 0.730 | Test Loss: 8.040 | Test AUC: 0.634 | Rate: 4108 examples/s (3080.52 s) \n",
      "Epoch 30, Step 2996120 | Training Acc: 0.988 | Test Acc: 0.812 | Test Loss: 2.981 | Test AUC: 0.717 | Rate: 3746 examples/s (3100.53 s) \n",
      "Epoch 31, Step 592524 | Training Acc: 0.867 | Test Acc: 0.852 | Test Loss: 1.929 | Test AUC: 0.763 | Rate: 3815 examples/s (3120.50 s) \n",
      "Epoch 31, Step 1193356 | Training Acc: 0.977 | Test Acc: 0.754 | Test Loss: 4.578 | Test AUC: 0.690 | Rate: 3940 examples/s (3140.46 s) \n",
      "Epoch 31, Step 1794188 | Training Acc: 0.980 | Test Acc: 0.750 | Test Loss: 6.358 | Test AUC: 0.639 | Rate: 4036 examples/s (3160.44 s) \n",
      "Epoch 31, Step 2395020 | Training Acc: 0.992 | Test Acc: 0.758 | Test Loss: 6.766 | Test AUC: 0.658 | Rate: 4079 examples/s (3180.44 s) \n",
      "Epoch 31, Step 2995852 | Training Acc: 0.992 | Test Acc: 0.824 | Test Loss: 3.127 | Test AUC: 0.750 | Rate: 4013 examples/s (3200.42 s) \n",
      "Epoch 32, Step 592256 | Training Acc: 0.898 | Test Acc: 0.824 | Test Loss: 2.984 | Test AUC: 0.773 | Rate: 4160 examples/s (3220.44 s) \n",
      "Epoch 32, Step 1193088 | Training Acc: 0.965 | Test Acc: 0.797 | Test Loss: 3.397 | Test AUC: 0.716 | Rate: 4021 examples/s (3240.45 s) \n",
      "Epoch 32, Step 1793920 | Training Acc: 1.000 | Test Acc: 0.785 | Test Loss: 6.203 | Test AUC: 0.708 | Rate: 4044 examples/s (3260.43 s) \n",
      "Epoch 32, Step 2394752 | Training Acc: 0.992 | Test Acc: 0.773 | Test Loss: 8.088 | Test AUC: 0.662 | Rate: 4002 examples/s (3280.45 s) \n",
      "Epoch 32, Step 2995584 | Training Acc: 0.988 | Test Acc: 0.801 | Test Loss: 3.596 | Test AUC: 0.717 | Rate: 3958 examples/s (3300.43 s) \n",
      "Epoch 33, Step 591988 | Training Acc: 0.836 | Test Acc: 0.836 | Test Loss: 1.493 | Test AUC: 0.740 | Rate: 4022 examples/s (3320.42 s) \n",
      "Epoch 33, Step 1192820 | Training Acc: 0.965 | Test Acc: 0.816 | Test Loss: 3.370 | Test AUC: 0.738 | Rate: 4118 examples/s (3340.44 s) \n",
      "Epoch 33, Step 1793652 | Training Acc: 0.996 | Test Acc: 0.809 | Test Loss: 5.104 | Test AUC: 0.715 | Rate: 3770 examples/s (3360.44 s) \n",
      "Epoch 33, Step 2394484 | Training Acc: 0.996 | Test Acc: 0.797 | Test Loss: 6.536 | Test AUC: 0.675 | Rate: 3696 examples/s (3380.45 s) \n",
      "Epoch 33, Step 2995316 | Training Acc: 0.980 | Test Acc: 0.797 | Test Loss: 5.099 | Test AUC: 0.728 | Rate: 4045 examples/s (3400.44 s) \n",
      "Epoch 34, Step 591720 | Training Acc: 0.844 | Test Acc: 0.812 | Test Loss: 2.356 | Test AUC: 0.730 | Rate: 4032 examples/s (3420.42 s) \n",
      "Epoch 34, Step 1192552 | Training Acc: 0.938 | Test Acc: 0.809 | Test Loss: 3.319 | Test AUC: 0.719 | Rate: 3888 examples/s (3440.45 s) \n",
      "Epoch 34, Step 1793384 | Training Acc: 0.996 | Test Acc: 0.750 | Test Loss: 4.507 | Test AUC: 0.672 | Rate: 4001 examples/s (3460.45 s) \n",
      "Epoch 34, Step 2394216 | Training Acc: 1.000 | Test Acc: 0.773 | Test Loss: 7.232 | Test AUC: 0.671 | Rate: 3974 examples/s (3480.45 s) \n",
      "Epoch 34, Step 2995048 | Training Acc: 1.000 | Test Acc: 0.820 | Test Loss: 2.538 | Test AUC: 0.752 | Rate: 4056 examples/s (3500.47 s) \n",
      "Epoch 35, Step 591452 | Training Acc: 0.824 | Test Acc: 0.836 | Test Loss: 1.889 | Test AUC: 0.696 | Rate: 3988 examples/s (3520.49 s) \n",
      "Epoch 35, Step 1192284 | Training Acc: 0.969 | Test Acc: 0.773 | Test Loss: 3.396 | Test AUC: 0.711 | Rate: 3649 examples/s (3540.48 s) \n",
      "Epoch 35, Step 1793116 | Training Acc: 0.977 | Test Acc: 0.738 | Test Loss: 5.800 | Test AUC: 0.669 | Rate: 3800 examples/s (3560.48 s) \n",
      "Epoch 35, Step 2393948 | Training Acc: 1.000 | Test Acc: 0.730 | Test Loss: 9.778 | Test AUC: 0.625 | Rate: 4029 examples/s (3580.47 s) \n",
      "Epoch 35, Step 2994780 | Training Acc: 0.977 | Test Acc: 0.801 | Test Loss: 3.788 | Test AUC: 0.699 | Rate: 4045 examples/s (3600.44 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Step 591184 | Training Acc: 0.863 | Test Acc: 0.789 | Test Loss: 2.353 | Test AUC: 0.719 | Rate: 4116 examples/s (3620.44 s) \n",
      "Epoch 36, Step 1192016 | Training Acc: 0.984 | Test Acc: 0.789 | Test Loss: 4.813 | Test AUC: 0.733 | Rate: 3986 examples/s (3640.43 s) \n",
      "Epoch 36, Step 1792848 | Training Acc: 0.984 | Test Acc: 0.750 | Test Loss: 6.068 | Test AUC: 0.662 | Rate: 4018 examples/s (3660.43 s) \n",
      "Epoch 36, Step 2393680 | Training Acc: 0.996 | Test Acc: 0.766 | Test Loss: 8.699 | Test AUC: 0.645 | Rate: 3976 examples/s (3680.41 s) \n",
      "Epoch 36, Step 2994512 | Training Acc: 0.980 | Test Acc: 0.785 | Test Loss: 3.962 | Test AUC: 0.718 | Rate: 3996 examples/s (3700.40 s) \n",
      "Epoch 37, Step 590916 | Training Acc: 0.895 | Test Acc: 0.832 | Test Loss: 2.367 | Test AUC: 0.761 | Rate: 3949 examples/s (3720.44 s) \n",
      "Epoch 37, Step 1191748 | Training Acc: 0.984 | Test Acc: 0.781 | Test Loss: 4.286 | Test AUC: 0.718 | Rate: 3728 examples/s (3740.45 s) \n",
      "Epoch 37, Step 1792580 | Training Acc: 0.988 | Test Acc: 0.730 | Test Loss: 7.476 | Test AUC: 0.663 | Rate: 3912 examples/s (3760.49 s) \n",
      "Epoch 37, Step 2393412 | Training Acc: 0.980 | Test Acc: 0.766 | Test Loss: 9.892 | Test AUC: 0.669 | Rate: 4014 examples/s (3780.51 s) \n",
      "Epoch 37, Step 2994244 | Training Acc: 0.984 | Test Acc: 0.820 | Test Loss: 2.786 | Test AUC: 0.736 | Rate: 4000 examples/s (3800.54 s) \n",
      "Epoch 38, Step 590648 | Training Acc: 0.879 | Test Acc: 0.816 | Test Loss: 1.741 | Test AUC: 0.753 | Rate: 3799 examples/s (3820.43 s) \n",
      "Epoch 38, Step 1191480 | Training Acc: 0.996 | Test Acc: 0.871 | Test Loss: 2.574 | Test AUC: 0.795 | Rate: 4062 examples/s (3840.30 s) \n",
      "Epoch 38, Step 1792312 | Training Acc: 0.996 | Test Acc: 0.781 | Test Loss: 5.118 | Test AUC: 0.702 | Rate: 4123 examples/s (3860.30 s) \n",
      "Epoch 38, Step 2393144 | Training Acc: 1.000 | Test Acc: 0.766 | Test Loss: 7.864 | Test AUC: 0.677 | Rate: 3989 examples/s (3880.32 s) \n",
      "Epoch 38, Step 2993976 | Training Acc: 0.977 | Test Acc: 0.777 | Test Loss: 4.460 | Test AUC: 0.729 | Rate: 3915 examples/s (3900.31 s) \n",
      "Epoch 39, Step 590380 | Training Acc: 0.926 | Test Acc: 0.797 | Test Loss: 2.073 | Test AUC: 0.751 | Rate: 3973 examples/s (3920.31 s) \n",
      "Epoch 39, Step 1191212 | Training Acc: 0.980 | Test Acc: 0.809 | Test Loss: 2.851 | Test AUC: 0.749 | Rate: 3653 examples/s (3940.31 s) \n",
      "Epoch 39, Step 1792044 | Training Acc: 0.992 | Test Acc: 0.781 | Test Loss: 6.259 | Test AUC: 0.723 | Rate: 3958 examples/s (3960.29 s) \n",
      "Epoch 39, Step 2392876 | Training Acc: 0.980 | Test Acc: 0.742 | Test Loss: 9.060 | Test AUC: 0.641 | Rate: 3999 examples/s (3980.29 s) \n",
      "Epoch 39, Step 2993708 | Training Acc: 0.992 | Test Acc: 0.812 | Test Loss: 4.733 | Test AUC: 0.736 | Rate: 3637 examples/s (4000.28 s) \n",
      "Epoch 40, Step 590112 | Training Acc: 0.898 | Test Acc: 0.785 | Test Loss: 2.320 | Test AUC: 0.730 | Rate: 3598 examples/s (4020.30 s) \n",
      "Epoch 40, Step 1190944 | Training Acc: 0.969 | Test Acc: 0.816 | Test Loss: 2.618 | Test AUC: 0.750 | Rate: 3978 examples/s (4040.31 s) \n",
      "Epoch 40, Step 1791776 | Training Acc: 1.000 | Test Acc: 0.777 | Test Loss: 4.843 | Test AUC: 0.716 | Rate: 3980 examples/s (4060.32 s) \n",
      "Epoch 40, Step 2392608 | Training Acc: 0.988 | Test Acc: 0.766 | Test Loss: 9.236 | Test AUC: 0.657 | Rate: 3948 examples/s (4080.31 s) \n",
      "Epoch 40, Step 2993440 | Training Acc: 0.988 | Test Acc: 0.809 | Test Loss: 3.585 | Test AUC: 0.764 | Rate: 3968 examples/s (4100.31 s) \n",
      "Epoch 41, Step 589844 | Training Acc: 0.902 | Test Acc: 0.836 | Test Loss: 2.128 | Test AUC: 0.784 | Rate: 3931 examples/s (4120.32 s) \n",
      "Epoch 41, Step 1190676 | Training Acc: 0.980 | Test Acc: 0.805 | Test Loss: 4.061 | Test AUC: 0.725 | Rate: 3667 examples/s (4140.35 s) \n",
      "Epoch 41, Step 1791508 | Training Acc: 1.000 | Test Acc: 0.766 | Test Loss: 5.317 | Test AUC: 0.684 | Rate: 3779 examples/s (4160.38 s) \n",
      "Epoch 41, Step 2392340 | Training Acc: 0.988 | Test Acc: 0.715 | Test Loss: 7.711 | Test AUC: 0.620 | Rate: 3835 examples/s (4180.36 s) \n",
      "Epoch 41, Step 2993172 | Training Acc: 0.988 | Test Acc: 0.785 | Test Loss: 4.095 | Test AUC: 0.733 | Rate: 3957 examples/s (4200.38 s) \n",
      "Epoch 42, Step 589576 | Training Acc: 0.879 | Test Acc: 0.852 | Test Loss: 2.014 | Test AUC: 0.774 | Rate: 3962 examples/s (4220.38 s) \n",
      "Epoch 42, Step 1190408 | Training Acc: 0.980 | Test Acc: 0.773 | Test Loss: 4.437 | Test AUC: 0.735 | Rate: 3930 examples/s (4240.38 s) \n",
      "Epoch 42, Step 1791240 | Training Acc: 1.000 | Test Acc: 0.816 | Test Loss: 3.996 | Test AUC: 0.745 | Rate: 3968 examples/s (4260.39 s) \n",
      "Epoch 42, Step 2392072 | Training Acc: 0.984 | Test Acc: 0.738 | Test Loss: 8.731 | Test AUC: 0.623 | Rate: 3595 examples/s (4280.41 s) \n",
      "Epoch 42, Step 2992904 | Training Acc: 0.988 | Test Acc: 0.750 | Test Loss: 4.177 | Test AUC: 0.699 | Rate: 3928 examples/s (4300.42 s) \n",
      "Epoch 43, Step 589308 | Training Acc: 0.922 | Test Acc: 0.797 | Test Loss: 2.370 | Test AUC: 0.759 | Rate: 3923 examples/s (4320.42 s) \n",
      "Epoch 43, Step 1190140 | Training Acc: 0.984 | Test Acc: 0.824 | Test Loss: 2.701 | Test AUC: 0.758 | Rate: 3915 examples/s (4340.42 s) \n",
      "Epoch 43, Step 1790972 | Training Acc: 0.996 | Test Acc: 0.789 | Test Loss: 4.564 | Test AUC: 0.703 | Rate: 3920 examples/s (4360.45 s) \n",
      "Epoch 43, Step 2391804 | Training Acc: 1.000 | Test Acc: 0.789 | Test Loss: 6.419 | Test AUC: 0.668 | Rate: 3960 examples/s (4380.45 s) \n",
      "Epoch 43, Step 2992636 | Training Acc: 0.988 | Test Acc: 0.785 | Test Loss: 4.879 | Test AUC: 0.710 | Rate: 3793 examples/s (4400.46 s) \n",
      "Epoch 44, Step 589040 | Training Acc: 0.891 | Test Acc: 0.824 | Test Loss: 2.220 | Test AUC: 0.751 | Rate: 3889 examples/s (4420.43 s) \n",
      "Epoch 44, Step 1189872 | Training Acc: 0.996 | Test Acc: 0.758 | Test Loss: 4.300 | Test AUC: 0.687 | Rate: 3907 examples/s (4440.44 s) \n",
      "Epoch 44, Step 1790704 | Training Acc: 0.984 | Test Acc: 0.758 | Test Loss: 6.423 | Test AUC: 0.655 | Rate: 3907 examples/s (4460.45 s) \n",
      "Epoch 44, Step 2391536 | Training Acc: 1.000 | Test Acc: 0.746 | Test Loss: 8.260 | Test AUC: 0.659 | Rate: 3849 examples/s (4480.44 s) \n",
      "Epoch 44, Step 2992368 | Training Acc: 0.984 | Test Acc: 0.844 | Test Loss: 3.060 | Test AUC: 0.743 | Rate: 3932 examples/s (4500.46 s) \n",
      "Epoch 45, Step 588772 | Training Acc: 0.914 | Test Acc: 0.844 | Test Loss: 1.357 | Test AUC: 0.796 | Rate: 4006 examples/s (4520.45 s) \n",
      "Epoch 45, Step 1189604 | Training Acc: 0.977 | Test Acc: 0.789 | Test Loss: 3.826 | Test AUC: 0.714 | Rate: 3930 examples/s (4540.45 s) \n",
      "Epoch 45, Step 1790436 | Training Acc: 1.000 | Test Acc: 0.816 | Test Loss: 4.575 | Test AUC: 0.694 | Rate: 3982 examples/s (4560.47 s) \n",
      "Epoch 45, Step 2391268 | Training Acc: 0.996 | Test Acc: 0.781 | Test Loss: 7.377 | Test AUC: 0.685 | Rate: 3915 examples/s (4580.51 s) \n",
      "Epoch 45, Step 2992100 | Training Acc: 0.980 | Test Acc: 0.828 | Test Loss: 2.646 | Test AUC: 0.744 | Rate: 3945 examples/s (4600.55 s) \n",
      "Epoch 46, Step 588504 | Training Acc: 0.914 | Test Acc: 0.840 | Test Loss: 1.423 | Test AUC: 0.753 | Rate: 3717 examples/s (4620.53 s) \n",
      "Epoch 46, Step 1189336 | Training Acc: 0.988 | Test Acc: 0.809 | Test Loss: 2.570 | Test AUC: 0.720 | Rate: 3923 examples/s (4640.51 s) \n",
      "Epoch 46, Step 1790168 | Training Acc: 0.988 | Test Acc: 0.754 | Test Loss: 6.182 | Test AUC: 0.639 | Rate: 3887 examples/s (4660.52 s) \n",
      "Epoch 46, Step 2391000 | Training Acc: 0.992 | Test Acc: 0.801 | Test Loss: 6.527 | Test AUC: 0.659 | Rate: 3832 examples/s (4680.56 s) \n",
      "Epoch 46, Step 2991832 | Training Acc: 0.988 | Test Acc: 0.762 | Test Loss: 4.853 | Test AUC: 0.714 | Rate: 3823 examples/s (4700.57 s) \n",
      "Epoch 47, Step 588236 | Training Acc: 0.902 | Test Acc: 0.809 | Test Loss: 2.120 | Test AUC: 0.753 | Rate: 3906 examples/s (4720.60 s) \n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: B2Xdy - continuum-selu | Layers: 7 | Dropout: 0.74 | Base LR: 0.0001 | Epochs: 8\n",
      "Epoch 0, Step 305152 | Training Acc: 0.875 | Test Acc: 0.910 | Test Loss: 0.740 | Test AUC: 0.896 | Rate: 2398 examples/s (12.87 s) [*]\n",
      "Epoch 0, Step 610304 | Training Acc: 0.910 | Test Acc: 0.887 | Test Loss: 1.313 | Test AUC: 0.898 | Rate: 4331 examples/s (24.09 s) [*]\n",
      "Epoch 0, Step 915456 | Training Acc: 0.906 | Test Acc: 0.898 | Test Loss: 1.213 | Test AUC: 0.902 | Rate: 3944 examples/s (35.38 s) [*]\n",
      "Epoch 0, Step 1220608 | Training Acc: 0.902 | Test Acc: 0.883 | Test Loss: 1.959 | Test AUC: 0.891 | Rate: 3594 examples/s (46.67 s) \n",
      "Epoch 0, Step 1525760 | Training Acc: 0.875 | Test Acc: 0.855 | Test Loss: 1.733 | Test AUC: 0.872 | Rate: 3466 examples/s (57.93 s) \n",
      "Epoch 1, Step 304317 | Training Acc: 0.930 | Test Acc: 0.848 | Test Loss: 2.799 | Test AUC: 0.894 | Rate: 4091 examples/s (69.21 s) \n",
      "Epoch 1, Step 609469 | Training Acc: 0.910 | Test Acc: 0.859 | Test Loss: 2.286 | Test AUC: 0.894 | Rate: 4053 examples/s (80.48 s) \n",
      "Epoch 1, Step 914621 | Training Acc: 0.949 | Test Acc: 0.895 | Test Loss: 1.732 | Test AUC: 0.929 | Rate: 3865 examples/s (91.75 s) [*]\n",
      "Epoch 1, Step 1219773 | Training Acc: 0.918 | Test Acc: 0.859 | Test Loss: 1.796 | Test AUC: 0.885 | Rate: 4090 examples/s (103.00 s) \n",
      "Epoch 1, Step 1524925 | Training Acc: 0.922 | Test Acc: 0.859 | Test Loss: 2.989 | Test AUC: 0.896 | Rate: 3907 examples/s (114.29 s) \n",
      "Epoch 2, Step 303482 | Training Acc: 0.957 | Test Acc: 0.906 | Test Loss: 1.433 | Test AUC: 0.932 | Rate: 3884 examples/s (125.60 s) [*]\n",
      "Epoch 2, Step 608634 | Training Acc: 0.906 | Test Acc: 0.918 | Test Loss: 0.981 | Test AUC: 0.917 | Rate: 4082 examples/s (136.92 s) \n",
      "Epoch 2, Step 913786 | Training Acc: 0.961 | Test Acc: 0.902 | Test Loss: 1.055 | Test AUC: 0.932 | Rate: 4052 examples/s (148.24 s) \n",
      "Epoch 2, Step 1218938 | Training Acc: 0.941 | Test Acc: 0.891 | Test Loss: 1.260 | Test AUC: 0.920 | Rate: 4081 examples/s (159.57 s) \n",
      "Epoch 2, Step 1524090 | Training Acc: 0.930 | Test Acc: 0.926 | Test Loss: 1.052 | Test AUC: 0.929 | Rate: 4131 examples/s (170.88 s) \n",
      "Epoch 3, Step 302647 | Training Acc: 0.941 | Test Acc: 0.883 | Test Loss: 2.077 | Test AUC: 0.916 | Rate: 4067 examples/s (182.19 s) \n",
      "Epoch 3, Step 607799 | Training Acc: 0.934 | Test Acc: 0.910 | Test Loss: 0.643 | Test AUC: 0.923 | Rate: 4061 examples/s (193.49 s) \n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch3.ckpt-3\n",
      "Epoch 3, Step 912951 | Training Acc: 0.938 | Test Acc: 0.938 | Test Loss: 0.681 | Test AUC: 0.938 | Rate: 679 examples/s (205.16 s) [*]\n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch3.ckpt-3\n",
      "Epoch 3, Step 1218103 | Training Acc: 0.957 | Test Acc: 0.922 | Test Loss: 1.129 | Test AUC: 0.942 | Rate: 792 examples/s (216.76 s) [*]\n",
      "Epoch 3, Step 1523255 | Training Acc: 0.930 | Test Acc: 0.910 | Test Loss: 0.724 | Test AUC: 0.921 | Rate: 3897 examples/s (228.07 s) \n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch4.ckpt-4\n",
      "Epoch 4, Step 301812 | Training Acc: 0.957 | Test Acc: 0.922 | Test Loss: 1.267 | Test AUC: 0.944 | Rate: 481 examples/s (239.85 s) [*]\n",
      "Epoch 4, Step 606964 | Training Acc: 0.926 | Test Acc: 0.926 | Test Loss: 0.785 | Test AUC: 0.928 | Rate: 4029 examples/s (251.17 s) \n",
      "Epoch 4, Step 912116 | Training Acc: 0.965 | Test Acc: 0.895 | Test Loss: 1.552 | Test AUC: 0.933 | Rate: 4067 examples/s (262.46 s) \n",
      "Epoch 4, Step 1217268 | Training Acc: 0.945 | Test Acc: 0.926 | Test Loss: 0.742 | Test AUC: 0.936 | Rate: 3991 examples/s (273.78 s) \n",
      "Epoch 4, Step 1522420 | Training Acc: 0.961 | Test Acc: 0.918 | Test Loss: 1.421 | Test AUC: 0.941 | Rate: 4110 examples/s (285.10 s) \n",
      "Epoch 5, Step 300977 | Training Acc: 0.938 | Test Acc: 0.898 | Test Loss: 1.225 | Test AUC: 0.925 | Rate: 3964 examples/s (296.44 s) \n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch5.ckpt-5\n",
      "Epoch 5, Step 606129 | Training Acc: 0.949 | Test Acc: 0.934 | Test Loss: 0.752 | Test AUC: 0.945 | Rate: 830 examples/s (308.01 s) [*]\n",
      "Epoch 5, Step 911281 | Training Acc: 0.914 | Test Acc: 0.898 | Test Loss: 0.826 | Test AUC: 0.909 | Rate: 4054 examples/s (319.33 s) \n",
      "Epoch 5, Step 1216433 | Training Acc: 0.941 | Test Acc: 0.906 | Test Loss: 1.485 | Test AUC: 0.925 | Rate: 4023 examples/s (330.62 s) \n",
      "Epoch 5, Step 1521585 | Training Acc: 0.938 | Test Acc: 0.910 | Test Loss: 1.063 | Test AUC: 0.925 | Rate: 4023 examples/s (341.85 s) \n",
      "Epoch 6, Step 300142 | Training Acc: 0.930 | Test Acc: 0.938 | Test Loss: 0.384 | Test AUC: 0.936 | Rate: 3727 examples/s (353.18 s) \n",
      "Epoch 6, Step 605294 | Training Acc: 0.938 | Test Acc: 0.891 | Test Loss: 0.906 | Test AUC: 0.919 | Rate: 4014 examples/s (364.43 s) \n",
      "Epoch 6, Step 910446 | Training Acc: 0.930 | Test Acc: 0.934 | Test Loss: 0.926 | Test AUC: 0.936 | Rate: 3545 examples/s (375.66 s) \n",
      "Epoch 6, Step 1215598 | Training Acc: 0.953 | Test Acc: 0.910 | Test Loss: 0.768 | Test AUC: 0.931 | Rate: 4071 examples/s (386.87 s) \n",
      "Epoch 6, Step 1520750 | Training Acc: 0.938 | Test Acc: 0.930 | Test Loss: 0.839 | Test AUC: 0.934 | Rate: 3998 examples/s (398.18 s) \n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch7.ckpt-7\n",
      "Epoch 7, Step 299307 | Training Acc: 0.965 | Test Acc: 0.934 | Test Loss: 0.866 | Test AUC: 0.950 | Rate: 859 examples/s (409.76 s) [*]\n",
      "Epoch 7, Step 604459 | Training Acc: 0.953 | Test Acc: 0.902 | Test Loss: 0.574 | Test AUC: 0.930 | Rate: 4038 examples/s (421.07 s) \n",
      "Epoch 7, Step 909611 | Training Acc: 0.961 | Test Acc: 0.922 | Test Loss: 0.655 | Test AUC: 0.943 | Rate: 3872 examples/s (432.37 s) \n",
      "Graph saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_epoch7.ckpt-7\n",
      "Epoch 7, Step 1214763 | Training Acc: 0.949 | Test Acc: 0.957 | Test Loss: 0.626 | Test AUC: 0.955 | Rate: 774 examples/s (443.96 s) [*]\n",
      "Training Complete. Model saved to file: checkpoints/vDNN_continuum-selu_B2Xdy_end.ckpt-7 Time elapsed: 455.391 s\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture: B2Xdy - cont-selu | Layers: 8 | Dropout: 0.75 | Base LR: 1e-05 | Epochs: 128\n",
      "Epoch 0, Step 666624 | Training Acc: 0.904 | Test Acc: 0.893 | Test Loss: 1.682 | Test AUC: 0.890 | Rate: 4292 examples/s (21.29 s) [*]\n",
      "Epoch 0, Step 1333248 | Training Acc: 0.916 | Test Acc: 0.904 | Test Loss: 1.349 | Test AUC: 0.895 | Rate: 7227 examples/s (40.36 s) [*]\n",
      "Epoch 0, Step 1999872 | Training Acc: 0.895 | Test Acc: 0.906 | Test Loss: 1.318 | Test AUC: 0.890 | Rate: 7227 examples/s (59.43 s) \n",
      "Epoch 0, Step 2666496 | Training Acc: 0.928 | Test Acc: 0.924 | Test Loss: 1.124 | Test AUC: 0.918 | Rate: 7413 examples/s (78.51 s) [*]\n",
      "Epoch 0, Step 3333120 | Training Acc: 0.920 | Test Acc: 0.922 | Test Loss: 1.052 | Test AUC: 0.907 | Rate: 7691 examples/s (97.61 s) \n",
      "Epoch 1, Step 666523 | Training Acc: 0.920 | Test Acc: 0.904 | Test Loss: 0.959 | Test AUC: 0.891 | Rate: 7446 examples/s (116.71 s) \n",
      "Epoch 1, Step 1333147 | Training Acc: 0.914 | Test Acc: 0.920 | Test Loss: 1.042 | Test AUC: 0.907 | Rate: 7472 examples/s (135.80 s) \n",
      "Epoch 1, Step 1999771 | Training Acc: 0.891 | Test Acc: 0.916 | Test Loss: 0.849 | Test AUC: 0.889 | Rate: 7535 examples/s (154.88 s) \n",
      "Epoch 1, Step 2666395 | Training Acc: 0.920 | Test Acc: 0.920 | Test Loss: 1.089 | Test AUC: 0.909 | Rate: 6580 examples/s (174.01 s) \n",
      "Epoch 1, Step 3333019 | Training Acc: 0.934 | Test Acc: 0.936 | Test Loss: 0.724 | Test AUC: 0.928 | Rate: 7544 examples/s (193.12 s) [*]\n",
      "Epoch 2, Step 666422 | Training Acc: 0.934 | Test Acc: 0.920 | Test Loss: 0.815 | Test AUC: 0.920 | Rate: 6909 examples/s (212.24 s) \n",
      "Epoch 2, Step 1333046 | Training Acc: 0.934 | Test Acc: 0.930 | Test Loss: 0.905 | Test AUC: 0.930 | Rate: 7331 examples/s (231.36 s) [*]\n",
      "Epoch 2, Step 1999670 | Training Acc: 0.932 | Test Acc: 0.930 | Test Loss: 0.633 | Test AUC: 0.938 | Rate: 7102 examples/s (250.49 s) [*]\n",
      "Epoch 2, Step 2666294 | Training Acc: 0.928 | Test Acc: 0.926 | Test Loss: 0.867 | Test AUC: 0.932 | Rate: 7158 examples/s (269.57 s) \n",
      "Epoch 2, Step 3332918 | Training Acc: 0.898 | Test Acc: 0.912 | Test Loss: 0.922 | Test AUC: 0.924 | Rate: 7468 examples/s (288.69 s) \n",
      "Epoch 3, Step 666321 | Training Acc: 0.898 | Test Acc: 0.891 | Test Loss: 1.249 | Test AUC: 0.910 | Rate: 7129 examples/s (307.81 s) \n",
      "Epoch 3, Step 1332945 | Training Acc: 0.895 | Test Acc: 0.928 | Test Loss: 0.937 | Test AUC: 0.923 | Rate: 7521 examples/s (326.90 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch3.ckpt-3\n",
      "Epoch 3, Step 1999569 | Training Acc: 0.930 | Test Acc: 0.922 | Test Loss: 1.172 | Test AUC: 0.939 | Rate: 1371 examples/s (346.33 s) [*]\n",
      "Epoch 3, Step 2666193 | Training Acc: 0.904 | Test Acc: 0.912 | Test Loss: 1.158 | Test AUC: 0.925 | Rate: 7409 examples/s (365.46 s) \n",
      "Epoch 3, Step 3332817 | Training Acc: 0.922 | Test Acc: 0.896 | Test Loss: 0.970 | Test AUC: 0.924 | Rate: 7058 examples/s (384.56 s) \n",
      "Epoch 4, Step 666220 | Training Acc: 0.930 | Test Acc: 0.900 | Test Loss: 1.166 | Test AUC: 0.933 | Rate: 7284 examples/s (403.69 s) \n",
      "Epoch 4, Step 1332844 | Training Acc: 0.887 | Test Acc: 0.930 | Test Loss: 0.905 | Test AUC: 0.927 | Rate: 7370 examples/s (422.80 s) \n",
      "Epoch 4, Step 1999468 | Training Acc: 0.924 | Test Acc: 0.906 | Test Loss: 1.130 | Test AUC: 0.934 | Rate: 7647 examples/s (441.93 s) \n",
      "Epoch 4, Step 2666092 | Training Acc: 0.889 | Test Acc: 0.891 | Test Loss: 1.742 | Test AUC: 0.915 | Rate: 6973 examples/s (461.06 s) \n",
      "Epoch 4, Step 3332716 | Training Acc: 0.914 | Test Acc: 0.896 | Test Loss: 1.229 | Test AUC: 0.928 | Rate: 7278 examples/s (480.22 s) \n",
      "Epoch 5, Step 666119 | Training Acc: 0.900 | Test Acc: 0.908 | Test Loss: 1.191 | Test AUC: 0.926 | Rate: 7217 examples/s (499.33 s) \n",
      "Epoch 5, Step 1332743 | Training Acc: 0.896 | Test Acc: 0.910 | Test Loss: 1.465 | Test AUC: 0.918 | Rate: 7173 examples/s (518.47 s) \n",
      "Epoch 5, Step 1999367 | Training Acc: 0.900 | Test Acc: 0.895 | Test Loss: 1.277 | Test AUC: 0.915 | Rate: 7511 examples/s (537.61 s) \n",
      "Epoch 5, Step 2665991 | Training Acc: 0.908 | Test Acc: 0.934 | Test Loss: 0.991 | Test AUC: 0.930 | Rate: 7269 examples/s (556.76 s) \n",
      "Epoch 5, Step 3332615 | Training Acc: 0.898 | Test Acc: 0.924 | Test Loss: 0.902 | Test AUC: 0.929 | Rate: 7367 examples/s (575.91 s) \n",
      "Epoch 6, Step 666018 | Training Acc: 0.914 | Test Acc: 0.920 | Test Loss: 1.423 | Test AUC: 0.932 | Rate: 7552 examples/s (595.04 s) \n",
      "Epoch 6, Step 1332642 | Training Acc: 0.922 | Test Acc: 0.912 | Test Loss: 1.071 | Test AUC: 0.931 | Rate: 7403 examples/s (614.16 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch6.ckpt-6\n",
      "Epoch 6, Step 1999266 | Training Acc: 0.926 | Test Acc: 0.924 | Test Loss: 1.207 | Test AUC: 0.939 | Rate: 1492 examples/s (633.58 s) [*]\n",
      "Epoch 6, Step 2665890 | Training Acc: 0.918 | Test Acc: 0.922 | Test Loss: 0.871 | Test AUC: 0.933 | Rate: 7297 examples/s (652.74 s) \n",
      "Epoch 6, Step 3332514 | Training Acc: 0.906 | Test Acc: 0.922 | Test Loss: 1.356 | Test AUC: 0.923 | Rate: 7441 examples/s (671.84 s) \n",
      "Epoch 7, Step 665917 | Training Acc: 0.918 | Test Acc: 0.924 | Test Loss: 1.457 | Test AUC: 0.936 | Rate: 7297 examples/s (690.97 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch7.ckpt-7\n",
      "Epoch 7, Step 1332541 | Training Acc: 0.930 | Test Acc: 0.926 | Test Loss: 0.969 | Test AUC: 0.940 | Rate: 1514 examples/s (710.35 s) [*]\n",
      "Epoch 7, Step 1999165 | Training Acc: 0.910 | Test Acc: 0.912 | Test Loss: 1.268 | Test AUC: 0.925 | Rate: 7304 examples/s (729.48 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch7.ckpt-7\n",
      "Epoch 7, Step 2665789 | Training Acc: 0.941 | Test Acc: 0.904 | Test Loss: 1.527 | Test AUC: 0.940 | Rate: 1356 examples/s (748.94 s) [*]\n",
      "Epoch 7, Step 3332413 | Training Acc: 0.904 | Test Acc: 0.926 | Test Loss: 1.104 | Test AUC: 0.928 | Rate: 7442 examples/s (768.08 s) \n",
      "Epoch 8, Step 665816 | Training Acc: 0.914 | Test Acc: 0.920 | Test Loss: 1.012 | Test AUC: 0.928 | Rate: 7275 examples/s (787.20 s) \n",
      "Epoch 8, Step 1332440 | Training Acc: 0.906 | Test Acc: 0.928 | Test Loss: 1.097 | Test AUC: 0.934 | Rate: 7284 examples/s (806.29 s) \n",
      "Epoch 8, Step 1999064 | Training Acc: 0.926 | Test Acc: 0.912 | Test Loss: 1.062 | Test AUC: 0.936 | Rate: 7312 examples/s (825.42 s) \n",
      "Epoch 8, Step 2665688 | Training Acc: 0.912 | Test Acc: 0.928 | Test Loss: 1.392 | Test AUC: 0.935 | Rate: 7113 examples/s (844.56 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch8.ckpt-8\n",
      "Epoch 8, Step 3332312 | Training Acc: 0.939 | Test Acc: 0.938 | Test Loss: 0.614 | Test AUC: 0.944 | Rate: 1472 examples/s (863.97 s) [*]\n",
      "Epoch 9, Step 665715 | Training Acc: 0.906 | Test Acc: 0.920 | Test Loss: 0.878 | Test AUC: 0.923 | Rate: 7079 examples/s (883.07 s) \n",
      "Epoch 9, Step 1332339 | Training Acc: 0.930 | Test Acc: 0.920 | Test Loss: 1.436 | Test AUC: 0.939 | Rate: 7207 examples/s (902.18 s) \n",
      "Epoch 9, Step 1998963 | Training Acc: 0.930 | Test Acc: 0.918 | Test Loss: 1.259 | Test AUC: 0.931 | Rate: 6892 examples/s (921.29 s) \n",
      "Epoch 9, Step 2665587 | Training Acc: 0.941 | Test Acc: 0.920 | Test Loss: 1.232 | Test AUC: 0.943 | Rate: 7267 examples/s (940.43 s) \n",
      "Epoch 9, Step 3332211 | Training Acc: 0.930 | Test Acc: 0.912 | Test Loss: 1.594 | Test AUC: 0.933 | Rate: 7397 examples/s (959.54 s) \n",
      "Epoch 10, Step 665614 | Training Acc: 0.920 | Test Acc: 0.918 | Test Loss: 1.284 | Test AUC: 0.927 | Rate: 7133 examples/s (978.64 s) \n",
      "Epoch 10, Step 1332238 | Training Acc: 0.928 | Test Acc: 0.916 | Test Loss: 1.215 | Test AUC: 0.935 | Rate: 7087 examples/s (997.77 s) \n",
      "Epoch 10, Step 1998862 | Training Acc: 0.932 | Test Acc: 0.922 | Test Loss: 0.918 | Test AUC: 0.942 | Rate: 7093 examples/s (1016.92 s) \n",
      "Epoch 10, Step 2665486 | Training Acc: 0.939 | Test Acc: 0.910 | Test Loss: 1.336 | Test AUC: 0.928 | Rate: 7383 examples/s (1036.05 s) \n",
      "Epoch 10, Step 3332110 | Training Acc: 0.922 | Test Acc: 0.943 | Test Loss: 0.771 | Test AUC: 0.942 | Rate: 7325 examples/s (1055.19 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch11.ckpt-11\n",
      "Epoch 11, Step 665513 | Training Acc: 0.941 | Test Acc: 0.934 | Test Loss: 1.030 | Test AUC: 0.950 | Rate: 1480 examples/s (1074.57 s) [*]\n",
      "Epoch 11, Step 1332137 | Training Acc: 0.930 | Test Acc: 0.938 | Test Loss: 0.824 | Test AUC: 0.944 | Rate: 6951 examples/s (1093.70 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Step 1998761 | Training Acc: 0.908 | Test Acc: 0.930 | Test Loss: 0.930 | Test AUC: 0.928 | Rate: 7164 examples/s (1112.84 s) \n",
      "Epoch 11, Step 2665385 | Training Acc: 0.920 | Test Acc: 0.920 | Test Loss: 1.109 | Test AUC: 0.927 | Rate: 7422 examples/s (1131.99 s) \n",
      "Epoch 11, Step 3332009 | Training Acc: 0.930 | Test Acc: 0.938 | Test Loss: 1.002 | Test AUC: 0.938 | Rate: 6183 examples/s (1151.11 s) \n",
      "Epoch 12, Step 665412 | Training Acc: 0.939 | Test Acc: 0.936 | Test Loss: 1.031 | Test AUC: 0.942 | Rate: 7289 examples/s (1170.20 s) \n",
      "Epoch 12, Step 1332036 | Training Acc: 0.918 | Test Acc: 0.928 | Test Loss: 0.947 | Test AUC: 0.935 | Rate: 7236 examples/s (1189.33 s) \n",
      "Epoch 12, Step 1998660 | Training Acc: 0.955 | Test Acc: 0.922 | Test Loss: 0.954 | Test AUC: 0.941 | Rate: 7135 examples/s (1208.48 s) \n",
      "Epoch 12, Step 2665284 | Training Acc: 0.934 | Test Acc: 0.889 | Test Loss: 1.990 | Test AUC: 0.922 | Rate: 7048 examples/s (1227.60 s) \n",
      "Epoch 12, Step 3331908 | Training Acc: 0.939 | Test Acc: 0.939 | Test Loss: 0.980 | Test AUC: 0.944 | Rate: 7143 examples/s (1246.73 s) \n",
      "Epoch 13, Step 665311 | Training Acc: 0.930 | Test Acc: 0.930 | Test Loss: 1.077 | Test AUC: 0.930 | Rate: 7385 examples/s (1265.84 s) \n",
      "Epoch 13, Step 1331935 | Training Acc: 0.953 | Test Acc: 0.900 | Test Loss: 1.065 | Test AUC: 0.928 | Rate: 6993 examples/s (1284.93 s) \n",
      "Epoch 13, Step 1998559 | Training Acc: 0.930 | Test Acc: 0.928 | Test Loss: 1.037 | Test AUC: 0.932 | Rate: 7107 examples/s (1304.04 s) \n",
      "Epoch 13, Step 2665183 | Training Acc: 0.914 | Test Acc: 0.934 | Test Loss: 1.019 | Test AUC: 0.930 | Rate: 6927 examples/s (1323.16 s) \n",
      "Epoch 13, Step 3331807 | Training Acc: 0.932 | Test Acc: 0.908 | Test Loss: 1.156 | Test AUC: 0.926 | Rate: 7133 examples/s (1342.28 s) \n",
      "Epoch 14, Step 665210 | Training Acc: 0.938 | Test Acc: 0.936 | Test Loss: 0.994 | Test AUC: 0.940 | Rate: 6972 examples/s (1361.37 s) \n",
      "Epoch 14, Step 1331834 | Training Acc: 0.920 | Test Acc: 0.941 | Test Loss: 0.740 | Test AUC: 0.936 | Rate: 7300 examples/s (1380.48 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch14.ckpt-14\n",
      "Epoch 14, Step 1998458 | Training Acc: 0.965 | Test Acc: 0.936 | Test Loss: 0.684 | Test AUC: 0.954 | Rate: 1388 examples/s (1399.89 s) [*]\n",
      "Epoch 14, Step 2665082 | Training Acc: 0.926 | Test Acc: 0.939 | Test Loss: 1.042 | Test AUC: 0.942 | Rate: 7093 examples/s (1419.00 s) \n",
      "Epoch 14, Step 3331706 | Training Acc: 0.932 | Test Acc: 0.926 | Test Loss: 0.962 | Test AUC: 0.937 | Rate: 6924 examples/s (1438.11 s) \n",
      "Epoch 15, Step 665109 | Training Acc: 0.932 | Test Acc: 0.945 | Test Loss: 0.873 | Test AUC: 0.938 | Rate: 6976 examples/s (1457.22 s) \n",
      "Epoch 15, Step 1331733 | Training Acc: 0.949 | Test Acc: 0.941 | Test Loss: 1.281 | Test AUC: 0.948 | Rate: 6872 examples/s (1476.34 s) \n",
      "Epoch 15, Step 1998357 | Training Acc: 0.945 | Test Acc: 0.947 | Test Loss: 0.721 | Test AUC: 0.953 | Rate: 7010 examples/s (1495.44 s) \n",
      "Epoch 15, Step 2664981 | Training Acc: 0.932 | Test Acc: 0.916 | Test Loss: 1.220 | Test AUC: 0.924 | Rate: 7058 examples/s (1514.56 s) \n",
      "Epoch 15, Step 3331605 | Training Acc: 0.932 | Test Acc: 0.936 | Test Loss: 0.962 | Test AUC: 0.936 | Rate: 7030 examples/s (1533.67 s) \n",
      "Epoch 16, Step 665008 | Training Acc: 0.941 | Test Acc: 0.938 | Test Loss: 0.984 | Test AUC: 0.939 | Rate: 7081 examples/s (1552.81 s) \n",
      "Epoch 16, Step 1331632 | Training Acc: 0.945 | Test Acc: 0.949 | Test Loss: 0.770 | Test AUC: 0.954 | Rate: 6780 examples/s (1571.89 s) \n",
      "Epoch 16, Step 1998256 | Training Acc: 0.932 | Test Acc: 0.955 | Test Loss: 0.452 | Test AUC: 0.945 | Rate: 7088 examples/s (1591.02 s) \n",
      "Epoch 16, Step 2664880 | Training Acc: 0.924 | Test Acc: 0.938 | Test Loss: 0.984 | Test AUC: 0.940 | Rate: 6984 examples/s (1610.12 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch16.ckpt-16\n",
      "Epoch 16, Step 3331504 | Training Acc: 0.959 | Test Acc: 0.953 | Test Loss: 0.659 | Test AUC: 0.957 | Rate: 1366 examples/s (1629.54 s) [*]\n",
      "Epoch 17, Step 664907 | Training Acc: 0.938 | Test Acc: 0.926 | Test Loss: 1.267 | Test AUC: 0.941 | Rate: 7161 examples/s (1648.66 s) \n",
      "Epoch 17, Step 1331531 | Training Acc: 0.947 | Test Acc: 0.936 | Test Loss: 1.066 | Test AUC: 0.940 | Rate: 7079 examples/s (1667.77 s) \n",
      "Epoch 17, Step 1998155 | Training Acc: 0.943 | Test Acc: 0.947 | Test Loss: 1.623 | Test AUC: 0.957 | Rate: 6974 examples/s (1686.89 s) \n",
      "Epoch 17, Step 2664779 | Training Acc: 0.934 | Test Acc: 0.936 | Test Loss: 0.902 | Test AUC: 0.943 | Rate: 6059 examples/s (1706.03 s) \n",
      "Epoch 17, Step 3331403 | Training Acc: 0.943 | Test Acc: 0.938 | Test Loss: 0.825 | Test AUC: 0.942 | Rate: 7194 examples/s (1725.16 s) \n",
      "Epoch 18, Step 664806 | Training Acc: 0.947 | Test Acc: 0.916 | Test Loss: 1.565 | Test AUC: 0.934 | Rate: 6935 examples/s (1744.27 s) \n",
      "Epoch 18, Step 1331430 | Training Acc: 0.955 | Test Acc: 0.928 | Test Loss: 1.578 | Test AUC: 0.937 | Rate: 6974 examples/s (1763.40 s) \n",
      "Epoch 18, Step 1998054 | Training Acc: 0.949 | Test Acc: 0.945 | Test Loss: 1.320 | Test AUC: 0.951 | Rate: 6902 examples/s (1782.44 s) \n",
      "Epoch 18, Step 2664678 | Training Acc: 0.947 | Test Acc: 0.943 | Test Loss: 0.523 | Test AUC: 0.950 | Rate: 6907 examples/s (1801.51 s) \n",
      "Epoch 18, Step 3331302 | Training Acc: 0.941 | Test Acc: 0.932 | Test Loss: 0.940 | Test AUC: 0.931 | Rate: 6977 examples/s (1820.65 s) \n",
      "Epoch 19, Step 664705 | Training Acc: 0.945 | Test Acc: 0.947 | Test Loss: 0.523 | Test AUC: 0.946 | Rate: 6708 examples/s (1839.76 s) \n",
      "Epoch 19, Step 1331329 | Training Acc: 0.934 | Test Acc: 0.930 | Test Loss: 0.782 | Test AUC: 0.936 | Rate: 6898 examples/s (1858.89 s) \n",
      "Epoch 19, Step 1997953 | Training Acc: 0.953 | Test Acc: 0.957 | Test Loss: 0.685 | Test AUC: 0.955 | Rate: 7006 examples/s (1878.00 s) \n",
      "Epoch 19, Step 2664577 | Training Acc: 0.957 | Test Acc: 0.934 | Test Loss: 1.005 | Test AUC: 0.948 | Rate: 6692 examples/s (1897.13 s) \n",
      "Epoch 19, Step 3331201 | Training Acc: 0.949 | Test Acc: 0.953 | Test Loss: 0.680 | Test AUC: 0.949 | Rate: 6638 examples/s (1916.24 s) \n",
      "Epoch 20, Step 664604 | Training Acc: 0.934 | Test Acc: 0.959 | Test Loss: 0.702 | Test AUC: 0.950 | Rate: 6916 examples/s (1935.37 s) \n",
      "Epoch 20, Step 1331228 | Training Acc: 0.924 | Test Acc: 0.930 | Test Loss: 1.116 | Test AUC: 0.936 | Rate: 6997 examples/s (1954.47 s) \n",
      "Epoch 20, Step 1997852 | Training Acc: 0.945 | Test Acc: 0.912 | Test Loss: 1.829 | Test AUC: 0.937 | Rate: 7099 examples/s (1973.61 s) \n",
      "Epoch 20, Step 2664476 | Training Acc: 0.947 | Test Acc: 0.961 | Test Loss: 0.308 | Test AUC: 0.952 | Rate: 6760 examples/s (1992.72 s) \n",
      "Epoch 20, Step 3331100 | Training Acc: 0.947 | Test Acc: 0.939 | Test Loss: 1.130 | Test AUC: 0.939 | Rate: 7052 examples/s (2011.84 s) \n",
      "Epoch 21, Step 664503 | Training Acc: 0.949 | Test Acc: 0.943 | Test Loss: 0.677 | Test AUC: 0.952 | Rate: 6946 examples/s (2030.97 s) \n",
      "Epoch 21, Step 1331127 | Training Acc: 0.934 | Test Acc: 0.936 | Test Loss: 0.545 | Test AUC: 0.932 | Rate: 6941 examples/s (2050.06 s) \n",
      "Epoch 21, Step 1997751 | Training Acc: 0.941 | Test Acc: 0.945 | Test Loss: 0.695 | Test AUC: 0.944 | Rate: 6946 examples/s (2069.18 s) \n",
      "Epoch 21, Step 2664375 | Training Acc: 0.951 | Test Acc: 0.939 | Test Loss: 0.871 | Test AUC: 0.949 | Rate: 6543 examples/s (2088.28 s) \n",
      "Epoch 21, Step 3330999 | Training Acc: 0.951 | Test Acc: 0.939 | Test Loss: 0.978 | Test AUC: 0.948 | Rate: 6972 examples/s (2107.40 s) \n",
      "Epoch 22, Step 664402 | Training Acc: 0.930 | Test Acc: 0.930 | Test Loss: 0.794 | Test AUC: 0.929 | Rate: 6604 examples/s (2126.49 s) \n",
      "Epoch 22, Step 1331026 | Training Acc: 0.939 | Test Acc: 0.922 | Test Loss: 1.024 | Test AUC: 0.935 | Rate: 6907 examples/s (2145.60 s) \n",
      "Epoch 22, Step 1997650 | Training Acc: 0.941 | Test Acc: 0.938 | Test Loss: 0.594 | Test AUC: 0.942 | Rate: 6941 examples/s (2164.70 s) \n",
      "Epoch 22, Step 2664274 | Training Acc: 0.941 | Test Acc: 0.951 | Test Loss: 0.417 | Test AUC: 0.947 | Rate: 6877 examples/s (2183.83 s) \n",
      "Epoch 22, Step 3330898 | Training Acc: 0.945 | Test Acc: 0.938 | Test Loss: 0.872 | Test AUC: 0.945 | Rate: 6789 examples/s (2202.96 s) \n",
      "Epoch 23, Step 664301 | Training Acc: 0.953 | Test Acc: 0.928 | Test Loss: 0.767 | Test AUC: 0.941 | Rate: 6881 examples/s (2222.05 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Step 1330925 | Training Acc: 0.949 | Test Acc: 0.945 | Test Loss: 0.581 | Test AUC: 0.952 | Rate: 6891 examples/s (2241.15 s) \n",
      "Epoch 23, Step 1997549 | Training Acc: 0.938 | Test Acc: 0.949 | Test Loss: 0.745 | Test AUC: 0.943 | Rate: 6819 examples/s (2260.28 s) \n",
      "Epoch 23, Step 2664173 | Training Acc: 0.938 | Test Acc: 0.938 | Test Loss: 0.707 | Test AUC: 0.937 | Rate: 6759 examples/s (2279.40 s) \n",
      "Epoch 23, Step 3330797 | Training Acc: 0.932 | Test Acc: 0.959 | Test Loss: 0.579 | Test AUC: 0.944 | Rate: 6788 examples/s (2298.51 s) \n",
      "Epoch 24, Step 664200 | Training Acc: 0.939 | Test Acc: 0.941 | Test Loss: 0.908 | Test AUC: 0.942 | Rate: 6959 examples/s (2317.62 s) \n",
      "Epoch 24, Step 1330824 | Training Acc: 0.959 | Test Acc: 0.963 | Test Loss: 0.288 | Test AUC: 0.954 | Rate: 6811 examples/s (2336.75 s) \n",
      "Epoch 24, Step 1997448 | Training Acc: 0.939 | Test Acc: 0.949 | Test Loss: 1.005 | Test AUC: 0.940 | Rate: 6327 examples/s (2355.88 s) \n",
      "Epoch 24, Step 2664072 | Training Acc: 0.951 | Test Acc: 0.939 | Test Loss: 1.009 | Test AUC: 0.944 | Rate: 5807 examples/s (2375.01 s) \n",
      "Epoch 24, Step 3330696 | Training Acc: 0.945 | Test Acc: 0.949 | Test Loss: 0.743 | Test AUC: 0.950 | Rate: 7003 examples/s (2394.14 s) \n",
      "Epoch 25, Step 664099 | Training Acc: 0.943 | Test Acc: 0.943 | Test Loss: 0.870 | Test AUC: 0.942 | Rate: 6957 examples/s (2413.25 s) \n",
      "Epoch 25, Step 1330723 | Training Acc: 0.959 | Test Acc: 0.934 | Test Loss: 0.525 | Test AUC: 0.952 | Rate: 6965 examples/s (2432.37 s) \n",
      "Epoch 25, Step 1997347 | Training Acc: 0.936 | Test Acc: 0.949 | Test Loss: 1.071 | Test AUC: 0.943 | Rate: 6711 examples/s (2451.49 s) \n",
      "Epoch 25, Step 2663971 | Training Acc: 0.951 | Test Acc: 0.953 | Test Loss: 0.381 | Test AUC: 0.951 | Rate: 6950 examples/s (2470.60 s) \n",
      "Epoch 25, Step 3330595 | Training Acc: 0.949 | Test Acc: 0.951 | Test Loss: 0.720 | Test AUC: 0.952 | Rate: 7162 examples/s (2489.72 s) \n",
      "Epoch 26, Step 663998 | Training Acc: 0.965 | Test Acc: 0.947 | Test Loss: 0.818 | Test AUC: 0.955 | Rate: 6851 examples/s (2508.84 s) \n",
      "Epoch 26, Step 1330622 | Training Acc: 0.955 | Test Acc: 0.951 | Test Loss: 0.428 | Test AUC: 0.955 | Rate: 6828 examples/s (2527.96 s) \n",
      "Epoch 26, Step 1997246 | Training Acc: 0.941 | Test Acc: 0.939 | Test Loss: 0.886 | Test AUC: 0.943 | Rate: 6834 examples/s (2547.10 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch26.ckpt-26\n",
      "Epoch 26, Step 2663870 | Training Acc: 0.955 | Test Acc: 0.969 | Test Loss: 0.224 | Test AUC: 0.962 | Rate: 1381 examples/s (2566.54 s) [*]\n",
      "Epoch 26, Step 3330494 | Training Acc: 0.951 | Test Acc: 0.941 | Test Loss: 0.879 | Test AUC: 0.936 | Rate: 6577 examples/s (2585.65 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch27.ckpt-27\n",
      "Epoch 27, Step 663897 | Training Acc: 0.977 | Test Acc: 0.959 | Test Loss: 0.552 | Test AUC: 0.968 | Rate: 1435 examples/s (2605.03 s) [*]\n",
      "Epoch 27, Step 1330521 | Training Acc: 0.951 | Test Acc: 0.947 | Test Loss: 0.930 | Test AUC: 0.947 | Rate: 6761 examples/s (2624.14 s) \n",
      "Epoch 27, Step 1997145 | Training Acc: 0.949 | Test Acc: 0.934 | Test Loss: 0.695 | Test AUC: 0.939 | Rate: 6868 examples/s (2643.27 s) \n",
      "Epoch 27, Step 2663769 | Training Acc: 0.947 | Test Acc: 0.945 | Test Loss: 0.617 | Test AUC: 0.949 | Rate: 6733 examples/s (2662.37 s) \n",
      "Epoch 27, Step 3330393 | Training Acc: 0.936 | Test Acc: 0.965 | Test Loss: 0.280 | Test AUC: 0.948 | Rate: 6551 examples/s (2681.51 s) \n",
      "Epoch 28, Step 663796 | Training Acc: 0.930 | Test Acc: 0.939 | Test Loss: 0.779 | Test AUC: 0.934 | Rate: 6643 examples/s (2700.60 s) \n",
      "Epoch 28, Step 1330420 | Training Acc: 0.955 | Test Acc: 0.947 | Test Loss: 0.861 | Test AUC: 0.953 | Rate: 6816 examples/s (2719.70 s) \n",
      "Epoch 28, Step 1997044 | Training Acc: 0.928 | Test Acc: 0.939 | Test Loss: 0.583 | Test AUC: 0.932 | Rate: 7028 examples/s (2738.81 s) \n",
      "Epoch 28, Step 2663668 | Training Acc: 0.967 | Test Acc: 0.961 | Test Loss: 0.255 | Test AUC: 0.963 | Rate: 6957 examples/s (2757.91 s) \n",
      "Epoch 28, Step 3330292 | Training Acc: 0.965 | Test Acc: 0.939 | Test Loss: 0.699 | Test AUC: 0.953 | Rate: 6398 examples/s (2777.01 s) \n",
      "Epoch 29, Step 663695 | Training Acc: 0.957 | Test Acc: 0.945 | Test Loss: 0.468 | Test AUC: 0.952 | Rate: 6636 examples/s (2796.13 s) \n",
      "Epoch 29, Step 1330319 | Training Acc: 0.947 | Test Acc: 0.941 | Test Loss: 0.726 | Test AUC: 0.939 | Rate: 6805 examples/s (2815.21 s) \n",
      "Epoch 29, Step 1996943 | Training Acc: 0.945 | Test Acc: 0.938 | Test Loss: 0.678 | Test AUC: 0.940 | Rate: 6707 examples/s (2834.32 s) \n",
      "Epoch 29, Step 2663567 | Training Acc: 0.951 | Test Acc: 0.953 | Test Loss: 0.365 | Test AUC: 0.948 | Rate: 6749 examples/s (2853.46 s) \n",
      "Epoch 29, Step 3330191 | Training Acc: 0.949 | Test Acc: 0.945 | Test Loss: 0.961 | Test AUC: 0.940 | Rate: 6603 examples/s (2872.60 s) \n",
      "Epoch 30, Step 663594 | Training Acc: 0.959 | Test Acc: 0.953 | Test Loss: 1.054 | Test AUC: 0.956 | Rate: 6807 examples/s (2891.69 s) \n",
      "Epoch 30, Step 1330218 | Training Acc: 0.938 | Test Acc: 0.943 | Test Loss: 0.601 | Test AUC: 0.935 | Rate: 6977 examples/s (2910.80 s) \n",
      "Epoch 30, Step 1996842 | Training Acc: 0.959 | Test Acc: 0.959 | Test Loss: 0.600 | Test AUC: 0.960 | Rate: 6796 examples/s (2929.91 s) \n",
      "Epoch 30, Step 2663466 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.457 | Test AUC: 0.950 | Rate: 6695 examples/s (2949.02 s) \n",
      "Epoch 30, Step 3330090 | Training Acc: 0.959 | Test Acc: 0.941 | Test Loss: 0.893 | Test AUC: 0.947 | Rate: 6691 examples/s (2968.15 s) \n",
      "Epoch 31, Step 663493 | Training Acc: 0.971 | Test Acc: 0.947 | Test Loss: 0.731 | Test AUC: 0.958 | Rate: 6727 examples/s (2987.25 s) \n",
      "Epoch 31, Step 1330117 | Training Acc: 0.949 | Test Acc: 0.928 | Test Loss: 0.763 | Test AUC: 0.931 | Rate: 7035 examples/s (3006.36 s) \n",
      "Epoch 31, Step 1996741 | Training Acc: 0.941 | Test Acc: 0.959 | Test Loss: 0.519 | Test AUC: 0.947 | Rate: 6420 examples/s (3025.50 s) \n",
      "Epoch 31, Step 2663365 | Training Acc: 0.953 | Test Acc: 0.951 | Test Loss: 0.605 | Test AUC: 0.947 | Rate: 6821 examples/s (3044.62 s) \n",
      "Epoch 31, Step 3329989 | Training Acc: 0.955 | Test Acc: 0.959 | Test Loss: 0.406 | Test AUC: 0.951 | Rate: 6790 examples/s (3063.72 s) \n",
      "Epoch 32, Step 663392 | Training Acc: 0.947 | Test Acc: 0.959 | Test Loss: 0.516 | Test AUC: 0.948 | Rate: 6842 examples/s (3082.83 s) \n",
      "Epoch 32, Step 1330016 | Training Acc: 0.965 | Test Acc: 0.973 | Test Loss: 0.173 | Test AUC: 0.967 | Rate: 6653 examples/s (3101.95 s) \n",
      "Epoch 32, Step 1996640 | Training Acc: 0.932 | Test Acc: 0.959 | Test Loss: 0.357 | Test AUC: 0.944 | Rate: 6706 examples/s (3121.08 s) \n",
      "Epoch 32, Step 2663264 | Training Acc: 0.941 | Test Acc: 0.941 | Test Loss: 0.999 | Test AUC: 0.941 | Rate: 6735 examples/s (3140.20 s) \n",
      "Epoch 32, Step 3329888 | Training Acc: 0.941 | Test Acc: 0.951 | Test Loss: 0.598 | Test AUC: 0.946 | Rate: 6938 examples/s (3159.33 s) \n",
      "Epoch 33, Step 663291 | Training Acc: 0.953 | Test Acc: 0.961 | Test Loss: 0.469 | Test AUC: 0.956 | Rate: 6561 examples/s (3178.43 s) \n",
      "Epoch 33, Step 1329915 | Training Acc: 0.963 | Test Acc: 0.955 | Test Loss: 0.444 | Test AUC: 0.955 | Rate: 6627 examples/s (3197.54 s) \n",
      "Epoch 33, Step 1996539 | Training Acc: 0.947 | Test Acc: 0.959 | Test Loss: 0.251 | Test AUC: 0.952 | Rate: 6702 examples/s (3216.66 s) \n",
      "Epoch 33, Step 2663163 | Training Acc: 0.957 | Test Acc: 0.955 | Test Loss: 0.553 | Test AUC: 0.958 | Rate: 6859 examples/s (3235.78 s) \n",
      "Epoch 33, Step 3329787 | Training Acc: 0.961 | Test Acc: 0.963 | Test Loss: 0.635 | Test AUC: 0.960 | Rate: 6806 examples/s (3254.91 s) \n",
      "Epoch 34, Step 663190 | Training Acc: 0.967 | Test Acc: 0.953 | Test Loss: 0.604 | Test AUC: 0.959 | Rate: 6793 examples/s (3274.01 s) \n",
      "Epoch 34, Step 1329814 | Training Acc: 0.949 | Test Acc: 0.955 | Test Loss: 0.730 | Test AUC: 0.946 | Rate: 6766 examples/s (3293.14 s) \n",
      "Epoch 34, Step 1996438 | Training Acc: 0.939 | Test Acc: 0.955 | Test Loss: 0.571 | Test AUC: 0.942 | Rate: 6970 examples/s (3312.23 s) \n",
      "Epoch 34, Step 2663062 | Training Acc: 0.963 | Test Acc: 0.947 | Test Loss: 0.771 | Test AUC: 0.953 | Rate: 6390 examples/s (3331.35 s) \n",
      "Epoch 34, Step 3329686 | Training Acc: 0.961 | Test Acc: 0.953 | Test Loss: 0.427 | Test AUC: 0.954 | Rate: 6721 examples/s (3350.46 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Step 663089 | Training Acc: 0.949 | Test Acc: 0.947 | Test Loss: 0.733 | Test AUC: 0.947 | Rate: 6750 examples/s (3369.60 s) \n",
      "Epoch 35, Step 1329713 | Training Acc: 0.938 | Test Acc: 0.953 | Test Loss: 0.622 | Test AUC: 0.941 | Rate: 6721 examples/s (3388.72 s) \n",
      "Epoch 35, Step 1996337 | Training Acc: 0.945 | Test Acc: 0.941 | Test Loss: 0.599 | Test AUC: 0.941 | Rate: 6697 examples/s (3407.81 s) \n",
      "Epoch 35, Step 2662961 | Training Acc: 0.934 | Test Acc: 0.949 | Test Loss: 0.713 | Test AUC: 0.940 | Rate: 6676 examples/s (3426.92 s) \n",
      "Epoch 35, Step 3329585 | Training Acc: 0.947 | Test Acc: 0.945 | Test Loss: 0.509 | Test AUC: 0.936 | Rate: 6516 examples/s (3446.05 s) \n",
      "Epoch 36, Step 662988 | Training Acc: 0.949 | Test Acc: 0.967 | Test Loss: 0.261 | Test AUC: 0.953 | Rate: 6709 examples/s (3465.15 s) \n",
      "Epoch 36, Step 1329612 | Training Acc: 0.957 | Test Acc: 0.961 | Test Loss: 0.429 | Test AUC: 0.954 | Rate: 6479 examples/s (3484.25 s) \n",
      "Epoch 36, Step 1996236 | Training Acc: 0.939 | Test Acc: 0.941 | Test Loss: 0.707 | Test AUC: 0.938 | Rate: 6702 examples/s (3503.37 s) \n",
      "Epoch 36, Step 2662860 | Training Acc: 0.941 | Test Acc: 0.930 | Test Loss: 1.042 | Test AUC: 0.933 | Rate: 6713 examples/s (3522.49 s) \n",
      "Epoch 36, Step 3329484 | Training Acc: 0.955 | Test Acc: 0.969 | Test Loss: 0.261 | Test AUC: 0.960 | Rate: 6698 examples/s (3541.60 s) \n",
      "Epoch 37, Step 662887 | Training Acc: 0.957 | Test Acc: 0.959 | Test Loss: 0.439 | Test AUC: 0.953 | Rate: 6421 examples/s (3560.71 s) \n",
      "Epoch 37, Step 1329511 | Training Acc: 0.930 | Test Acc: 0.934 | Test Loss: 0.743 | Test AUC: 0.931 | Rate: 6541 examples/s (3579.72 s) \n",
      "Epoch 37, Step 1996135 | Training Acc: 0.951 | Test Acc: 0.938 | Test Loss: 0.724 | Test AUC: 0.934 | Rate: 6759 examples/s (3598.76 s) \n",
      "Epoch 37, Step 2662759 | Training Acc: 0.965 | Test Acc: 0.949 | Test Loss: 0.533 | Test AUC: 0.954 | Rate: 6401 examples/s (3617.89 s) \n",
      "Epoch 37, Step 3329383 | Training Acc: 0.951 | Test Acc: 0.953 | Test Loss: 0.654 | Test AUC: 0.948 | Rate: 6592 examples/s (3637.00 s) \n",
      "Epoch 38, Step 662786 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.387 | Test AUC: 0.943 | Rate: 6800 examples/s (3656.14 s) \n",
      "Epoch 38, Step 1329410 | Training Acc: 0.967 | Test Acc: 0.961 | Test Loss: 0.626 | Test AUC: 0.961 | Rate: 6616 examples/s (3675.27 s) \n",
      "Epoch 38, Step 1996034 | Training Acc: 0.945 | Test Acc: 0.959 | Test Loss: 0.331 | Test AUC: 0.942 | Rate: 6737 examples/s (3694.39 s) \n",
      "Epoch 38, Step 2662658 | Training Acc: 0.965 | Test Acc: 0.949 | Test Loss: 0.726 | Test AUC: 0.955 | Rate: 6650 examples/s (3713.50 s) \n",
      "Epoch 38, Step 3329282 | Training Acc: 0.945 | Test Acc: 0.951 | Test Loss: 0.691 | Test AUC: 0.940 | Rate: 6575 examples/s (3732.62 s) \n",
      "Epoch 39, Step 662685 | Training Acc: 0.959 | Test Acc: 0.973 | Test Loss: 0.426 | Test AUC: 0.968 | Rate: 6622 examples/s (3751.72 s) \n",
      "Epoch 39, Step 1329309 | Training Acc: 0.949 | Test Acc: 0.967 | Test Loss: 0.349 | Test AUC: 0.952 | Rate: 6666 examples/s (3770.82 s) \n",
      "Epoch 39, Step 1995933 | Training Acc: 0.939 | Test Acc: 0.939 | Test Loss: 0.559 | Test AUC: 0.932 | Rate: 6558 examples/s (3789.92 s) \n",
      "Epoch 39, Step 2662557 | Training Acc: 0.934 | Test Acc: 0.947 | Test Loss: 0.650 | Test AUC: 0.939 | Rate: 6511 examples/s (3809.02 s) \n",
      "Epoch 39, Step 3329181 | Training Acc: 0.973 | Test Acc: 0.945 | Test Loss: 0.474 | Test AUC: 0.956 | Rate: 6585 examples/s (3828.15 s) \n",
      "Epoch 40, Step 662584 | Training Acc: 0.963 | Test Acc: 0.949 | Test Loss: 0.621 | Test AUC: 0.957 | Rate: 6581 examples/s (3847.27 s) \n",
      "Epoch 40, Step 1329208 | Training Acc: 0.949 | Test Acc: 0.939 | Test Loss: 0.798 | Test AUC: 0.940 | Rate: 6593 examples/s (3866.41 s) \n",
      "Epoch 40, Step 1995832 | Training Acc: 0.955 | Test Acc: 0.955 | Test Loss: 0.565 | Test AUC: 0.954 | Rate: 6474 examples/s (3885.55 s) \n",
      "Epoch 40, Step 2662456 | Training Acc: 0.965 | Test Acc: 0.959 | Test Loss: 0.298 | Test AUC: 0.952 | Rate: 6407 examples/s (3904.66 s) \n",
      "Epoch 40, Step 3329080 | Training Acc: 0.961 | Test Acc: 0.939 | Test Loss: 0.662 | Test AUC: 0.948 | Rate: 6497 examples/s (3923.79 s) \n",
      "Epoch 41, Step 662483 | Training Acc: 0.953 | Test Acc: 0.955 | Test Loss: 0.750 | Test AUC: 0.947 | Rate: 6374 examples/s (3942.93 s) \n",
      "Epoch 41, Step 1329107 | Training Acc: 0.938 | Test Acc: 0.951 | Test Loss: 0.628 | Test AUC: 0.944 | Rate: 6686 examples/s (3962.06 s) \n",
      "Epoch 41, Step 1995731 | Training Acc: 0.949 | Test Acc: 0.957 | Test Loss: 0.356 | Test AUC: 0.958 | Rate: 6741 examples/s (3981.18 s) \n",
      "Epoch 41, Step 2662355 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.344 | Test AUC: 0.947 | Rate: 6689 examples/s (4000.29 s) \n",
      "Epoch 41, Step 3328979 | Training Acc: 0.971 | Test Acc: 0.967 | Test Loss: 0.666 | Test AUC: 0.967 | Rate: 6523 examples/s (4019.43 s) \n",
      "Epoch 42, Step 662382 | Training Acc: 0.959 | Test Acc: 0.938 | Test Loss: 0.459 | Test AUC: 0.942 | Rate: 6374 examples/s (4038.57 s) \n",
      "Epoch 42, Step 1329006 | Training Acc: 0.965 | Test Acc: 0.973 | Test Loss: 0.148 | Test AUC: 0.965 | Rate: 6565 examples/s (4057.69 s) \n",
      "Epoch 42, Step 1995630 | Training Acc: 0.945 | Test Acc: 0.959 | Test Loss: 0.465 | Test AUC: 0.950 | Rate: 6684 examples/s (4076.81 s) \n",
      "Epoch 42, Step 2662254 | Training Acc: 0.938 | Test Acc: 0.949 | Test Loss: 0.328 | Test AUC: 0.936 | Rate: 6517 examples/s (4095.94 s) \n",
      "Epoch 42, Step 3328878 | Training Acc: 0.934 | Test Acc: 0.951 | Test Loss: 0.582 | Test AUC: 0.937 | Rate: 6812 examples/s (4115.07 s) \n",
      "Epoch 43, Step 662281 | Training Acc: 0.955 | Test Acc: 0.963 | Test Loss: 0.580 | Test AUC: 0.955 | Rate: 6606 examples/s (4134.20 s) \n",
      "Epoch 43, Step 1328905 | Training Acc: 0.949 | Test Acc: 0.963 | Test Loss: 0.445 | Test AUC: 0.951 | Rate: 6475 examples/s (4153.34 s) \n",
      "Epoch 43, Step 1995529 | Training Acc: 0.965 | Test Acc: 0.947 | Test Loss: 0.494 | Test AUC: 0.955 | Rate: 6460 examples/s (4172.47 s) \n",
      "Epoch 43, Step 2662153 | Training Acc: 0.957 | Test Acc: 0.965 | Test Loss: 0.375 | Test AUC: 0.957 | Rate: 6381 examples/s (4191.58 s) \n",
      "Epoch 43, Step 3328777 | Training Acc: 0.945 | Test Acc: 0.959 | Test Loss: 0.446 | Test AUC: 0.949 | Rate: 6213 examples/s (4210.72 s) \n",
      "Epoch 44, Step 662180 | Training Acc: 0.957 | Test Acc: 0.959 | Test Loss: 0.719 | Test AUC: 0.951 | Rate: 6534 examples/s (4229.82 s) \n",
      "Epoch 44, Step 1328804 | Training Acc: 0.947 | Test Acc: 0.953 | Test Loss: 0.493 | Test AUC: 0.940 | Rate: 6513 examples/s (4248.92 s) \n",
      "Epoch 44, Step 1995428 | Training Acc: 0.947 | Test Acc: 0.953 | Test Loss: 0.357 | Test AUC: 0.945 | Rate: 6477 examples/s (4268.06 s) \n",
      "Epoch 44, Step 2662052 | Training Acc: 0.955 | Test Acc: 0.945 | Test Loss: 0.421 | Test AUC: 0.939 | Rate: 6562 examples/s (4287.18 s) \n",
      "Epoch 44, Step 3328676 | Training Acc: 0.973 | Test Acc: 0.955 | Test Loss: 0.455 | Test AUC: 0.956 | Rate: 6450 examples/s (4306.29 s) \n",
      "Epoch 45, Step 662079 | Training Acc: 0.941 | Test Acc: 0.955 | Test Loss: 0.284 | Test AUC: 0.945 | Rate: 6198 examples/s (4325.41 s) \n",
      "Epoch 45, Step 1328703 | Training Acc: 0.949 | Test Acc: 0.943 | Test Loss: 0.563 | Test AUC: 0.947 | Rate: 6294 examples/s (4344.53 s) \n",
      "Epoch 45, Step 1995327 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.376 | Test AUC: 0.937 | Rate: 6528 examples/s (4363.64 s) \n",
      "Epoch 45, Step 2661951 | Training Acc: 0.949 | Test Acc: 0.955 | Test Loss: 0.470 | Test AUC: 0.947 | Rate: 6260 examples/s (4382.76 s) \n",
      "Epoch 45, Step 3328575 | Training Acc: 0.945 | Test Acc: 0.938 | Test Loss: 0.369 | Test AUC: 0.941 | Rate: 6230 examples/s (4401.89 s) \n",
      "Epoch 46, Step 661978 | Training Acc: 0.969 | Test Acc: 0.945 | Test Loss: 0.377 | Test AUC: 0.946 | Rate: 6517 examples/s (4421.01 s) \n",
      "Epoch 46, Step 1328602 | Training Acc: 0.967 | Test Acc: 0.961 | Test Loss: 0.343 | Test AUC: 0.958 | Rate: 6651 examples/s (4440.11 s) \n",
      "Epoch 46, Step 1995226 | Training Acc: 0.953 | Test Acc: 0.973 | Test Loss: 0.231 | Test AUC: 0.959 | Rate: 6236 examples/s (4459.24 s) \n",
      "Epoch 46, Step 2661850 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.680 | Test AUC: 0.955 | Rate: 6495 examples/s (4478.37 s) \n",
      "Epoch 46, Step 3328474 | Training Acc: 0.951 | Test Acc: 0.949 | Test Loss: 0.526 | Test AUC: 0.941 | Rate: 6671 examples/s (4497.47 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Step 661877 | Training Acc: 0.957 | Test Acc: 0.963 | Test Loss: 0.236 | Test AUC: 0.957 | Rate: 6396 examples/s (4516.58 s) \n",
      "Epoch 47, Step 1328501 | Training Acc: 0.955 | Test Acc: 0.963 | Test Loss: 0.463 | Test AUC: 0.953 | Rate: 6465 examples/s (4535.70 s) \n",
      "Epoch 47, Step 1995125 | Training Acc: 0.961 | Test Acc: 0.951 | Test Loss: 0.341 | Test AUC: 0.948 | Rate: 6635 examples/s (4554.82 s) \n",
      "Epoch 47, Step 2661749 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.462 | Test AUC: 0.949 | Rate: 6537 examples/s (4573.96 s) \n",
      "Epoch 47, Step 3328373 | Training Acc: 0.955 | Test Acc: 0.971 | Test Loss: 0.240 | Test AUC: 0.958 | Rate: 6465 examples/s (4593.11 s) \n",
      "Epoch 48, Step 661776 | Training Acc: 0.959 | Test Acc: 0.941 | Test Loss: 0.785 | Test AUC: 0.942 | Rate: 6475 examples/s (4612.20 s) \n",
      "Epoch 48, Step 1328400 | Training Acc: 0.967 | Test Acc: 0.977 | Test Loss: 0.137 | Test AUC: 0.967 | Rate: 6557 examples/s (4631.35 s) \n",
      "Epoch 48, Step 1995024 | Training Acc: 0.951 | Test Acc: 0.969 | Test Loss: 0.167 | Test AUC: 0.951 | Rate: 6457 examples/s (4650.45 s) \n",
      "Epoch 48, Step 2661648 | Training Acc: 0.967 | Test Acc: 0.963 | Test Loss: 0.590 | Test AUC: 0.958 | Rate: 6465 examples/s (4669.57 s) \n",
      "Epoch 48, Step 3328272 | Training Acc: 0.965 | Test Acc: 0.947 | Test Loss: 0.681 | Test AUC: 0.952 | Rate: 6437 examples/s (4688.68 s) \n",
      "Epoch 49, Step 661675 | Training Acc: 0.959 | Test Acc: 0.953 | Test Loss: 0.339 | Test AUC: 0.953 | Rate: 6508 examples/s (4707.82 s) \n",
      "Epoch 49, Step 1328299 | Training Acc: 0.957 | Test Acc: 0.963 | Test Loss: 0.390 | Test AUC: 0.952 | Rate: 6552 examples/s (4726.94 s) \n",
      "Epoch 49, Step 1994923 | Training Acc: 0.961 | Test Acc: 0.947 | Test Loss: 0.528 | Test AUC: 0.947 | Rate: 6493 examples/s (4746.06 s) \n",
      "Epoch 49, Step 2661547 | Training Acc: 0.945 | Test Acc: 0.945 | Test Loss: 0.452 | Test AUC: 0.932 | Rate: 6219 examples/s (4765.15 s) \n",
      "Epoch 49, Step 3328171 | Training Acc: 0.951 | Test Acc: 0.947 | Test Loss: 0.599 | Test AUC: 0.936 | Rate: 6219 examples/s (4784.27 s) \n",
      "Epoch 50, Step 661574 | Training Acc: 0.957 | Test Acc: 0.938 | Test Loss: 0.543 | Test AUC: 0.944 | Rate: 6460 examples/s (4803.41 s) \n",
      "Epoch 50, Step 1328198 | Training Acc: 0.934 | Test Acc: 0.965 | Test Loss: 0.237 | Test AUC: 0.946 | Rate: 6184 examples/s (4822.50 s) \n",
      "Epoch 50, Step 1994822 | Training Acc: 0.951 | Test Acc: 0.951 | Test Loss: 0.534 | Test AUC: 0.942 | Rate: 6471 examples/s (4841.61 s) \n",
      "Epoch 50, Step 2661446 | Training Acc: 0.949 | Test Acc: 0.969 | Test Loss: 0.264 | Test AUC: 0.951 | Rate: 5968 examples/s (4860.73 s) \n",
      "Epoch 50, Step 3328070 | Training Acc: 0.951 | Test Acc: 0.943 | Test Loss: 0.441 | Test AUC: 0.943 | Rate: 6424 examples/s (4879.83 s) \n",
      "Epoch 51, Step 661473 | Training Acc: 0.963 | Test Acc: 0.947 | Test Loss: 0.457 | Test AUC: 0.950 | Rate: 6461 examples/s (4898.95 s) \n",
      "Epoch 51, Step 1328097 | Training Acc: 0.961 | Test Acc: 0.963 | Test Loss: 0.274 | Test AUC: 0.954 | Rate: 5812 examples/s (4918.07 s) \n",
      "Epoch 51, Step 1994721 | Training Acc: 0.965 | Test Acc: 0.943 | Test Loss: 0.572 | Test AUC: 0.945 | Rate: 6052 examples/s (4937.17 s) \n",
      "Epoch 51, Step 2661345 | Training Acc: 0.955 | Test Acc: 0.953 | Test Loss: 0.539 | Test AUC: 0.948 | Rate: 6438 examples/s (4956.29 s) \n",
      "Epoch 51, Step 3327969 | Training Acc: 0.963 | Test Acc: 0.971 | Test Loss: 0.353 | Test AUC: 0.960 | Rate: 6519 examples/s (4975.42 s) \n",
      "Epoch 52, Step 661372 | Training Acc: 0.967 | Test Acc: 0.963 | Test Loss: 0.365 | Test AUC: 0.960 | Rate: 6359 examples/s (4994.52 s) \n",
      "Epoch 52, Step 1327996 | Training Acc: 0.973 | Test Acc: 0.955 | Test Loss: 0.570 | Test AUC: 0.962 | Rate: 6462 examples/s (5013.66 s) \n",
      "Epoch 52, Step 1994620 | Training Acc: 0.947 | Test Acc: 0.945 | Test Loss: 0.335 | Test AUC: 0.943 | Rate: 6435 examples/s (5032.76 s) \n",
      "Epoch 52, Step 2661244 | Training Acc: 0.941 | Test Acc: 0.951 | Test Loss: 0.346 | Test AUC: 0.933 | Rate: 6550 examples/s (5051.88 s) \n",
      "Epoch 52, Step 3327868 | Training Acc: 0.947 | Test Acc: 0.949 | Test Loss: 0.394 | Test AUC: 0.942 | Rate: 6465 examples/s (5071.02 s) \n",
      "Epoch 53, Step 661271 | Training Acc: 0.934 | Test Acc: 0.971 | Test Loss: 0.456 | Test AUC: 0.941 | Rate: 6485 examples/s (5090.12 s) \n",
      "Epoch 53, Step 1327895 | Training Acc: 0.953 | Test Acc: 0.949 | Test Loss: 0.433 | Test AUC: 0.936 | Rate: 6419 examples/s (5109.25 s) \n",
      "Epoch 53, Step 1994519 | Training Acc: 0.941 | Test Acc: 0.973 | Test Loss: 0.259 | Test AUC: 0.954 | Rate: 6418 examples/s (5128.34 s) \n",
      "Epoch 53, Step 2661143 | Training Acc: 0.943 | Test Acc: 0.941 | Test Loss: 0.500 | Test AUC: 0.938 | Rate: 6514 examples/s (5147.44 s) \n",
      "Epoch 53, Step 3327767 | Training Acc: 0.963 | Test Acc: 0.965 | Test Loss: 0.234 | Test AUC: 0.956 | Rate: 6436 examples/s (5166.52 s) \n",
      "Epoch 54, Step 661170 | Training Acc: 0.965 | Test Acc: 0.941 | Test Loss: 0.523 | Test AUC: 0.956 | Rate: 6640 examples/s (5185.63 s) \n",
      "Epoch 54, Step 1327794 | Training Acc: 0.945 | Test Acc: 0.953 | Test Loss: 0.283 | Test AUC: 0.942 | Rate: 5606 examples/s (5204.73 s) \n",
      "Epoch 54, Step 1994418 | Training Acc: 0.947 | Test Acc: 0.953 | Test Loss: 0.318 | Test AUC: 0.934 | Rate: 6570 examples/s (5223.82 s) \n",
      "Epoch 54, Step 2661042 | Training Acc: 0.951 | Test Acc: 0.947 | Test Loss: 0.420 | Test AUC: 0.941 | Rate: 6521 examples/s (5242.93 s) \n",
      "Epoch 54, Step 3327666 | Training Acc: 0.941 | Test Acc: 0.953 | Test Loss: 0.287 | Test AUC: 0.945 | Rate: 6376 examples/s (5262.03 s) \n",
      "Epoch 55, Step 661069 | Training Acc: 0.951 | Test Acc: 0.951 | Test Loss: 0.401 | Test AUC: 0.938 | Rate: 6353 examples/s (5281.13 s) \n",
      "Epoch 55, Step 1327693 | Training Acc: 0.967 | Test Acc: 0.955 | Test Loss: 0.437 | Test AUC: 0.958 | Rate: 6429 examples/s (5300.23 s) \n",
      "Epoch 55, Step 1994317 | Training Acc: 0.953 | Test Acc: 0.959 | Test Loss: 0.434 | Test AUC: 0.949 | Rate: 6523 examples/s (5319.35 s) \n",
      "Epoch 55, Step 2660941 | Training Acc: 0.953 | Test Acc: 0.967 | Test Loss: 0.249 | Test AUC: 0.960 | Rate: 6489 examples/s (5338.44 s) \n",
      "Epoch 55, Step 3327565 | Training Acc: 0.953 | Test Acc: 0.953 | Test Loss: 0.294 | Test AUC: 0.943 | Rate: 6401 examples/s (5357.54 s) \n",
      "Epoch 56, Step 660968 | Training Acc: 0.967 | Test Acc: 0.947 | Test Loss: 0.651 | Test AUC: 0.954 | Rate: 6277 examples/s (5376.62 s) \n",
      "Epoch 56, Step 1327592 | Training Acc: 0.945 | Test Acc: 0.926 | Test Loss: 0.492 | Test AUC: 0.924 | Rate: 6301 examples/s (5395.61 s) \n",
      "Epoch 56, Step 1994216 | Training Acc: 0.955 | Test Acc: 0.943 | Test Loss: 0.620 | Test AUC: 0.938 | Rate: 6251 examples/s (5414.74 s) \n",
      "Epoch 56, Step 2660840 | Training Acc: 0.943 | Test Acc: 0.959 | Test Loss: 0.281 | Test AUC: 0.945 | Rate: 6269 examples/s (5433.83 s) \n",
      "Epoch 56, Step 3327464 | Training Acc: 0.951 | Test Acc: 0.973 | Test Loss: 0.340 | Test AUC: 0.959 | Rate: 6356 examples/s (5452.97 s) \n",
      "Epoch 57, Step 660867 | Training Acc: 0.947 | Test Acc: 0.965 | Test Loss: 0.310 | Test AUC: 0.937 | Rate: 6429 examples/s (5472.07 s) \n",
      "Epoch 57, Step 1327491 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.318 | Test AUC: 0.959 | Rate: 5959 examples/s (5491.20 s) \n",
      "Epoch 57, Step 1994115 | Training Acc: 0.965 | Test Acc: 0.963 | Test Loss: 0.220 | Test AUC: 0.956 | Rate: 6321 examples/s (5510.31 s) \n",
      "Epoch 57, Step 2660739 | Training Acc: 0.945 | Test Acc: 0.959 | Test Loss: 0.403 | Test AUC: 0.946 | Rate: 6563 examples/s (5529.43 s) \n",
      "Epoch 57, Step 3327363 | Training Acc: 0.963 | Test Acc: 0.949 | Test Loss: 0.266 | Test AUC: 0.949 | Rate: 6197 examples/s (5548.57 s) \n",
      "Epoch 58, Step 660766 | Training Acc: 0.969 | Test Acc: 0.953 | Test Loss: 0.437 | Test AUC: 0.955 | Rate: 6037 examples/s (5567.69 s) \n",
      "Epoch 58, Step 1327390 | Training Acc: 0.947 | Test Acc: 0.959 | Test Loss: 0.396 | Test AUC: 0.947 | Rate: 6257 examples/s (5586.84 s) \n",
      "Epoch 58, Step 1994014 | Training Acc: 0.949 | Test Acc: 0.959 | Test Loss: 0.290 | Test AUC: 0.945 | Rate: 6324 examples/s (5605.94 s) \n",
      "Epoch 58, Step 2660638 | Training Acc: 0.963 | Test Acc: 0.969 | Test Loss: 0.135 | Test AUC: 0.960 | Rate: 6370 examples/s (5625.09 s) \n",
      "Epoch 58, Step 3327262 | Training Acc: 0.945 | Test Acc: 0.953 | Test Loss: 0.464 | Test AUC: 0.944 | Rate: 6274 examples/s (5644.19 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Step 660665 | Training Acc: 0.977 | Test Acc: 0.947 | Test Loss: 0.413 | Test AUC: 0.952 | Rate: 6231 examples/s (5663.31 s) \n",
      "Epoch 59, Step 1327289 | Training Acc: 0.965 | Test Acc: 0.949 | Test Loss: 0.310 | Test AUC: 0.952 | Rate: 6000 examples/s (5682.43 s) \n",
      "Epoch 59, Step 1993913 | Training Acc: 0.961 | Test Acc: 0.971 | Test Loss: 0.153 | Test AUC: 0.954 | Rate: 6476 examples/s (5701.57 s) \n",
      "Epoch 59, Step 2660537 | Training Acc: 0.959 | Test Acc: 0.953 | Test Loss: 0.752 | Test AUC: 0.957 | Rate: 6355 examples/s (5720.68 s) \n",
      "Epoch 59, Step 3327161 | Training Acc: 0.953 | Test Acc: 0.971 | Test Loss: 0.273 | Test AUC: 0.957 | Rate: 6480 examples/s (5739.82 s) \n",
      "Epoch 60, Step 660564 | Training Acc: 0.959 | Test Acc: 0.957 | Test Loss: 0.338 | Test AUC: 0.958 | Rate: 6158 examples/s (5758.95 s) \n",
      "Epoch 60, Step 1327188 | Training Acc: 0.957 | Test Acc: 0.957 | Test Loss: 0.338 | Test AUC: 0.951 | Rate: 6357 examples/s (5778.07 s) \n",
      "Epoch 60, Step 1993812 | Training Acc: 0.963 | Test Acc: 0.971 | Test Loss: 0.205 | Test AUC: 0.960 | Rate: 6238 examples/s (5797.18 s) \n",
      "Epoch 60, Step 2660436 | Training Acc: 0.939 | Test Acc: 0.965 | Test Loss: 0.260 | Test AUC: 0.942 | Rate: 6125 examples/s (5816.30 s) \n",
      "Epoch 60, Step 3327060 | Training Acc: 0.947 | Test Acc: 0.959 | Test Loss: 0.336 | Test AUC: 0.941 | Rate: 6250 examples/s (5835.45 s) \n",
      "Epoch 61, Step 660463 | Training Acc: 0.961 | Test Acc: 0.971 | Test Loss: 0.233 | Test AUC: 0.959 | Rate: 6534 examples/s (5854.56 s) \n",
      "Epoch 61, Step 1327087 | Training Acc: 0.969 | Test Acc: 0.951 | Test Loss: 0.477 | Test AUC: 0.955 | Rate: 6397 examples/s (5873.69 s) \n",
      "Epoch 61, Step 1993711 | Training Acc: 0.967 | Test Acc: 0.953 | Test Loss: 0.232 | Test AUC: 0.952 | Rate: 6220 examples/s (5892.83 s) \n",
      "Epoch 61, Step 2660335 | Training Acc: 0.951 | Test Acc: 0.961 | Test Loss: 0.306 | Test AUC: 0.949 | Rate: 5964 examples/s (5911.98 s) \n",
      "Epoch 61, Step 3326959 | Training Acc: 0.945 | Test Acc: 0.955 | Test Loss: 0.288 | Test AUC: 0.944 | Rate: 6233 examples/s (5931.11 s) \n",
      "Epoch 62, Step 660362 | Training Acc: 0.963 | Test Acc: 0.961 | Test Loss: 0.535 | Test AUC: 0.956 | Rate: 5147 examples/s (5950.24 s) \n",
      "Epoch 62, Step 1326986 | Training Acc: 0.957 | Test Acc: 0.961 | Test Loss: 0.185 | Test AUC: 0.950 | Rate: 6324 examples/s (5969.34 s) \n",
      "Epoch 62, Step 1993610 | Training Acc: 0.957 | Test Acc: 0.953 | Test Loss: 0.524 | Test AUC: 0.945 | Rate: 6383 examples/s (5988.45 s) \n",
      "Epoch 62, Step 2660234 | Training Acc: 0.975 | Test Acc: 0.967 | Test Loss: 0.242 | Test AUC: 0.965 | Rate: 6354 examples/s (6007.59 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch62.ckpt-62\n",
      "Epoch 62, Step 3326858 | Training Acc: 0.971 | Test Acc: 0.975 | Test Loss: 0.177 | Test AUC: 0.969 | Rate: 1373 examples/s (6027.03 s) [*]\n",
      "Epoch 63, Step 660261 | Training Acc: 0.957 | Test Acc: 0.959 | Test Loss: 0.272 | Test AUC: 0.948 | Rate: 6236 examples/s (6046.14 s) \n",
      "Epoch 63, Step 1326885 | Training Acc: 0.953 | Test Acc: 0.953 | Test Loss: 0.257 | Test AUC: 0.948 | Rate: 6172 examples/s (6065.25 s) \n",
      "Epoch 63, Step 1993509 | Training Acc: 0.943 | Test Acc: 0.969 | Test Loss: 0.241 | Test AUC: 0.943 | Rate: 6302 examples/s (6084.37 s) \n",
      "Epoch 63, Step 2660133 | Training Acc: 0.961 | Test Acc: 0.959 | Test Loss: 0.434 | Test AUC: 0.956 | Rate: 6043 examples/s (6103.51 s) \n",
      "Epoch 63, Step 3326757 | Training Acc: 0.957 | Test Acc: 0.947 | Test Loss: 0.564 | Test AUC: 0.941 | Rate: 6133 examples/s (6122.63 s) \n",
      "Epoch 64, Step 660160 | Training Acc: 0.943 | Test Acc: 0.957 | Test Loss: 0.302 | Test AUC: 0.940 | Rate: 6165 examples/s (6141.74 s) \n",
      "Epoch 64, Step 1326784 | Training Acc: 0.953 | Test Acc: 0.957 | Test Loss: 0.275 | Test AUC: 0.945 | Rate: 6259 examples/s (6160.88 s) \n",
      "Epoch 64, Step 1993408 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.349 | Test AUC: 0.961 | Rate: 6180 examples/s (6180.00 s) \n",
      "Epoch 64, Step 2660032 | Training Acc: 0.967 | Test Acc: 0.965 | Test Loss: 0.250 | Test AUC: 0.963 | Rate: 6299 examples/s (6199.10 s) \n",
      "Epoch 64, Step 3326656 | Training Acc: 0.945 | Test Acc: 0.957 | Test Loss: 0.545 | Test AUC: 0.930 | Rate: 6174 examples/s (6218.20 s) \n",
      "Epoch 65, Step 660059 | Training Acc: 0.959 | Test Acc: 0.949 | Test Loss: 0.372 | Test AUC: 0.946 | Rate: 6285 examples/s (6237.30 s) \n",
      "Epoch 65, Step 1326683 | Training Acc: 0.969 | Test Acc: 0.961 | Test Loss: 0.261 | Test AUC: 0.963 | Rate: 6401 examples/s (6256.41 s) \n",
      "Epoch 65, Step 1993307 | Training Acc: 0.971 | Test Acc: 0.957 | Test Loss: 0.300 | Test AUC: 0.952 | Rate: 5935 examples/s (6275.51 s) \n",
      "Epoch 65, Step 2659931 | Training Acc: 0.961 | Test Acc: 0.977 | Test Loss: 0.251 | Test AUC: 0.964 | Rate: 6234 examples/s (6294.61 s) \n",
      "Epoch 65, Step 3326555 | Training Acc: 0.969 | Test Acc: 0.947 | Test Loss: 0.433 | Test AUC: 0.955 | Rate: 6262 examples/s (6313.74 s) \n",
      "Epoch 66, Step 659958 | Training Acc: 0.963 | Test Acc: 0.971 | Test Loss: 0.150 | Test AUC: 0.963 | Rate: 6252 examples/s (6332.86 s) \n",
      "Epoch 66, Step 1326582 | Training Acc: 0.947 | Test Acc: 0.953 | Test Loss: 0.392 | Test AUC: 0.943 | Rate: 6450 examples/s (6351.98 s) \n",
      "Epoch 66, Step 1993206 | Training Acc: 0.959 | Test Acc: 0.961 | Test Loss: 0.344 | Test AUC: 0.945 | Rate: 6232 examples/s (6371.11 s) \n",
      "Epoch 66, Step 2659830 | Training Acc: 0.975 | Test Acc: 0.959 | Test Loss: 0.175 | Test AUC: 0.960 | Rate: 6322 examples/s (6390.22 s) \n",
      "Epoch 66, Step 3326454 | Training Acc: 0.961 | Test Acc: 0.961 | Test Loss: 0.373 | Test AUC: 0.951 | Rate: 6338 examples/s (6409.33 s) \n",
      "Epoch 67, Step 659857 | Training Acc: 0.945 | Test Acc: 0.961 | Test Loss: 0.410 | Test AUC: 0.948 | Rate: 6353 examples/s (6428.44 s) \n",
      "Epoch 67, Step 1326481 | Training Acc: 0.953 | Test Acc: 0.973 | Test Loss: 0.257 | Test AUC: 0.956 | Rate: 6209 examples/s (6447.55 s) \n",
      "Epoch 67, Step 1993105 | Training Acc: 0.959 | Test Acc: 0.955 | Test Loss: 0.481 | Test AUC: 0.955 | Rate: 6294 examples/s (6466.65 s) \n",
      "Epoch 67, Step 2659729 | Training Acc: 0.965 | Test Acc: 0.953 | Test Loss: 0.350 | Test AUC: 0.955 | Rate: 5269 examples/s (6485.78 s) \n",
      "Epoch 67, Step 3326353 | Training Acc: 0.963 | Test Acc: 0.967 | Test Loss: 0.203 | Test AUC: 0.959 | Rate: 6225 examples/s (6504.89 s) \n",
      "Epoch 68, Step 659756 | Training Acc: 0.965 | Test Acc: 0.953 | Test Loss: 0.302 | Test AUC: 0.948 | Rate: 6247 examples/s (6524.01 s) \n",
      "Epoch 68, Step 1326380 | Training Acc: 0.957 | Test Acc: 0.959 | Test Loss: 0.219 | Test AUC: 0.953 | Rate: 6295 examples/s (6543.12 s) \n",
      "Epoch 68, Step 1993004 | Training Acc: 0.971 | Test Acc: 0.939 | Test Loss: 0.516 | Test AUC: 0.944 | Rate: 6413 examples/s (6562.20 s) \n",
      "Epoch 68, Step 2659628 | Training Acc: 0.961 | Test Acc: 0.969 | Test Loss: 0.263 | Test AUC: 0.956 | Rate: 6240 examples/s (6581.33 s) \n",
      "Epoch 68, Step 3326252 | Training Acc: 0.953 | Test Acc: 0.951 | Test Loss: 0.313 | Test AUC: 0.946 | Rate: 6330 examples/s (6600.44 s) \n",
      "Epoch 69, Step 659655 | Training Acc: 0.969 | Test Acc: 0.945 | Test Loss: 0.547 | Test AUC: 0.946 | Rate: 6109 examples/s (6619.55 s) \n",
      "Epoch 69, Step 1326279 | Training Acc: 0.957 | Test Acc: 0.957 | Test Loss: 0.260 | Test AUC: 0.951 | Rate: 6167 examples/s (6638.69 s) \n",
      "Epoch 69, Step 1992903 | Training Acc: 0.965 | Test Acc: 0.943 | Test Loss: 0.477 | Test AUC: 0.948 | Rate: 6397 examples/s (6657.80 s) \n",
      "Epoch 69, Step 2659527 | Training Acc: 0.963 | Test Acc: 0.961 | Test Loss: 0.360 | Test AUC: 0.957 | Rate: 6096 examples/s (6676.91 s) \n",
      "Epoch 69, Step 3326151 | Training Acc: 0.961 | Test Acc: 0.947 | Test Loss: 0.425 | Test AUC: 0.932 | Rate: 6237 examples/s (6696.02 s) \n",
      "Epoch 70, Step 659554 | Training Acc: 0.949 | Test Acc: 0.965 | Test Loss: 0.335 | Test AUC: 0.950 | Rate: 6196 examples/s (6715.13 s) \n",
      "Epoch 70, Step 1326178 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.301 | Test AUC: 0.960 | Rate: 6293 examples/s (6734.22 s) \n",
      "Epoch 70, Step 1992802 | Training Acc: 0.953 | Test Acc: 0.959 | Test Loss: 0.288 | Test AUC: 0.943 | Rate: 6034 examples/s (6753.33 s) \n",
      "Epoch 70, Step 2659426 | Training Acc: 0.951 | Test Acc: 0.963 | Test Loss: 0.421 | Test AUC: 0.943 | Rate: 6157 examples/s (6772.46 s) \n",
      "Epoch 70, Step 3326050 | Training Acc: 0.965 | Test Acc: 0.957 | Test Loss: 0.424 | Test AUC: 0.945 | Rate: 6230 examples/s (6791.59 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Step 659453 | Training Acc: 0.949 | Test Acc: 0.955 | Test Loss: 0.275 | Test AUC: 0.937 | Rate: 6126 examples/s (6810.72 s) \n",
      "Epoch 71, Step 1326077 | Training Acc: 0.975 | Test Acc: 0.971 | Test Loss: 0.294 | Test AUC: 0.968 | Rate: 5949 examples/s (6829.84 s) \n",
      "Epoch 71, Step 1992701 | Training Acc: 0.963 | Test Acc: 0.965 | Test Loss: 0.353 | Test AUC: 0.959 | Rate: 6269 examples/s (6848.98 s) \n",
      "Epoch 71, Step 2659325 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.467 | Test AUC: 0.952 | Rate: 6210 examples/s (6868.11 s) \n",
      "Epoch 71, Step 3325949 | Training Acc: 0.957 | Test Acc: 0.959 | Test Loss: 0.471 | Test AUC: 0.950 | Rate: 6209 examples/s (6887.25 s) \n",
      "Epoch 72, Step 659352 | Training Acc: 0.959 | Test Acc: 0.961 | Test Loss: 0.187 | Test AUC: 0.955 | Rate: 6133 examples/s (6906.33 s) \n",
      "Epoch 72, Step 1325976 | Training Acc: 0.961 | Test Acc: 0.975 | Test Loss: 0.080 | Test AUC: 0.962 | Rate: 6257 examples/s (6925.50 s) \n",
      "Epoch 72, Step 1992600 | Training Acc: 0.963 | Test Acc: 0.957 | Test Loss: 0.325 | Test AUC: 0.956 | Rate: 6134 examples/s (6944.61 s) \n",
      "Epoch 72, Step 2659224 | Training Acc: 0.973 | Test Acc: 0.945 | Test Loss: 0.396 | Test AUC: 0.952 | Rate: 6218 examples/s (6963.71 s) \n",
      "Epoch 72, Step 3325848 | Training Acc: 0.949 | Test Acc: 0.963 | Test Loss: 0.452 | Test AUC: 0.947 | Rate: 6049 examples/s (6982.83 s) \n",
      "Epoch 73, Step 659251 | Training Acc: 0.953 | Test Acc: 0.959 | Test Loss: 0.419 | Test AUC: 0.945 | Rate: 6185 examples/s (7001.96 s) \n",
      "Epoch 73, Step 1325875 | Training Acc: 0.953 | Test Acc: 0.961 | Test Loss: 0.186 | Test AUC: 0.947 | Rate: 5824 examples/s (7021.10 s) \n",
      "Epoch 73, Step 1992499 | Training Acc: 0.975 | Test Acc: 0.938 | Test Loss: 0.307 | Test AUC: 0.951 | Rate: 6133 examples/s (7040.24 s) \n",
      "Epoch 73, Step 2659123 | Training Acc: 0.975 | Test Acc: 0.955 | Test Loss: 0.306 | Test AUC: 0.953 | Rate: 5895 examples/s (7059.38 s) \n",
      "Epoch 73, Step 3325747 | Training Acc: 0.961 | Test Acc: 0.953 | Test Loss: 0.662 | Test AUC: 0.947 | Rate: 6231 examples/s (7078.53 s) \n",
      "Epoch 74, Step 659150 | Training Acc: 0.975 | Test Acc: 0.959 | Test Loss: 0.417 | Test AUC: 0.953 | Rate: 5725 examples/s (7097.64 s) \n",
      "Epoch 74, Step 1325774 | Training Acc: 0.959 | Test Acc: 0.943 | Test Loss: 0.387 | Test AUC: 0.941 | Rate: 6179 examples/s (7116.77 s) \n",
      "Epoch 74, Step 1992398 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.247 | Test AUC: 0.950 | Rate: 6064 examples/s (7135.90 s) \n",
      "Epoch 74, Step 2659022 | Training Acc: 0.957 | Test Acc: 0.969 | Test Loss: 0.219 | Test AUC: 0.962 | Rate: 6160 examples/s (7155.02 s) \n",
      "Epoch 74, Step 3325646 | Training Acc: 0.963 | Test Acc: 0.967 | Test Loss: 0.163 | Test AUC: 0.953 | Rate: 5677 examples/s (7174.13 s) \n",
      "Epoch 75, Step 659049 | Training Acc: 0.957 | Test Acc: 0.965 | Test Loss: 0.233 | Test AUC: 0.955 | Rate: 6067 examples/s (7193.13 s) \n",
      "Epoch 75, Step 1325673 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.309 | Test AUC: 0.944 | Rate: 6121 examples/s (7212.25 s) \n",
      "Epoch 75, Step 1992297 | Training Acc: 0.965 | Test Acc: 0.963 | Test Loss: 0.121 | Test AUC: 0.958 | Rate: 5875 examples/s (7231.37 s) \n",
      "Epoch 75, Step 2658921 | Training Acc: 0.969 | Test Acc: 0.945 | Test Loss: 0.316 | Test AUC: 0.944 | Rate: 6182 examples/s (7250.51 s) \n",
      "Epoch 75, Step 3325545 | Training Acc: 0.953 | Test Acc: 0.957 | Test Loss: 0.186 | Test AUC: 0.948 | Rate: 6073 examples/s (7269.63 s) \n",
      "Epoch 76, Step 658948 | Training Acc: 0.943 | Test Acc: 0.969 | Test Loss: 0.244 | Test AUC: 0.951 | Rate: 6125 examples/s (7288.75 s) \n",
      "Epoch 76, Step 1325572 | Training Acc: 0.979 | Test Acc: 0.934 | Test Loss: 0.649 | Test AUC: 0.946 | Rate: 5900 examples/s (7307.88 s) \n",
      "Epoch 76, Step 1992196 | Training Acc: 0.965 | Test Acc: 0.967 | Test Loss: 0.202 | Test AUC: 0.963 | Rate: 6319 examples/s (7326.98 s) \n",
      "Epoch 76, Step 2658820 | Training Acc: 0.941 | Test Acc: 0.975 | Test Loss: 0.142 | Test AUC: 0.947 | Rate: 6172 examples/s (7346.09 s) \n",
      "Epoch 76, Step 3325444 | Training Acc: 0.941 | Test Acc: 0.971 | Test Loss: 0.336 | Test AUC: 0.946 | Rate: 6103 examples/s (7365.23 s) \n",
      "Epoch 77, Step 658847 | Training Acc: 0.938 | Test Acc: 0.957 | Test Loss: 0.175 | Test AUC: 0.924 | Rate: 6035 examples/s (7384.35 s) \n",
      "Epoch 77, Step 1325471 | Training Acc: 0.967 | Test Acc: 0.957 | Test Loss: 0.356 | Test AUC: 0.960 | Rate: 6323 examples/s (7403.46 s) \n",
      "Epoch 77, Step 1992095 | Training Acc: 0.955 | Test Acc: 0.963 | Test Loss: 0.201 | Test AUC: 0.952 | Rate: 6023 examples/s (7422.58 s) \n",
      "Epoch 77, Step 2658719 | Training Acc: 0.977 | Test Acc: 0.959 | Test Loss: 0.182 | Test AUC: 0.961 | Rate: 6215 examples/s (7441.71 s) \n",
      "Epoch 77, Step 3325343 | Training Acc: 0.953 | Test Acc: 0.951 | Test Loss: 0.388 | Test AUC: 0.949 | Rate: 6080 examples/s (7460.84 s) \n",
      "Epoch 78, Step 658746 | Training Acc: 0.947 | Test Acc: 0.969 | Test Loss: 0.241 | Test AUC: 0.947 | Rate: 6141 examples/s (7479.96 s) \n",
      "Epoch 78, Step 1325370 | Training Acc: 0.965 | Test Acc: 0.959 | Test Loss: 0.374 | Test AUC: 0.949 | Rate: 6074 examples/s (7499.09 s) \n",
      "Epoch 78, Step 1991994 | Training Acc: 0.969 | Test Acc: 0.965 | Test Loss: 0.263 | Test AUC: 0.960 | Rate: 6070 examples/s (7518.22 s) \n",
      "Epoch 78, Step 2658618 | Training Acc: 0.951 | Test Acc: 0.963 | Test Loss: 0.346 | Test AUC: 0.949 | Rate: 6175 examples/s (7537.36 s) \n",
      "Epoch 78, Step 3325242 | Training Acc: 0.953 | Test Acc: 0.955 | Test Loss: 0.295 | Test AUC: 0.950 | Rate: 5362 examples/s (7556.49 s) \n",
      "Epoch 79, Step 658645 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.295 | Test AUC: 0.952 | Rate: 6032 examples/s (7575.61 s) \n",
      "Epoch 79, Step 1325269 | Training Acc: 0.953 | Test Acc: 0.955 | Test Loss: 0.222 | Test AUC: 0.937 | Rate: 6056 examples/s (7594.72 s) \n",
      "Epoch 79, Step 1991893 | Training Acc: 0.945 | Test Acc: 0.961 | Test Loss: 0.197 | Test AUC: 0.941 | Rate: 6095 examples/s (7613.83 s) \n",
      "Epoch 79, Step 2658517 | Training Acc: 0.957 | Test Acc: 0.949 | Test Loss: 0.403 | Test AUC: 0.942 | Rate: 5909 examples/s (7632.94 s) \n",
      "Epoch 79, Step 3325141 | Training Acc: 0.963 | Test Acc: 0.947 | Test Loss: 0.295 | Test AUC: 0.944 | Rate: 6058 examples/s (7652.06 s) \n",
      "Epoch 80, Step 658544 | Training Acc: 0.957 | Test Acc: 0.965 | Test Loss: 0.611 | Test AUC: 0.950 | Rate: 6113 examples/s (7671.16 s) \n",
      "Epoch 80, Step 1325168 | Training Acc: 0.949 | Test Acc: 0.969 | Test Loss: 0.298 | Test AUC: 0.949 | Rate: 6110 examples/s (7690.28 s) \n",
      "Epoch 80, Step 1991792 | Training Acc: 0.975 | Test Acc: 0.963 | Test Loss: 0.313 | Test AUC: 0.962 | Rate: 6271 examples/s (7709.40 s) \n",
      "Epoch 80, Step 2658416 | Training Acc: 0.949 | Test Acc: 0.953 | Test Loss: 0.287 | Test AUC: 0.945 | Rate: 6302 examples/s (7728.52 s) \n",
      "Epoch 80, Step 3325040 | Training Acc: 0.947 | Test Acc: 0.963 | Test Loss: 0.214 | Test AUC: 0.952 | Rate: 6213 examples/s (7747.64 s) \n",
      "Epoch 81, Step 658443 | Training Acc: 0.969 | Test Acc: 0.959 | Test Loss: 0.274 | Test AUC: 0.955 | Rate: 6116 examples/s (7766.76 s) \n",
      "Epoch 81, Step 1325067 | Training Acc: 0.965 | Test Acc: 0.959 | Test Loss: 0.337 | Test AUC: 0.955 | Rate: 6059 examples/s (7785.86 s) \n",
      "Epoch 81, Step 1991691 | Training Acc: 0.959 | Test Acc: 0.965 | Test Loss: 0.326 | Test AUC: 0.957 | Rate: 5827 examples/s (7804.97 s) \n",
      "Epoch 81, Step 2658315 | Training Acc: 0.957 | Test Acc: 0.971 | Test Loss: 0.200 | Test AUC: 0.957 | Rate: 6187 examples/s (7824.09 s) \n",
      "Epoch 81, Step 3324939 | Training Acc: 0.961 | Test Acc: 0.953 | Test Loss: 0.379 | Test AUC: 0.944 | Rate: 6041 examples/s (7843.23 s) \n",
      "Epoch 82, Step 658342 | Training Acc: 0.949 | Test Acc: 0.955 | Test Loss: 0.370 | Test AUC: 0.942 | Rate: 6059 examples/s (7862.33 s) \n",
      "Epoch 82, Step 1324966 | Training Acc: 0.957 | Test Acc: 0.951 | Test Loss: 0.246 | Test AUC: 0.948 | Rate: 6083 examples/s (7881.44 s) \n",
      "Epoch 82, Step 1991590 | Training Acc: 0.957 | Test Acc: 0.963 | Test Loss: 0.290 | Test AUC: 0.953 | Rate: 6099 examples/s (7900.56 s) \n",
      "Epoch 82, Step 2658214 | Training Acc: 0.961 | Test Acc: 0.963 | Test Loss: 0.222 | Test AUC: 0.947 | Rate: 5964 examples/s (7919.70 s) \n",
      "Epoch 82, Step 3324838 | Training Acc: 0.969 | Test Acc: 0.953 | Test Loss: 0.304 | Test AUC: 0.943 | Rate: 6050 examples/s (7938.84 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Step 658241 | Training Acc: 0.955 | Test Acc: 0.957 | Test Loss: 0.185 | Test AUC: 0.946 | Rate: 6050 examples/s (7957.97 s) \n",
      "Epoch 83, Step 1324865 | Training Acc: 0.975 | Test Acc: 0.945 | Test Loss: 0.397 | Test AUC: 0.952 | Rate: 5756 examples/s (7977.11 s) \n",
      "Epoch 83, Step 1991489 | Training Acc: 0.969 | Test Acc: 0.969 | Test Loss: 0.294 | Test AUC: 0.963 | Rate: 6017 examples/s (7996.24 s) \n",
      "Epoch 83, Step 2658113 | Training Acc: 0.963 | Test Acc: 0.965 | Test Loss: 0.123 | Test AUC: 0.950 | Rate: 5957 examples/s (8015.37 s) \n",
      "Epoch 83, Step 3324737 | Training Acc: 0.953 | Test Acc: 0.947 | Test Loss: 0.270 | Test AUC: 0.934 | Rate: 6030 examples/s (8034.48 s) \n",
      "Epoch 84, Step 658140 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.596 | Test AUC: 0.946 | Rate: 5780 examples/s (8053.59 s) \n",
      "Epoch 84, Step 1324764 | Training Acc: 0.967 | Test Acc: 0.977 | Test Loss: 0.194 | Test AUC: 0.967 | Rate: 5991 examples/s (8072.70 s) \n",
      "Epoch 84, Step 1991388 | Training Acc: 0.955 | Test Acc: 0.955 | Test Loss: 0.276 | Test AUC: 0.945 | Rate: 6036 examples/s (8091.80 s) \n",
      "Epoch 84, Step 2658012 | Training Acc: 0.971 | Test Acc: 0.959 | Test Loss: 0.171 | Test AUC: 0.961 | Rate: 5923 examples/s (8110.93 s) \n",
      "Epoch 84, Step 3324636 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.262 | Test AUC: 0.952 | Rate: 5989 examples/s (8130.04 s) \n",
      "Epoch 85, Step 658039 | Training Acc: 0.971 | Test Acc: 0.973 | Test Loss: 0.171 | Test AUC: 0.965 | Rate: 6075 examples/s (8149.16 s) \n",
      "Epoch 85, Step 1324663 | Training Acc: 0.963 | Test Acc: 0.961 | Test Loss: 0.207 | Test AUC: 0.949 | Rate: 6185 examples/s (8168.29 s) \n",
      "Epoch 85, Step 1991287 | Training Acc: 0.961 | Test Acc: 0.953 | Test Loss: 0.304 | Test AUC: 0.941 | Rate: 5892 examples/s (8187.41 s) \n",
      "Epoch 85, Step 2657911 | Training Acc: 0.961 | Test Acc: 0.957 | Test Loss: 0.311 | Test AUC: 0.947 | Rate: 6046 examples/s (8206.54 s) \n",
      "Epoch 85, Step 3324535 | Training Acc: 0.965 | Test Acc: 0.959 | Test Loss: 0.434 | Test AUC: 0.955 | Rate: 5995 examples/s (8225.67 s) \n",
      "Epoch 86, Step 657938 | Training Acc: 0.951 | Test Acc: 0.973 | Test Loss: 0.294 | Test AUC: 0.956 | Rate: 5963 examples/s (8244.80 s) \n",
      "Epoch 86, Step 1324562 | Training Acc: 0.943 | Test Acc: 0.967 | Test Loss: 0.204 | Test AUC: 0.942 | Rate: 5955 examples/s (8263.93 s) \n",
      "Epoch 86, Step 1991186 | Training Acc: 0.973 | Test Acc: 0.961 | Test Loss: 0.354 | Test AUC: 0.957 | Rate: 6002 examples/s (8283.04 s) \n",
      "Epoch 86, Step 2657810 | Training Acc: 0.945 | Test Acc: 0.973 | Test Loss: 0.250 | Test AUC: 0.953 | Rate: 5873 examples/s (8302.18 s) \n",
      "Epoch 86, Step 3324434 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.311 | Test AUC: 0.951 | Rate: 5917 examples/s (8321.31 s) \n",
      "Epoch 87, Step 657837 | Training Acc: 0.947 | Test Acc: 0.971 | Test Loss: 0.254 | Test AUC: 0.955 | Rate: 6062 examples/s (8340.42 s) \n",
      "Epoch 87, Step 1324461 | Training Acc: 0.973 | Test Acc: 0.945 | Test Loss: 0.354 | Test AUC: 0.954 | Rate: 6045 examples/s (8359.55 s) \n",
      "Epoch 87, Step 1991085 | Training Acc: 0.951 | Test Acc: 0.957 | Test Loss: 0.151 | Test AUC: 0.946 | Rate: 6021 examples/s (8378.70 s) \n",
      "Epoch 87, Step 2657709 | Training Acc: 0.969 | Test Acc: 0.963 | Test Loss: 0.232 | Test AUC: 0.957 | Rate: 6172 examples/s (8397.86 s) \n",
      "Epoch 87, Step 3324333 | Training Acc: 0.965 | Test Acc: 0.961 | Test Loss: 0.271 | Test AUC: 0.952 | Rate: 5971 examples/s (8417.03 s) \n",
      "Epoch 88, Step 657736 | Training Acc: 0.967 | Test Acc: 0.965 | Test Loss: 0.329 | Test AUC: 0.958 | Rate: 6040 examples/s (8436.18 s) \n",
      "Epoch 88, Step 1324360 | Training Acc: 0.957 | Test Acc: 0.949 | Test Loss: 0.364 | Test AUC: 0.945 | Rate: 5986 examples/s (8455.32 s) \n",
      "Epoch 88, Step 1990984 | Training Acc: 0.979 | Test Acc: 0.959 | Test Loss: 0.366 | Test AUC: 0.959 | Rate: 6097 examples/s (8474.46 s) \n",
      "Epoch 88, Step 2657608 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.332 | Test AUC: 0.950 | Rate: 6045 examples/s (8493.58 s) \n",
      "Epoch 88, Step 3324232 | Training Acc: 0.955 | Test Acc: 0.945 | Test Loss: 0.308 | Test AUC: 0.942 | Rate: 5671 examples/s (8512.71 s) \n",
      "Epoch 89, Step 657635 | Training Acc: 0.936 | Test Acc: 0.951 | Test Loss: 0.429 | Test AUC: 0.929 | Rate: 5967 examples/s (8531.84 s) \n",
      "Epoch 89, Step 1324259 | Training Acc: 0.979 | Test Acc: 0.969 | Test Loss: 0.206 | Test AUC: 0.959 | Rate: 5956 examples/s (8550.96 s) \n",
      "Epoch 89, Step 1990883 | Training Acc: 0.943 | Test Acc: 0.967 | Test Loss: 0.203 | Test AUC: 0.944 | Rate: 6077 examples/s (8570.11 s) \n",
      "Epoch 89, Step 2657507 | Training Acc: 0.973 | Test Acc: 0.967 | Test Loss: 0.439 | Test AUC: 0.961 | Rate: 5341 examples/s (8589.24 s) \n",
      "Epoch 89, Step 3324131 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.245 | Test AUC: 0.958 | Rate: 5925 examples/s (8608.38 s) \n",
      "Epoch 90, Step 657534 | Training Acc: 0.967 | Test Acc: 0.963 | Test Loss: 0.250 | Test AUC: 0.956 | Rate: 5930 examples/s (8627.53 s) \n",
      "Epoch 90, Step 1324158 | Training Acc: 0.959 | Test Acc: 0.961 | Test Loss: 0.143 | Test AUC: 0.952 | Rate: 5713 examples/s (8646.69 s) \n",
      "Epoch 90, Step 1990782 | Training Acc: 0.971 | Test Acc: 0.963 | Test Loss: 0.259 | Test AUC: 0.962 | Rate: 5912 examples/s (8665.80 s) \n",
      "Epoch 90, Step 2657406 | Training Acc: 0.953 | Test Acc: 0.963 | Test Loss: 0.327 | Test AUC: 0.952 | Rate: 6073 examples/s (8684.91 s) \n",
      "Epoch 90, Step 3324030 | Training Acc: 0.959 | Test Acc: 0.973 | Test Loss: 0.354 | Test AUC: 0.956 | Rate: 5947 examples/s (8704.02 s) \n",
      "Epoch 91, Step 657433 | Training Acc: 0.977 | Test Acc: 0.971 | Test Loss: 0.192 | Test AUC: 0.967 | Rate: 6101 examples/s (8723.15 s) \n",
      "Epoch 91, Step 1324057 | Training Acc: 0.969 | Test Acc: 0.955 | Test Loss: 0.416 | Test AUC: 0.950 | Rate: 5278 examples/s (8742.28 s) \n",
      "Epoch 91, Step 1990681 | Training Acc: 0.975 | Test Acc: 0.967 | Test Loss: 0.130 | Test AUC: 0.964 | Rate: 6013 examples/s (8761.40 s) \n",
      "Epoch 91, Step 2657305 | Training Acc: 0.951 | Test Acc: 0.969 | Test Loss: 0.221 | Test AUC: 0.953 | Rate: 6071 examples/s (8780.53 s) \n",
      "Epoch 91, Step 3323929 | Training Acc: 0.945 | Test Acc: 0.961 | Test Loss: 0.182 | Test AUC: 0.951 | Rate: 5994 examples/s (8799.69 s) \n",
      "Epoch 92, Step 657332 | Training Acc: 0.969 | Test Acc: 0.965 | Test Loss: 0.444 | Test AUC: 0.963 | Rate: 6137 examples/s (8818.84 s) \n",
      "Epoch 92, Step 1323956 | Training Acc: 0.969 | Test Acc: 0.965 | Test Loss: 0.238 | Test AUC: 0.954 | Rate: 6038 examples/s (8837.97 s) \n",
      "Epoch 92, Step 1990580 | Training Acc: 0.967 | Test Acc: 0.961 | Test Loss: 0.403 | Test AUC: 0.958 | Rate: 6027 examples/s (8857.10 s) \n",
      "Epoch 92, Step 2657204 | Training Acc: 0.945 | Test Acc: 0.957 | Test Loss: 0.226 | Test AUC: 0.941 | Rate: 5916 examples/s (8876.24 s) \n",
      "Epoch 92, Step 3323828 | Training Acc: 0.951 | Test Acc: 0.967 | Test Loss: 0.252 | Test AUC: 0.956 | Rate: 6146 examples/s (8895.40 s) \n",
      "Epoch 93, Step 657231 | Training Acc: 0.977 | Test Acc: 0.949 | Test Loss: 0.217 | Test AUC: 0.957 | Rate: 6032 examples/s (8914.54 s) \n",
      "Epoch 93, Step 1323855 | Training Acc: 0.971 | Test Acc: 0.957 | Test Loss: 0.321 | Test AUC: 0.950 | Rate: 5826 examples/s (8933.67 s) \n",
      "Epoch 93, Step 1990479 | Training Acc: 0.947 | Test Acc: 0.971 | Test Loss: 0.249 | Test AUC: 0.953 | Rate: 5776 examples/s (8952.82 s) \n",
      "Epoch 93, Step 2657103 | Training Acc: 0.961 | Test Acc: 0.967 | Test Loss: 0.176 | Test AUC: 0.956 | Rate: 6003 examples/s (8971.94 s) \n",
      "Epoch 93, Step 3323727 | Training Acc: 0.959 | Test Acc: 0.951 | Test Loss: 0.332 | Test AUC: 0.941 | Rate: 5926 examples/s (8990.98 s) \n",
      "Epoch 94, Step 657130 | Training Acc: 0.955 | Test Acc: 0.961 | Test Loss: 0.288 | Test AUC: 0.942 | Rate: 6021 examples/s (9010.13 s) \n",
      "Epoch 94, Step 1323754 | Training Acc: 0.959 | Test Acc: 0.959 | Test Loss: 0.454 | Test AUC: 0.949 | Rate: 5988 examples/s (9029.26 s) \n",
      "Epoch 94, Step 1990378 | Training Acc: 0.963 | Test Acc: 0.953 | Test Loss: 0.318 | Test AUC: 0.949 | Rate: 5980 examples/s (9048.39 s) \n",
      "Epoch 94, Step 2657002 | Training Acc: 0.963 | Test Acc: 0.959 | Test Loss: 0.229 | Test AUC: 0.951 | Rate: 5835 examples/s (9067.52 s) \n",
      "Epoch 94, Step 3323626 | Training Acc: 0.967 | Test Acc: 0.957 | Test Loss: 0.572 | Test AUC: 0.950 | Rate: 5944 examples/s (9086.66 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Step 657029 | Training Acc: 0.961 | Test Acc: 0.980 | Test Loss: 0.120 | Test AUC: 0.964 | Rate: 5853 examples/s (9105.82 s) \n",
      "Epoch 95, Step 1323653 | Training Acc: 0.969 | Test Acc: 0.967 | Test Loss: 0.232 | Test AUC: 0.960 | Rate: 5998 examples/s (9124.96 s) \n",
      "Epoch 95, Step 1990277 | Training Acc: 0.961 | Test Acc: 0.957 | Test Loss: 0.249 | Test AUC: 0.949 | Rate: 5962 examples/s (9144.11 s) \n",
      "Epoch 95, Step 2656901 | Training Acc: 0.965 | Test Acc: 0.969 | Test Loss: 0.240 | Test AUC: 0.966 | Rate: 5712 examples/s (9163.27 s) \n",
      "Epoch 95, Step 3323525 | Training Acc: 0.973 | Test Acc: 0.967 | Test Loss: 0.233 | Test AUC: 0.960 | Rate: 5997 examples/s (9182.42 s) \n",
      "Epoch 96, Step 656928 | Training Acc: 0.955 | Test Acc: 0.953 | Test Loss: 0.210 | Test AUC: 0.944 | Rate: 5871 examples/s (9201.55 s) \n",
      "Epoch 96, Step 1323552 | Training Acc: 0.967 | Test Acc: 0.943 | Test Loss: 0.458 | Test AUC: 0.941 | Rate: 5945 examples/s (9220.68 s) \n",
      "Epoch 96, Step 1990176 | Training Acc: 0.961 | Test Acc: 0.959 | Test Loss: 0.276 | Test AUC: 0.952 | Rate: 5867 examples/s (9239.82 s) \n",
      "Epoch 96, Step 2656800 | Training Acc: 0.967 | Test Acc: 0.945 | Test Loss: 0.354 | Test AUC: 0.946 | Rate: 5985 examples/s (9258.96 s) \n",
      "Epoch 96, Step 3323424 | Training Acc: 0.947 | Test Acc: 0.957 | Test Loss: 0.327 | Test AUC: 0.943 | Rate: 5955 examples/s (9278.09 s) \n",
      "Epoch 97, Step 656827 | Training Acc: 0.963 | Test Acc: 0.982 | Test Loss: 0.119 | Test AUC: 0.963 | Rate: 5994 examples/s (9297.21 s) \n",
      "Epoch 97, Step 1323451 | Training Acc: 0.973 | Test Acc: 0.973 | Test Loss: 0.205 | Test AUC: 0.968 | Rate: 5993 examples/s (9316.32 s) \n",
      "Epoch 97, Step 1990075 | Training Acc: 0.963 | Test Acc: 0.957 | Test Loss: 0.330 | Test AUC: 0.957 | Rate: 5976 examples/s (9335.45 s) \n",
      "Epoch 97, Step 2656699 | Training Acc: 0.957 | Test Acc: 0.961 | Test Loss: 0.279 | Test AUC: 0.946 | Rate: 6099 examples/s (9354.58 s) \n",
      "Epoch 97, Step 3323323 | Training Acc: 0.947 | Test Acc: 0.965 | Test Loss: 0.214 | Test AUC: 0.942 | Rate: 5894 examples/s (9373.73 s) \n",
      "Epoch 98, Step 656726 | Training Acc: 0.963 | Test Acc: 0.961 | Test Loss: 0.196 | Test AUC: 0.956 | Rate: 6122 examples/s (9392.84 s) \n",
      "Epoch 98, Step 1323350 | Training Acc: 0.973 | Test Acc: 0.963 | Test Loss: 0.334 | Test AUC: 0.963 | Rate: 5947 examples/s (9411.93 s) \n",
      "Epoch 98, Step 1989974 | Training Acc: 0.984 | Test Acc: 0.959 | Test Loss: 0.277 | Test AUC: 0.965 | Rate: 6010 examples/s (9431.08 s) \n",
      "Epoch 98, Step 2656598 | Training Acc: 0.967 | Test Acc: 0.961 | Test Loss: 0.236 | Test AUC: 0.948 | Rate: 5978 examples/s (9450.22 s) \n",
      "Epoch 98, Step 3323222 | Training Acc: 0.955 | Test Acc: 0.947 | Test Loss: 0.391 | Test AUC: 0.940 | Rate: 5580 examples/s (9469.33 s) \n",
      "Epoch 99, Step 656625 | Training Acc: 0.959 | Test Acc: 0.969 | Test Loss: 0.218 | Test AUC: 0.957 | Rate: 6031 examples/s (9488.45 s) \n",
      "Epoch 99, Step 1323249 | Training Acc: 0.961 | Test Acc: 0.965 | Test Loss: 0.251 | Test AUC: 0.955 | Rate: 5955 examples/s (9507.57 s) \n",
      "Epoch 99, Step 1989873 | Training Acc: 0.967 | Test Acc: 0.936 | Test Loss: 0.400 | Test AUC: 0.941 | Rate: 5969 examples/s (9526.72 s) \n",
      "Epoch 99, Step 2656497 | Training Acc: 0.963 | Test Acc: 0.967 | Test Loss: 0.159 | Test AUC: 0.954 | Rate: 5880 examples/s (9545.87 s) \n",
      "Epoch 99, Step 3323121 | Training Acc: 0.959 | Test Acc: 0.957 | Test Loss: 0.230 | Test AUC: 0.949 | Rate: 5983 examples/s (9565.00 s) \n",
      "Epoch 100, Step 656524 | Training Acc: 0.959 | Test Acc: 0.953 | Test Loss: 0.321 | Test AUC: 0.940 | Rate: 5690 examples/s (9584.15 s) \n",
      "Epoch 100, Step 1323148 | Training Acc: 0.949 | Test Acc: 0.967 | Test Loss: 0.357 | Test AUC: 0.947 | Rate: 5970 examples/s (9603.26 s) \n",
      "Epoch 100, Step 1989772 | Training Acc: 0.969 | Test Acc: 0.959 | Test Loss: 0.155 | Test AUC: 0.955 | Rate: 5871 examples/s (9622.38 s) \n",
      "Epoch 100, Step 2656396 | Training Acc: 0.947 | Test Acc: 0.951 | Test Loss: 0.377 | Test AUC: 0.946 | Rate: 5942 examples/s (9641.51 s) \n",
      "Epoch 100, Step 3323020 | Training Acc: 0.957 | Test Acc: 0.957 | Test Loss: 0.328 | Test AUC: 0.947 | Rate: 5961 examples/s (9660.63 s) \n",
      "Epoch 101, Step 656423 | Training Acc: 0.957 | Test Acc: 0.967 | Test Loss: 0.349 | Test AUC: 0.958 | Rate: 5954 examples/s (9679.75 s) \n",
      "Epoch 101, Step 1323047 | Training Acc: 0.975 | Test Acc: 0.953 | Test Loss: 0.356 | Test AUC: 0.953 | Rate: 5969 examples/s (9698.87 s) \n",
      "Epoch 101, Step 1989671 | Training Acc: 0.953 | Test Acc: 0.959 | Test Loss: 0.209 | Test AUC: 0.940 | Rate: 5868 examples/s (9718.00 s) \n",
      "Epoch 101, Step 2656295 | Training Acc: 0.965 | Test Acc: 0.963 | Test Loss: 0.331 | Test AUC: 0.958 | Rate: 5870 examples/s (9737.14 s) \n",
      "Epoch 101, Step 3322919 | Training Acc: 0.953 | Test Acc: 0.969 | Test Loss: 0.341 | Test AUC: 0.958 | Rate: 5985 examples/s (9756.29 s) \n",
      "Epoch 102, Step 656322 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.376 | Test AUC: 0.950 | Rate: 5908 examples/s (9775.41 s) \n",
      "Epoch 102, Step 1322946 | Training Acc: 0.961 | Test Acc: 0.965 | Test Loss: 0.351 | Test AUC: 0.956 | Rate: 6016 examples/s (9794.53 s) \n",
      "Epoch 102, Step 1989570 | Training Acc: 0.967 | Test Acc: 0.951 | Test Loss: 0.235 | Test AUC: 0.951 | Rate: 5963 examples/s (9813.65 s) \n",
      "Epoch 102, Step 2656194 | Training Acc: 0.973 | Test Acc: 0.961 | Test Loss: 0.188 | Test AUC: 0.962 | Rate: 5931 examples/s (9832.79 s) \n",
      "Epoch 102, Step 3322818 | Training Acc: 0.973 | Test Acc: 0.955 | Test Loss: 0.198 | Test AUC: 0.958 | Rate: 5819 examples/s (9851.93 s) \n",
      "Epoch 103, Step 656221 | Training Acc: 0.965 | Test Acc: 0.943 | Test Loss: 0.528 | Test AUC: 0.933 | Rate: 5817 examples/s (9871.07 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch103.ckpt-103\n",
      "Epoch 103, Step 1322845 | Training Acc: 0.982 | Test Acc: 0.971 | Test Loss: 0.393 | Test AUC: 0.971 | Rate: 1393 examples/s (9890.46 s) [*]\n",
      "Epoch 103, Step 1989469 | Training Acc: 0.941 | Test Acc: 0.951 | Test Loss: 0.299 | Test AUC: 0.936 | Rate: 5881 examples/s (9909.61 s) \n",
      "Epoch 103, Step 2656093 | Training Acc: 0.961 | Test Acc: 0.961 | Test Loss: 0.478 | Test AUC: 0.956 | Rate: 5567 examples/s (9928.73 s) \n",
      "Epoch 103, Step 3322717 | Training Acc: 0.973 | Test Acc: 0.961 | Test Loss: 0.250 | Test AUC: 0.962 | Rate: 5793 examples/s (9947.85 s) \n",
      "Epoch 104, Step 656120 | Training Acc: 0.959 | Test Acc: 0.949 | Test Loss: 0.331 | Test AUC: 0.947 | Rate: 5829 examples/s (9966.97 s) \n",
      "Epoch 104, Step 1322744 | Training Acc: 0.973 | Test Acc: 0.961 | Test Loss: 0.207 | Test AUC: 0.957 | Rate: 5811 examples/s (9986.10 s) \n",
      "Epoch 104, Step 1989368 | Training Acc: 0.943 | Test Acc: 0.967 | Test Loss: 0.367 | Test AUC: 0.950 | Rate: 5772 examples/s (10005.23 s) \n",
      "Epoch 104, Step 2655992 | Training Acc: 0.953 | Test Acc: 0.961 | Test Loss: 0.223 | Test AUC: 0.948 | Rate: 5917 examples/s (10024.36 s) \n",
      "Epoch 104, Step 3322616 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.254 | Test AUC: 0.953 | Rate: 5931 examples/s (10043.47 s) \n",
      "Epoch 105, Step 656019 | Training Acc: 0.943 | Test Acc: 0.979 | Test Loss: 0.145 | Test AUC: 0.951 | Rate: 5885 examples/s (10062.62 s) \n",
      "Epoch 105, Step 1322643 | Training Acc: 0.973 | Test Acc: 0.945 | Test Loss: 0.428 | Test AUC: 0.952 | Rate: 5694 examples/s (10081.74 s) \n",
      "Epoch 105, Step 1989267 | Training Acc: 0.949 | Test Acc: 0.961 | Test Loss: 0.361 | Test AUC: 0.940 | Rate: 5929 examples/s (10100.83 s) \n",
      "Epoch 105, Step 2655891 | Training Acc: 0.965 | Test Acc: 0.969 | Test Loss: 0.141 | Test AUC: 0.964 | Rate: 5859 examples/s (10119.93 s) \n",
      "Epoch 105, Step 3322515 | Training Acc: 0.961 | Test Acc: 0.965 | Test Loss: 0.423 | Test AUC: 0.953 | Rate: 5924 examples/s (10139.06 s) \n",
      "Epoch 106, Step 655918 | Training Acc: 0.949 | Test Acc: 0.959 | Test Loss: 0.258 | Test AUC: 0.939 | Rate: 5867 examples/s (10158.16 s) \n",
      "Epoch 106, Step 1322542 | Training Acc: 0.955 | Test Acc: 0.959 | Test Loss: 0.243 | Test AUC: 0.952 | Rate: 5185 examples/s (10177.30 s) \n",
      "Epoch 106, Step 1989166 | Training Acc: 0.959 | Test Acc: 0.965 | Test Loss: 0.084 | Test AUC: 0.950 | Rate: 5861 examples/s (10196.41 s) \n",
      "Epoch 106, Step 2655790 | Training Acc: 0.961 | Test Acc: 0.957 | Test Loss: 0.242 | Test AUC: 0.952 | Rate: 5620 examples/s (10215.55 s) \n",
      "Epoch 106, Step 3322414 | Training Acc: 0.961 | Test Acc: 0.955 | Test Loss: 0.267 | Test AUC: 0.953 | Rate: 5880 examples/s (10234.65 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107, Step 655817 | Training Acc: 0.961 | Test Acc: 0.957 | Test Loss: 0.235 | Test AUC: 0.948 | Rate: 5884 examples/s (10253.80 s) \n",
      "Epoch 107, Step 1322441 | Training Acc: 0.965 | Test Acc: 0.951 | Test Loss: 0.459 | Test AUC: 0.949 | Rate: 5889 examples/s (10272.93 s) \n",
      "Epoch 107, Step 1989065 | Training Acc: 0.947 | Test Acc: 0.965 | Test Loss: 0.309 | Test AUC: 0.945 | Rate: 5875 examples/s (10292.05 s) \n",
      "Epoch 107, Step 2655689 | Training Acc: 0.975 | Test Acc: 0.947 | Test Loss: 0.283 | Test AUC: 0.949 | Rate: 5859 examples/s (10311.17 s) \n",
      "Epoch 107, Step 3322313 | Training Acc: 0.967 | Test Acc: 0.953 | Test Loss: 0.398 | Test AUC: 0.952 | Rate: 6037 examples/s (10330.26 s) \n",
      "Epoch 108, Step 655716 | Training Acc: 0.947 | Test Acc: 0.965 | Test Loss: 0.324 | Test AUC: 0.944 | Rate: 5777 examples/s (10349.39 s) \n",
      "Epoch 108, Step 1322340 | Training Acc: 0.975 | Test Acc: 0.956 | Test Loss: 0.336 | Test AUC: 0.964 | Rate: 6151 examples/s (10368.49 s) \n",
      "Epoch 108, Step 1988964 | Training Acc: 0.959 | Test Acc: 0.980 | Test Loss: 0.135 | Test AUC: 0.964 | Rate: 5179 examples/s (10387.62 s) \n",
      "Epoch 108, Step 2655588 | Training Acc: 0.969 | Test Acc: 0.951 | Test Loss: 0.335 | Test AUC: 0.948 | Rate: 6009 examples/s (10406.72 s) \n",
      "Epoch 108, Step 3322212 | Training Acc: 0.963 | Test Acc: 0.947 | Test Loss: 0.391 | Test AUC: 0.944 | Rate: 5857 examples/s (10425.82 s) \n",
      "Epoch 109, Step 655615 | Training Acc: 0.971 | Test Acc: 0.965 | Test Loss: 0.145 | Test AUC: 0.954 | Rate: 5764 examples/s (10444.97 s) \n",
      "Epoch 109, Step 1322239 | Training Acc: 0.973 | Test Acc: 0.963 | Test Loss: 0.178 | Test AUC: 0.959 | Rate: 5764 examples/s (10464.10 s) \n",
      "Epoch 109, Step 1988863 | Training Acc: 0.975 | Test Acc: 0.969 | Test Loss: 0.149 | Test AUC: 0.964 | Rate: 5593 examples/s (10483.23 s) \n",
      "Epoch 109, Step 2655487 | Training Acc: 0.965 | Test Acc: 0.969 | Test Loss: 0.174 | Test AUC: 0.963 | Rate: 5779 examples/s (10502.38 s) \n",
      "Epoch 109, Step 3322111 | Training Acc: 0.973 | Test Acc: 0.971 | Test Loss: 0.152 | Test AUC: 0.961 | Rate: 5166 examples/s (10521.53 s) \n",
      "Epoch 110, Step 655514 | Training Acc: 0.955 | Test Acc: 0.955 | Test Loss: 0.251 | Test AUC: 0.947 | Rate: 5676 examples/s (10540.62 s) \n",
      "Epoch 110, Step 1322138 | Training Acc: 0.965 | Test Acc: 0.955 | Test Loss: 0.205 | Test AUC: 0.944 | Rate: 5842 examples/s (10559.77 s) \n",
      "Epoch 110, Step 1988762 | Training Acc: 0.955 | Test Acc: 0.953 | Test Loss: 0.346 | Test AUC: 0.944 | Rate: 5893 examples/s (10578.91 s) \n",
      "Epoch 110, Step 2655386 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.151 | Test AUC: 0.953 | Rate: 5091 examples/s (10598.08 s) \n",
      "Epoch 110, Step 3322010 | Training Acc: 0.961 | Test Acc: 0.959 | Test Loss: 0.316 | Test AUC: 0.951 | Rate: 5998 examples/s (10617.23 s) \n",
      "Epoch 111, Step 655413 | Training Acc: 0.953 | Test Acc: 0.949 | Test Loss: 0.409 | Test AUC: 0.943 | Rate: 5752 examples/s (10636.38 s) \n",
      "Epoch 111, Step 1322037 | Training Acc: 0.957 | Test Acc: 0.947 | Test Loss: 0.243 | Test AUC: 0.944 | Rate: 5161 examples/s (10655.51 s) \n",
      "Epoch 111, Step 1988661 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.255 | Test AUC: 0.959 | Rate: 5867 examples/s (10674.61 s) \n",
      "Epoch 111, Step 2655285 | Training Acc: 0.961 | Test Acc: 0.957 | Test Loss: 0.162 | Test AUC: 0.950 | Rate: 5715 examples/s (10693.76 s) \n",
      "Epoch 111, Step 3321909 | Training Acc: 0.963 | Test Acc: 0.947 | Test Loss: 0.450 | Test AUC: 0.946 | Rate: 5819 examples/s (10712.90 s) \n",
      "Epoch 112, Step 655312 | Training Acc: 0.969 | Test Acc: 0.967 | Test Loss: 0.129 | Test AUC: 0.957 | Rate: 5883 examples/s (10732.03 s) \n",
      "Epoch 112, Step 1321936 | Training Acc: 0.969 | Test Acc: 0.957 | Test Loss: 0.401 | Test AUC: 0.955 | Rate: 5790 examples/s (10751.15 s) \n",
      "Epoch 112, Step 1988560 | Training Acc: 0.963 | Test Acc: 0.953 | Test Loss: 0.255 | Test AUC: 0.939 | Rate: 5609 examples/s (10770.28 s) \n",
      "Epoch 112, Step 2655184 | Training Acc: 0.965 | Test Acc: 0.963 | Test Loss: 0.335 | Test AUC: 0.951 | Rate: 5617 examples/s (10789.30 s) \n",
      "Epoch 112, Step 3321808 | Training Acc: 0.961 | Test Acc: 0.943 | Test Loss: 0.292 | Test AUC: 0.938 | Rate: 5436 examples/s (10808.47 s) \n",
      "Epoch 113, Step 655211 | Training Acc: 0.949 | Test Acc: 0.967 | Test Loss: 0.135 | Test AUC: 0.950 | Rate: 5760 examples/s (10827.61 s) \n",
      "Epoch 113, Step 1321835 | Training Acc: 0.953 | Test Acc: 0.963 | Test Loss: 0.090 | Test AUC: 0.943 | Rate: 5387 examples/s (10846.74 s) \n",
      "Epoch 113, Step 1988459 | Training Acc: 0.961 | Test Acc: 0.943 | Test Loss: 0.491 | Test AUC: 0.949 | Rate: 5574 examples/s (10865.87 s) \n",
      "Epoch 113, Step 2655083 | Training Acc: 0.977 | Test Acc: 0.951 | Test Loss: 0.180 | Test AUC: 0.952 | Rate: 5834 examples/s (10884.99 s) \n",
      "Epoch 113, Step 3321707 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.438 | Test AUC: 0.940 | Rate: 5588 examples/s (10904.15 s) \n",
      "Epoch 114, Step 655110 | Training Acc: 0.939 | Test Acc: 0.957 | Test Loss: 0.298 | Test AUC: 0.939 | Rate: 5738 examples/s (10923.28 s) \n",
      "Epoch 114, Step 1321734 | Training Acc: 0.949 | Test Acc: 0.977 | Test Loss: 0.077 | Test AUC: 0.953 | Rate: 6046 examples/s (10942.42 s) \n",
      "Epoch 114, Step 1988358 | Training Acc: 0.951 | Test Acc: 0.959 | Test Loss: 0.253 | Test AUC: 0.943 | Rate: 5879 examples/s (10961.58 s) \n",
      "Epoch 114, Step 2654982 | Training Acc: 0.967 | Test Acc: 0.963 | Test Loss: 0.408 | Test AUC: 0.957 | Rate: 5830 examples/s (10980.69 s) \n",
      "Epoch 114, Step 3321606 | Training Acc: 0.963 | Test Acc: 0.955 | Test Loss: 0.196 | Test AUC: 0.948 | Rate: 5100 examples/s (10999.83 s) \n",
      "Epoch 115, Step 655009 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.178 | Test AUC: 0.949 | Rate: 5784 examples/s (11018.92 s) \n",
      "Epoch 115, Step 1321633 | Training Acc: 0.949 | Test Acc: 0.943 | Test Loss: 0.367 | Test AUC: 0.933 | Rate: 5882 examples/s (11038.07 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch115.ckpt-115\n",
      "Epoch 115, Step 1988257 | Training Acc: 0.975 | Test Acc: 0.979 | Test Loss: 0.168 | Test AUC: 0.972 | Rate: 1304 examples/s (11057.52 s) [*]\n",
      "Epoch 115, Step 2654881 | Training Acc: 0.969 | Test Acc: 0.947 | Test Loss: 0.356 | Test AUC: 0.944 | Rate: 5794 examples/s (11076.65 s) \n",
      "Epoch 115, Step 3321505 | Training Acc: 0.965 | Test Acc: 0.973 | Test Loss: 0.179 | Test AUC: 0.961 | Rate: 5814 examples/s (11095.81 s) \n",
      "Epoch 116, Step 654908 | Training Acc: 0.963 | Test Acc: 0.969 | Test Loss: 0.280 | Test AUC: 0.953 | Rate: 5759 examples/s (11114.93 s) \n",
      "Epoch 116, Step 1321532 | Training Acc: 0.953 | Test Acc: 0.967 | Test Loss: 0.202 | Test AUC: 0.954 | Rate: 5939 examples/s (11134.04 s) \n",
      "Epoch 116, Step 1988156 | Training Acc: 0.953 | Test Acc: 0.971 | Test Loss: 0.205 | Test AUC: 0.952 | Rate: 5678 examples/s (11153.20 s) \n",
      "Epoch 116, Step 2654780 | Training Acc: 0.963 | Test Acc: 0.967 | Test Loss: 0.296 | Test AUC: 0.960 | Rate: 5805 examples/s (11172.37 s) \n",
      "Epoch 116, Step 3321404 | Training Acc: 0.955 | Test Acc: 0.969 | Test Loss: 0.242 | Test AUC: 0.958 | Rate: 5798 examples/s (11191.53 s) \n",
      "Epoch 117, Step 654807 | Training Acc: 0.955 | Test Acc: 0.961 | Test Loss: 0.261 | Test AUC: 0.951 | Rate: 5642 examples/s (11210.65 s) \n",
      "Epoch 117, Step 1321431 | Training Acc: 0.965 | Test Acc: 0.971 | Test Loss: 0.157 | Test AUC: 0.959 | Rate: 5775 examples/s (11229.78 s) \n",
      "Epoch 117, Step 1988055 | Training Acc: 0.969 | Test Acc: 0.969 | Test Loss: 0.338 | Test AUC: 0.964 | Rate: 5749 examples/s (11248.91 s) \n",
      "Epoch 117, Step 2654679 | Training Acc: 0.943 | Test Acc: 0.953 | Test Loss: 0.259 | Test AUC: 0.945 | Rate: 5788 examples/s (11268.06 s) \n",
      "Epoch 117, Step 3321303 | Training Acc: 0.967 | Test Acc: 0.965 | Test Loss: 0.279 | Test AUC: 0.951 | Rate: 5760 examples/s (11287.20 s) \n",
      "Epoch 118, Step 654706 | Training Acc: 0.945 | Test Acc: 0.953 | Test Loss: 0.407 | Test AUC: 0.937 | Rate: 5763 examples/s (11306.30 s) \n",
      "Epoch 118, Step 1321330 | Training Acc: 0.953 | Test Acc: 0.963 | Test Loss: 0.188 | Test AUC: 0.953 | Rate: 5669 examples/s (11325.36 s) \n",
      "Epoch 118, Step 1987954 | Training Acc: 0.961 | Test Acc: 0.953 | Test Loss: 0.431 | Test AUC: 0.945 | Rate: 5719 examples/s (11344.50 s) \n",
      "Epoch 118, Step 2654578 | Training Acc: 0.967 | Test Acc: 0.967 | Test Loss: 0.309 | Test AUC: 0.960 | Rate: 5729 examples/s (11363.64 s) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118, Step 3321202 | Training Acc: 0.947 | Test Acc: 0.965 | Test Loss: 0.168 | Test AUC: 0.946 | Rate: 5744 examples/s (11382.78 s) \n",
      "Epoch 119, Step 654605 | Training Acc: 0.965 | Test Acc: 0.969 | Test Loss: 0.200 | Test AUC: 0.961 | Rate: 5810 examples/s (11401.91 s) \n",
      "Epoch 119, Step 1321229 | Training Acc: 0.971 | Test Acc: 0.971 | Test Loss: 0.271 | Test AUC: 0.960 | Rate: 5806 examples/s (11421.07 s) \n",
      "Epoch 119, Step 1987853 | Training Acc: 0.949 | Test Acc: 0.986 | Test Loss: 0.100 | Test AUC: 0.959 | Rate: 5729 examples/s (11440.21 s) \n",
      "Epoch 119, Step 2654477 | Training Acc: 0.961 | Test Acc: 0.971 | Test Loss: 0.287 | Test AUC: 0.961 | Rate: 5674 examples/s (11459.34 s) \n",
      "Epoch 119, Step 3321101 | Training Acc: 0.949 | Test Acc: 0.955 | Test Loss: 0.190 | Test AUC: 0.940 | Rate: 5623 examples/s (11478.49 s) \n",
      "Epoch 120, Step 654504 | Training Acc: 0.953 | Test Acc: 0.945 | Test Loss: 0.219 | Test AUC: 0.932 | Rate: 5730 examples/s (11497.62 s) \n",
      "Epoch 120, Step 1321128 | Training Acc: 0.957 | Test Acc: 0.969 | Test Loss: 0.325 | Test AUC: 0.952 | Rate: 5909 examples/s (11516.77 s) \n",
      "Epoch 120, Step 1987752 | Training Acc: 0.969 | Test Acc: 0.967 | Test Loss: 0.285 | Test AUC: 0.963 | Rate: 5707 examples/s (11535.93 s) \n",
      "Epoch 120, Step 2654376 | Training Acc: 0.961 | Test Acc: 0.959 | Test Loss: 0.359 | Test AUC: 0.953 | Rate: 5789 examples/s (11555.09 s) \n",
      "Epoch 120, Step 3321000 | Training Acc: 0.961 | Test Acc: 0.971 | Test Loss: 0.289 | Test AUC: 0.959 | Rate: 5502 examples/s (11574.25 s) \n",
      "Epoch 121, Step 654403 | Training Acc: 0.953 | Test Acc: 0.959 | Test Loss: 0.156 | Test AUC: 0.939 | Rate: 5643 examples/s (11593.37 s) \n",
      "Epoch 121, Step 1321027 | Training Acc: 0.965 | Test Acc: 0.955 | Test Loss: 0.311 | Test AUC: 0.949 | Rate: 5868 examples/s (11612.52 s) \n",
      "Epoch 121, Step 1987651 | Training Acc: 0.965 | Test Acc: 0.955 | Test Loss: 0.188 | Test AUC: 0.950 | Rate: 5746 examples/s (11631.66 s) \n",
      "Graph saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_epoch121.ckpt-121\n",
      "Epoch 121, Step 2654275 | Training Acc: 0.980 | Test Acc: 0.977 | Test Loss: 0.116 | Test AUC: 0.973 | Rate: 1048 examples/s (11651.18 s) [*]\n",
      "Epoch 121, Step 3320899 | Training Acc: 0.949 | Test Acc: 0.973 | Test Loss: 0.161 | Test AUC: 0.950 | Rate: 5756 examples/s (11670.31 s) \n",
      "Epoch 122, Step 654302 | Training Acc: 0.949 | Test Acc: 0.961 | Test Loss: 0.133 | Test AUC: 0.947 | Rate: 5749 examples/s (11689.44 s) \n",
      "Epoch 122, Step 1320926 | Training Acc: 0.963 | Test Acc: 0.982 | Test Loss: 0.190 | Test AUC: 0.971 | Rate: 5799 examples/s (11708.59 s) \n",
      "Epoch 122, Step 1987550 | Training Acc: 0.957 | Test Acc: 0.967 | Test Loss: 0.261 | Test AUC: 0.950 | Rate: 5794 examples/s (11727.73 s) \n",
      "Epoch 122, Step 2654174 | Training Acc: 0.955 | Test Acc: 0.938 | Test Loss: 0.454 | Test AUC: 0.928 | Rate: 5642 examples/s (11746.84 s) \n",
      "Epoch 122, Step 3320798 | Training Acc: 0.953 | Test Acc: 0.947 | Test Loss: 0.366 | Test AUC: 0.935 | Rate: 5633 examples/s (11765.97 s) \n",
      "Epoch 123, Step 654201 | Training Acc: 0.965 | Test Acc: 0.963 | Test Loss: 0.266 | Test AUC: 0.960 | Rate: 5673 examples/s (11785.11 s) \n",
      "Epoch 123, Step 1320825 | Training Acc: 0.955 | Test Acc: 0.951 | Test Loss: 0.443 | Test AUC: 0.936 | Rate: 5676 examples/s (11804.24 s) \n",
      "Epoch 123, Step 1987449 | Training Acc: 0.959 | Test Acc: 0.951 | Test Loss: 0.303 | Test AUC: 0.944 | Rate: 5725 examples/s (11823.40 s) \n",
      "Epoch 123, Step 2654073 | Training Acc: 0.977 | Test Acc: 0.957 | Test Loss: 0.215 | Test AUC: 0.954 | Rate: 5712 examples/s (11842.53 s) \n",
      "Epoch 123, Step 3320697 | Training Acc: 0.963 | Test Acc: 0.967 | Test Loss: 0.389 | Test AUC: 0.951 | Rate: 5783 examples/s (11861.68 s) \n",
      "Epoch 124, Step 654100 | Training Acc: 0.971 | Test Acc: 0.945 | Test Loss: 0.465 | Test AUC: 0.953 | Rate: 5628 examples/s (11880.85 s) \n",
      "Epoch 124, Step 1320724 | Training Acc: 0.969 | Test Acc: 0.961 | Test Loss: 0.221 | Test AUC: 0.960 | Rate: 5704 examples/s (11899.99 s) \n",
      "Epoch 124, Step 1987348 | Training Acc: 0.973 | Test Acc: 0.971 | Test Loss: 0.189 | Test AUC: 0.966 | Rate: 5816 examples/s (11919.13 s) \n",
      "Epoch 124, Step 2653972 | Training Acc: 0.955 | Test Acc: 0.957 | Test Loss: 0.198 | Test AUC: 0.941 | Rate: 5763 examples/s (11938.25 s) \n",
      "Epoch 124, Step 3320596 | Training Acc: 0.963 | Test Acc: 0.959 | Test Loss: 0.457 | Test AUC: 0.951 | Rate: 5845 examples/s (11957.37 s) \n",
      "Epoch 125, Step 653999 | Training Acc: 0.959 | Test Acc: 0.967 | Test Loss: 0.136 | Test AUC: 0.961 | Rate: 5726 examples/s (11976.51 s) \n",
      "Epoch 125, Step 1320623 | Training Acc: 0.947 | Test Acc: 0.959 | Test Loss: 0.199 | Test AUC: 0.946 | Rate: 5655 examples/s (11995.66 s) \n",
      "Epoch 125, Step 1987247 | Training Acc: 0.971 | Test Acc: 0.961 | Test Loss: 0.171 | Test AUC: 0.960 | Rate: 5636 examples/s (12014.80 s) \n",
      "Epoch 125, Step 2653871 | Training Acc: 0.967 | Test Acc: 0.959 | Test Loss: 0.380 | Test AUC: 0.958 | Rate: 5594 examples/s (12033.95 s) \n",
      "Epoch 125, Step 3320495 | Training Acc: 0.969 | Test Acc: 0.965 | Test Loss: 0.430 | Test AUC: 0.961 | Rate: 5654 examples/s (12053.11 s) \n",
      "Epoch 126, Step 653898 | Training Acc: 0.951 | Test Acc: 0.963 | Test Loss: 0.147 | Test AUC: 0.934 | Rate: 5662 examples/s (12072.25 s) \n",
      "Epoch 126, Step 1320522 | Training Acc: 0.969 | Test Acc: 0.955 | Test Loss: 0.444 | Test AUC: 0.944 | Rate: 5666 examples/s (12091.41 s) \n",
      "Epoch 126, Step 1987146 | Training Acc: 0.963 | Test Acc: 0.963 | Test Loss: 0.190 | Test AUC: 0.956 | Rate: 5650 examples/s (12110.56 s) \n",
      "Epoch 126, Step 2653770 | Training Acc: 0.955 | Test Acc: 0.967 | Test Loss: 0.163 | Test AUC: 0.948 | Rate: 5585 examples/s (12129.71 s) \n",
      "Epoch 126, Step 3320394 | Training Acc: 0.957 | Test Acc: 0.949 | Test Loss: 0.327 | Test AUC: 0.941 | Rate: 5609 examples/s (12148.83 s) \n",
      "Epoch 127, Step 653797 | Training Acc: 0.959 | Test Acc: 0.957 | Test Loss: 0.300 | Test AUC: 0.944 | Rate: 5477 examples/s (12167.99 s) \n",
      "Epoch 127, Step 1320421 | Training Acc: 0.959 | Test Acc: 0.955 | Test Loss: 0.343 | Test AUC: 0.953 | Rate: 5814 examples/s (12187.10 s) \n",
      "Epoch 127, Step 1987045 | Training Acc: 0.967 | Test Acc: 0.967 | Test Loss: 0.206 | Test AUC: 0.958 | Rate: 5713 examples/s (12206.25 s) \n",
      "Epoch 127, Step 2653669 | Training Acc: 0.945 | Test Acc: 0.957 | Test Loss: 0.208 | Test AUC: 0.940 | Rate: 5641 examples/s (12225.38 s) \n",
      "Training Complete. Model saved to file: cont_b2dy/vDNN_cont-selu_B2Xdy_end.ckpt-127 Time elapsed: 12237.225 s\n"
     ]
    }
   ],
   "source": [
    "train(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making Predictions\n",
    "Classification on a new instance is given by the softmax of the output of the final readout layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/vDNN_continuum-selu_B2Xdy_end.ckpt-7\n",
      "checkpoints/vDNN_continuum-selu_B2Xdy_end.ckpt-7 restored.\n",
      "Validation accuracy: 0.929\n",
      "AUC: 0.9885861509236746\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAEqCAYAAAA4WQj3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b71d5ded470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting signal efficiency versus background rejection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b72fbadd080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe0AAAEqCAYAAAA4WQj3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b72fbae46a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Duration: 9.03982 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "ckpt = tf.train.get_checkpoint_state(directories.checkpoints)\n",
    "vDNN = vanillaDNN(config, training = False)\n",
    "labels, preds, output = vDNN.predict(ckpt)\n",
    "\n",
    "# Add predictions to test set as a new column, save as HDF5\n",
    "output = pd.Series(output, name='probs')\n",
    "test = pd.concat([pdf1, output], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         9.345390e-01\n",
       "1         1.000000e+00\n",
       "2         0.000000e+00\n",
       "3         0.000000e+00\n",
       "4         8.583705e-35\n",
       "5         0.000000e+00\n",
       "6         0.000000e+00\n",
       "7         1.000000e+00\n",
       "8         0.000000e+00\n",
       "9         6.897035e-22\n",
       "10        1.000000e+00\n",
       "11        9.314652e-01\n",
       "12        1.000000e+00\n",
       "13        1.000000e+00\n",
       "14        1.000000e+00\n",
       "15        2.619639e-07\n",
       "16        1.101381e-32\n",
       "17        9.320296e-20\n",
       "18        1.651990e-04\n",
       "19        1.000000e+00\n",
       "20        1.000000e+00\n",
       "21        1.000000e+00\n",
       "22        0.000000e+00\n",
       "23        3.899324e-35\n",
       "24        1.641836e-16\n",
       "25        1.000000e+00\n",
       "26        1.000000e+00\n",
       "27        1.000000e+00\n",
       "28        6.911855e-01\n",
       "29        1.000000e+00\n",
       "              ...     \n",
       "138278    1.000000e+00\n",
       "138279    1.000000e+00\n",
       "138280    9.999439e-01\n",
       "138281    1.000000e+00\n",
       "138282    1.000000e+00\n",
       "138283    9.890485e-05\n",
       "138284    1.000000e+00\n",
       "138285    1.000000e+00\n",
       "138286    1.000000e+00\n",
       "138287    1.000000e+00\n",
       "138288    1.000000e+00\n",
       "138289    1.000000e+00\n",
       "138290    1.000000e+00\n",
       "138291    1.000000e+00\n",
       "138292    1.000000e+00\n",
       "138293    0.000000e+00\n",
       "138294    0.000000e+00\n",
       "138295    1.000000e+00\n",
       "138296    9.999981e-01\n",
       "138297    1.000000e+00\n",
       "138298    0.000000e+00\n",
       "138299    1.000000e+00\n",
       "138300    1.000000e+00\n",
       "138301    1.000000e+00\n",
       "138302    1.000000e+00\n",
       "138303    4.118808e-11\n",
       "138304    1.000000e+00\n",
       "138305    1.000000e+00\n",
       "138306    1.000000e+00\n",
       "138307    3.470825e-05\n",
       "Name: probs, Length: 138308, dtype: float32"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347319, 108)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(347319, 107)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.000000e+00\n",
       "1         1.000000e+00\n",
       "2         9.827306e-01\n",
       "3         1.000000e+00\n",
       "4         1.000000e+00\n",
       "5         1.000000e+00\n",
       "6         1.000000e+00\n",
       "7         1.000000e+00\n",
       "8         1.065946e-10\n",
       "9         9.526831e-01\n",
       "10        9.994755e-01\n",
       "11        1.000000e+00\n",
       "12        0.000000e+00\n",
       "13        1.275826e-33\n",
       "14        9.961402e-01\n",
       "15        9.999176e-01\n",
       "16        9.068767e-01\n",
       "17        3.129100e-03\n",
       "18        1.000000e+00\n",
       "19        9.999944e-01\n",
       "20        1.000000e+00\n",
       "21        1.000000e+00\n",
       "22        1.000000e+00\n",
       "23        1.000000e+00\n",
       "24        3.955720e-33\n",
       "25        9.902784e-01\n",
       "26        1.000000e+00\n",
       "27        9.997016e-01\n",
       "28        1.000000e+00\n",
       "29        0.000000e+00\n",
       "              ...     \n",
       "347289    1.000000e+00\n",
       "347290    9.946324e-01\n",
       "347291    8.464706e-17\n",
       "347292    1.000000e+00\n",
       "347293    2.609681e-16\n",
       "347294    1.000000e+00\n",
       "347295    1.000000e+00\n",
       "347296    9.960432e-01\n",
       "347297    1.000000e+00\n",
       "347298    0.000000e+00\n",
       "347299    1.000000e+00\n",
       "347300    9.977423e-01\n",
       "347301    1.000000e+00\n",
       "347302    1.000000e+00\n",
       "347303    9.997661e-01\n",
       "347304    1.000000e+00\n",
       "347305    3.874876e-18\n",
       "347306    0.000000e+00\n",
       "347307    1.000000e+00\n",
       "347308    0.000000e+00\n",
       "347309    5.732486e-26\n",
       "347310    9.291208e-01\n",
       "347311    2.691619e-23\n",
       "347312    9.968556e-01\n",
       "347313    3.592745e-13\n",
       "347314    8.560661e-10\n",
       "347315    1.000000e+00\n",
       "347316    9.916703e-01\n",
       "347317    1.000000e+00\n",
       "347318    0.000000e+00\n",
       "Name: probs, Length: 347319, dtype: float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BB_gamma_hel</th>\n",
       "      <th>B_CosTBTO</th>\n",
       "      <th>B_CosTBz</th>\n",
       "      <th>B_DeltaT</th>\n",
       "      <th>B_R2</th>\n",
       "      <th>B_TagVPvalue</th>\n",
       "      <th>B_TagVex</th>\n",
       "      <th>B_TagVey</th>\n",
       "      <th>B_TagVez</th>\n",
       "      <th>B_TagVx</th>\n",
       "      <th>...</th>\n",
       "      <th>B_useCMSFrame_bodaughterSumOf_bopz_bc_bc</th>\n",
       "      <th>mbc</th>\n",
       "      <th>deltae</th>\n",
       "      <th>daughterInvM</th>\n",
       "      <th>evtNum</th>\n",
       "      <th>MCtype</th>\n",
       "      <th>nCands</th>\n",
       "      <th>channel</th>\n",
       "      <th>labels</th>\n",
       "      <th>probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.588105</td>\n",
       "      <td>-0.566125</td>\n",
       "      <td>2.060708</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>-1.170962</td>\n",
       "      <td>1.273394</td>\n",
       "      <td>-0.012045</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>-0.012057</td>\n",
       "      <td>-0.005894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024435</td>\n",
       "      <td>5.277649</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>1.657435</td>\n",
       "      <td>3.249500e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.583511</td>\n",
       "      <td>0.326212</td>\n",
       "      <td>1.965546</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>-0.139086</td>\n",
       "      <td>-0.987356</td>\n",
       "      <td>-0.012026</td>\n",
       "      <td>0.042042</td>\n",
       "      <td>-0.012054</td>\n",
       "      <td>0.023391</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.396096</td>\n",
       "      <td>5.280044</td>\n",
       "      <td>0.024862</td>\n",
       "      <td>1.672550</td>\n",
       "      <td>4.530700e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.582909</td>\n",
       "      <td>-1.306332</td>\n",
       "      <td>2.205411</td>\n",
       "      <td>0.021708</td>\n",
       "      <td>-1.382867</td>\n",
       "      <td>-0.158151</td>\n",
       "      <td>-0.011995</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011983</td>\n",
       "      <td>-0.031475</td>\n",
       "      <td>...</td>\n",
       "      <td>1.046946</td>\n",
       "      <td>5.279857</td>\n",
       "      <td>-0.106811</td>\n",
       "      <td>2.220620</td>\n",
       "      <td>1.386191e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.827306e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.581905</td>\n",
       "      <td>-0.050490</td>\n",
       "      <td>1.774217</td>\n",
       "      <td>-0.000200</td>\n",
       "      <td>0.090453</td>\n",
       "      <td>-1.093808</td>\n",
       "      <td>-0.011944</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011924</td>\n",
       "      <td>-0.002600</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.155154</td>\n",
       "      <td>5.272421</td>\n",
       "      <td>-0.122822</td>\n",
       "      <td>2.332373</td>\n",
       "      <td>2.621646e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.575761</td>\n",
       "      <td>-0.710063</td>\n",
       "      <td>1.977314</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>-1.013066</td>\n",
       "      <td>0.622739</td>\n",
       "      <td>-0.012038</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.012050</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.647386</td>\n",
       "      <td>5.281581</td>\n",
       "      <td>0.055017</td>\n",
       "      <td>2.082395</td>\n",
       "      <td>7.297000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.574043</td>\n",
       "      <td>0.709838</td>\n",
       "      <td>1.887089</td>\n",
       "      <td>0.022262</td>\n",
       "      <td>-0.304458</td>\n",
       "      <td>0.978461</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>-0.012022</td>\n",
       "      <td>-0.001381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.462639</td>\n",
       "      <td>5.273503</td>\n",
       "      <td>-0.166270</td>\n",
       "      <td>2.049185</td>\n",
       "      <td>3.964600e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-1.573475</td>\n",
       "      <td>-0.185380</td>\n",
       "      <td>2.025063</td>\n",
       "      <td>0.011435</td>\n",
       "      <td>-0.784249</td>\n",
       "      <td>1.865040</td>\n",
       "      <td>-0.012011</td>\n",
       "      <td>0.042205</td>\n",
       "      <td>-0.012009</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.210416</td>\n",
       "      <td>5.275576</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>1.822210</td>\n",
       "      <td>1.213567e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.569883</td>\n",
       "      <td>-1.009027</td>\n",
       "      <td>2.177724</td>\n",
       "      <td>0.011549</td>\n",
       "      <td>-0.807890</td>\n",
       "      <td>1.904731</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>0.042192</td>\n",
       "      <td>-0.012047</td>\n",
       "      <td>-0.019559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.878339</td>\n",
       "      <td>5.280567</td>\n",
       "      <td>0.179423</td>\n",
       "      <td>1.994851</td>\n",
       "      <td>2.481396e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.569604</td>\n",
       "      <td>-0.014816</td>\n",
       "      <td>2.004494</td>\n",
       "      <td>0.047200</td>\n",
       "      <td>-0.350407</td>\n",
       "      <td>-0.201612</td>\n",
       "      <td>-0.012022</td>\n",
       "      <td>0.042195</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>-0.017455</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.578034</td>\n",
       "      <td>5.274578</td>\n",
       "      <td>-0.108732</td>\n",
       "      <td>0.953385</td>\n",
       "      <td>6.900000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.065946e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.569481</td>\n",
       "      <td>-1.954512</td>\n",
       "      <td>2.171281</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-1.450575</td>\n",
       "      <td>-1.119552</td>\n",
       "      <td>-0.012044</td>\n",
       "      <td>0.042205</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>-0.001675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627990</td>\n",
       "      <td>5.277041</td>\n",
       "      <td>0.085580</td>\n",
       "      <td>1.227704</td>\n",
       "      <td>3.599200e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.526831e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-1.569182</td>\n",
       "      <td>1.151153</td>\n",
       "      <td>1.900612</td>\n",
       "      <td>0.012091</td>\n",
       "      <td>0.203661</td>\n",
       "      <td>-1.112279</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>-0.012007</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.322810</td>\n",
       "      <td>5.282043</td>\n",
       "      <td>-0.135296</td>\n",
       "      <td>1.479965</td>\n",
       "      <td>9.190214e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.994755e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-1.568435</td>\n",
       "      <td>-0.565598</td>\n",
       "      <td>1.992011</td>\n",
       "      <td>0.028741</td>\n",
       "      <td>0.130616</td>\n",
       "      <td>-0.521075</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>0.042117</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>-0.004857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.929582</td>\n",
       "      <td>5.274243</td>\n",
       "      <td>-0.003875</td>\n",
       "      <td>1.013377</td>\n",
       "      <td>3.161500e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.567715</td>\n",
       "      <td>1.239812</td>\n",
       "      <td>1.800736</td>\n",
       "      <td>0.015411</td>\n",
       "      <td>1.816450</td>\n",
       "      <td>0.289027</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.042169</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>...</td>\n",
       "      <td>0.155492</td>\n",
       "      <td>5.284900</td>\n",
       "      <td>-0.069414</td>\n",
       "      <td>1.761511</td>\n",
       "      <td>4.859773e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.567501</td>\n",
       "      <td>-0.476724</td>\n",
       "      <td>1.985347</td>\n",
       "      <td>0.010538</td>\n",
       "      <td>-2.046603</td>\n",
       "      <td>0.424110</td>\n",
       "      <td>-0.012028</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.003214</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.408360</td>\n",
       "      <td>5.272332</td>\n",
       "      <td>-0.029731</td>\n",
       "      <td>2.272628</td>\n",
       "      <td>3.696101e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.275826e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-1.566319</td>\n",
       "      <td>0.031546</td>\n",
       "      <td>1.854112</td>\n",
       "      <td>0.044732</td>\n",
       "      <td>-1.219332</td>\n",
       "      <td>-0.360032</td>\n",
       "      <td>-0.012039</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.012048</td>\n",
       "      <td>0.032568</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.457383</td>\n",
       "      <td>5.275331</td>\n",
       "      <td>0.013524</td>\n",
       "      <td>1.213096</td>\n",
       "      <td>8.118000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.961402e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.564832</td>\n",
       "      <td>-1.970770</td>\n",
       "      <td>2.149373</td>\n",
       "      <td>0.027442</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>-0.513775</td>\n",
       "      <td>-0.012014</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.032058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.099204</td>\n",
       "      <td>5.284764</td>\n",
       "      <td>0.156094</td>\n",
       "      <td>0.959191</td>\n",
       "      <td>1.128291e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.999176e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.563681</td>\n",
       "      <td>-0.660002</td>\n",
       "      <td>1.922876</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>-0.536715</td>\n",
       "      <td>-0.790237</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.042208</td>\n",
       "      <td>-0.011999</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.295934</td>\n",
       "      <td>5.285688</td>\n",
       "      <td>0.105284</td>\n",
       "      <td>1.741828</td>\n",
       "      <td>2.034600e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.068767e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.563292</td>\n",
       "      <td>0.557182</td>\n",
       "      <td>2.007596</td>\n",
       "      <td>-0.016845</td>\n",
       "      <td>0.846171</td>\n",
       "      <td>-0.835290</td>\n",
       "      <td>-0.012024</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>-0.012029</td>\n",
       "      <td>0.027877</td>\n",
       "      <td>...</td>\n",
       "      <td>1.331109</td>\n",
       "      <td>5.273226</td>\n",
       "      <td>-0.052433</td>\n",
       "      <td>2.030820</td>\n",
       "      <td>1.680045e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.129100e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-1.562395</td>\n",
       "      <td>-1.927134</td>\n",
       "      <td>2.147949</td>\n",
       "      <td>0.027548</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>-0.513775</td>\n",
       "      <td>-0.012014</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.012010</td>\n",
       "      <td>-0.032058</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.245328</td>\n",
       "      <td>5.283813</td>\n",
       "      <td>0.170637</td>\n",
       "      <td>0.922816</td>\n",
       "      <td>1.128291e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.561559</td>\n",
       "      <td>-1.420410</td>\n",
       "      <td>1.757970</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>-0.599403</td>\n",
       "      <td>0.007394</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.042138</td>\n",
       "      <td>-0.012042</td>\n",
       "      <td>0.012916</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.559707</td>\n",
       "      <td>5.280505</td>\n",
       "      <td>-0.167871</td>\n",
       "      <td>1.622055</td>\n",
       "      <td>2.512200e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.999944e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.560382</td>\n",
       "      <td>0.954287</td>\n",
       "      <td>1.845459</td>\n",
       "      <td>0.010007</td>\n",
       "      <td>1.672864</td>\n",
       "      <td>1.046969</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>0.042210</td>\n",
       "      <td>-0.012027</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.808633</td>\n",
       "      <td>5.274035</td>\n",
       "      <td>-0.003709</td>\n",
       "      <td>0.851073</td>\n",
       "      <td>4.347667e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.560270</td>\n",
       "      <td>0.883145</td>\n",
       "      <td>2.025181</td>\n",
       "      <td>0.011548</td>\n",
       "      <td>0.610251</td>\n",
       "      <td>-0.459426</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.042189</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>-0.007102</td>\n",
       "      <td>...</td>\n",
       "      <td>0.609037</td>\n",
       "      <td>5.280972</td>\n",
       "      <td>0.126612</td>\n",
       "      <td>2.361797</td>\n",
       "      <td>9.870547e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-1.560108</td>\n",
       "      <td>1.292310</td>\n",
       "      <td>1.941228</td>\n",
       "      <td>0.022710</td>\n",
       "      <td>1.175870</td>\n",
       "      <td>-0.532663</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>-0.011994</td>\n",
       "      <td>-0.008904</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.305296</td>\n",
       "      <td>5.275794</td>\n",
       "      <td>0.120669</td>\n",
       "      <td>2.280151</td>\n",
       "      <td>2.564789e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-1.556034</td>\n",
       "      <td>-0.581448</td>\n",
       "      <td>1.814682</td>\n",
       "      <td>0.011301</td>\n",
       "      <td>0.705363</td>\n",
       "      <td>1.948656</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.012017</td>\n",
       "      <td>-0.011938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.360283</td>\n",
       "      <td>5.279708</td>\n",
       "      <td>0.073355</td>\n",
       "      <td>1.398857</td>\n",
       "      <td>3.519000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-1.556008</td>\n",
       "      <td>-1.944766</td>\n",
       "      <td>1.977108</td>\n",
       "      <td>0.005731</td>\n",
       "      <td>-1.118724</td>\n",
       "      <td>-1.038394</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>0.042210</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>0.029229</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>5.280077</td>\n",
       "      <td>-0.038192</td>\n",
       "      <td>1.571976</td>\n",
       "      <td>2.417300e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.955720e-33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-1.555477</td>\n",
       "      <td>0.947163</td>\n",
       "      <td>1.692409</td>\n",
       "      <td>0.003611</td>\n",
       "      <td>0.469500</td>\n",
       "      <td>-0.688393</td>\n",
       "      <td>-0.011810</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011778</td>\n",
       "      <td>-0.004983</td>\n",
       "      <td>...</td>\n",
       "      <td>1.511616</td>\n",
       "      <td>5.270384</td>\n",
       "      <td>-0.094270</td>\n",
       "      <td>2.044542</td>\n",
       "      <td>2.665656e+08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.902784e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.554658</td>\n",
       "      <td>1.301108</td>\n",
       "      <td>2.126952</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>0.142358</td>\n",
       "      <td>1.879190</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011998</td>\n",
       "      <td>-0.007538</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.513486</td>\n",
       "      <td>5.272954</td>\n",
       "      <td>-0.186648</td>\n",
       "      <td>2.162750</td>\n",
       "      <td>2.496540e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>-1.554489</td>\n",
       "      <td>0.541929</td>\n",
       "      <td>2.085999</td>\n",
       "      <td>0.025837</td>\n",
       "      <td>2.523899</td>\n",
       "      <td>2.095510</td>\n",
       "      <td>-0.011718</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.011681</td>\n",
       "      <td>-0.007442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.821759</td>\n",
       "      <td>5.280763</td>\n",
       "      <td>0.012349</td>\n",
       "      <td>0.906070</td>\n",
       "      <td>3.329600e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.997016e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-1.554356</td>\n",
       "      <td>-1.141241</td>\n",
       "      <td>2.066996</td>\n",
       "      <td>0.191681</td>\n",
       "      <td>-0.732644</td>\n",
       "      <td>0.585884</td>\n",
       "      <td>-0.012009</td>\n",
       "      <td>0.042208</td>\n",
       "      <td>-0.012006</td>\n",
       "      <td>-0.013884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.282872</td>\n",
       "      <td>5.285203</td>\n",
       "      <td>-0.062984</td>\n",
       "      <td>1.370913</td>\n",
       "      <td>4.515000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-1.551773</td>\n",
       "      <td>-1.472757</td>\n",
       "      <td>1.820266</td>\n",
       "      <td>-0.002964</td>\n",
       "      <td>-0.404230</td>\n",
       "      <td>-0.735953</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>0.042184</td>\n",
       "      <td>-0.012041</td>\n",
       "      <td>-0.004175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.801543</td>\n",
       "      <td>5.281014</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>0.946660</td>\n",
       "      <td>5.807000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347289</th>\n",
       "      <td>3.854896</td>\n",
       "      <td>-1.692997</td>\n",
       "      <td>1.480798</td>\n",
       "      <td>0.046164</td>\n",
       "      <td>-2.198134</td>\n",
       "      <td>-0.796348</td>\n",
       "      <td>-0.012041</td>\n",
       "      <td>0.042187</td>\n",
       "      <td>-0.012053</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>...</td>\n",
       "      <td>2.252906</td>\n",
       "      <td>5.273190</td>\n",
       "      <td>-0.150184</td>\n",
       "      <td>1.698309</td>\n",
       "      <td>4.034400e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347290</th>\n",
       "      <td>3.887383</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>1.584645</td>\n",
       "      <td>0.020289</td>\n",
       "      <td>-1.000228</td>\n",
       "      <td>-0.487211</td>\n",
       "      <td>-0.011995</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011987</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>...</td>\n",
       "      <td>1.465676</td>\n",
       "      <td>5.278460</td>\n",
       "      <td>-0.057058</td>\n",
       "      <td>1.839832</td>\n",
       "      <td>4.434000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.946324e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347291</th>\n",
       "      <td>3.892479</td>\n",
       "      <td>-0.065853</td>\n",
       "      <td>2.061924</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>-0.292694</td>\n",
       "      <td>0.887248</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.042176</td>\n",
       "      <td>-0.012041</td>\n",
       "      <td>-0.006805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.817587</td>\n",
       "      <td>5.284071</td>\n",
       "      <td>0.172147</td>\n",
       "      <td>2.218086</td>\n",
       "      <td>1.621740e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.464706e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347292</th>\n",
       "      <td>3.902444</td>\n",
       "      <td>-1.064211</td>\n",
       "      <td>1.677861</td>\n",
       "      <td>0.003299</td>\n",
       "      <td>-1.331728</td>\n",
       "      <td>-0.797308</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>0.042182</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>0.012150</td>\n",
       "      <td>...</td>\n",
       "      <td>1.191764</td>\n",
       "      <td>5.280465</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>1.857218</td>\n",
       "      <td>1.027600e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347293</th>\n",
       "      <td>3.903558</td>\n",
       "      <td>-1.626939</td>\n",
       "      <td>1.651184</td>\n",
       "      <td>0.043389</td>\n",
       "      <td>-0.376686</td>\n",
       "      <td>-0.132686</td>\n",
       "      <td>-0.012047</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>-0.012059</td>\n",
       "      <td>-0.006949</td>\n",
       "      <td>...</td>\n",
       "      <td>1.963822</td>\n",
       "      <td>5.270998</td>\n",
       "      <td>-0.039089</td>\n",
       "      <td>0.359104</td>\n",
       "      <td>1.164800e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.609681e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347294</th>\n",
       "      <td>3.952121</td>\n",
       "      <td>1.299437</td>\n",
       "      <td>1.937808</td>\n",
       "      <td>0.027862</td>\n",
       "      <td>1.435039</td>\n",
       "      <td>2.141457</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.012031</td>\n",
       "      <td>-0.011890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.673755</td>\n",
       "      <td>5.275138</td>\n",
       "      <td>-0.043785</td>\n",
       "      <td>1.302903</td>\n",
       "      <td>8.121799e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347295</th>\n",
       "      <td>4.034023</td>\n",
       "      <td>0.098159</td>\n",
       "      <td>1.722211</td>\n",
       "      <td>-0.002962</td>\n",
       "      <td>-0.002591</td>\n",
       "      <td>-0.868576</td>\n",
       "      <td>-0.011880</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011856</td>\n",
       "      <td>0.026800</td>\n",
       "      <td>...</td>\n",
       "      <td>2.033746</td>\n",
       "      <td>5.270757</td>\n",
       "      <td>-0.173174</td>\n",
       "      <td>1.346038</td>\n",
       "      <td>3.985900e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347296</th>\n",
       "      <td>4.037626</td>\n",
       "      <td>0.818402</td>\n",
       "      <td>1.499229</td>\n",
       "      <td>0.009324</td>\n",
       "      <td>0.215447</td>\n",
       "      <td>-0.210065</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.042153</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>-0.034153</td>\n",
       "      <td>...</td>\n",
       "      <td>1.708497</td>\n",
       "      <td>5.271570</td>\n",
       "      <td>0.145039</td>\n",
       "      <td>1.766551</td>\n",
       "      <td>1.113881e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.960432e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347297</th>\n",
       "      <td>4.229581</td>\n",
       "      <td>1.300406</td>\n",
       "      <td>1.705838</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.944568</td>\n",
       "      <td>-0.055836</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>0.042164</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.020875</td>\n",
       "      <td>...</td>\n",
       "      <td>1.514197</td>\n",
       "      <td>5.275967</td>\n",
       "      <td>0.163549</td>\n",
       "      <td>2.183398</td>\n",
       "      <td>7.082510e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347298</th>\n",
       "      <td>4.432310</td>\n",
       "      <td>-0.510520</td>\n",
       "      <td>1.354849</td>\n",
       "      <td>0.015130</td>\n",
       "      <td>-0.724999</td>\n",
       "      <td>0.369340</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.042195</td>\n",
       "      <td>-0.012032</td>\n",
       "      <td>0.026033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760395</td>\n",
       "      <td>5.273506</td>\n",
       "      <td>-0.076322</td>\n",
       "      <td>1.854061</td>\n",
       "      <td>3.156900e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347299</th>\n",
       "      <td>4.489255</td>\n",
       "      <td>0.564778</td>\n",
       "      <td>1.635464</td>\n",
       "      <td>0.025745</td>\n",
       "      <td>-0.457566</td>\n",
       "      <td>0.508801</td>\n",
       "      <td>-0.011854</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.011825</td>\n",
       "      <td>-0.017161</td>\n",
       "      <td>...</td>\n",
       "      <td>1.412109</td>\n",
       "      <td>5.272670</td>\n",
       "      <td>0.110744</td>\n",
       "      <td>2.161471</td>\n",
       "      <td>4.122000e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347300</th>\n",
       "      <td>4.516870</td>\n",
       "      <td>1.312192</td>\n",
       "      <td>2.100788</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>1.441639</td>\n",
       "      <td>0.661746</td>\n",
       "      <td>-0.012019</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>-0.012012</td>\n",
       "      <td>-0.014997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859256</td>\n",
       "      <td>5.270451</td>\n",
       "      <td>-0.126251</td>\n",
       "      <td>2.235873</td>\n",
       "      <td>1.798806e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.977423e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347301</th>\n",
       "      <td>4.542540</td>\n",
       "      <td>1.298028</td>\n",
       "      <td>1.920674</td>\n",
       "      <td>0.012971</td>\n",
       "      <td>1.157315</td>\n",
       "      <td>0.068808</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>0.042184</td>\n",
       "      <td>-0.012036</td>\n",
       "      <td>-0.006348</td>\n",
       "      <td>...</td>\n",
       "      <td>1.929028</td>\n",
       "      <td>5.275460</td>\n",
       "      <td>-0.017815</td>\n",
       "      <td>1.964402</td>\n",
       "      <td>8.592804e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347302</th>\n",
       "      <td>4.598580</td>\n",
       "      <td>1.247436</td>\n",
       "      <td>1.782253</td>\n",
       "      <td>0.010198</td>\n",
       "      <td>1.954320</td>\n",
       "      <td>0.381041</td>\n",
       "      <td>-0.011935</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.011918</td>\n",
       "      <td>-0.015006</td>\n",
       "      <td>...</td>\n",
       "      <td>1.745383</td>\n",
       "      <td>5.275244</td>\n",
       "      <td>0.013244</td>\n",
       "      <td>1.851924</td>\n",
       "      <td>1.017070e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347303</th>\n",
       "      <td>4.670085</td>\n",
       "      <td>0.955770</td>\n",
       "      <td>1.571868</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.085037</td>\n",
       "      <td>0.696914</td>\n",
       "      <td>-0.011995</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>0.011673</td>\n",
       "      <td>...</td>\n",
       "      <td>1.823107</td>\n",
       "      <td>5.279059</td>\n",
       "      <td>0.058655</td>\n",
       "      <td>2.169301</td>\n",
       "      <td>6.610593e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.997661e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347304</th>\n",
       "      <td>4.671551</td>\n",
       "      <td>-1.677279</td>\n",
       "      <td>1.777513</td>\n",
       "      <td>0.014126</td>\n",
       "      <td>-1.745042</td>\n",
       "      <td>-1.086039</td>\n",
       "      <td>-0.012018</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.012014</td>\n",
       "      <td>-0.000708</td>\n",
       "      <td>...</td>\n",
       "      <td>2.067389</td>\n",
       "      <td>5.274599</td>\n",
       "      <td>-0.145649</td>\n",
       "      <td>2.385571</td>\n",
       "      <td>1.103195e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347305</th>\n",
       "      <td>4.731845</td>\n",
       "      <td>-0.942247</td>\n",
       "      <td>1.973351</td>\n",
       "      <td>0.011722</td>\n",
       "      <td>-1.738874</td>\n",
       "      <td>0.286266</td>\n",
       "      <td>-0.012025</td>\n",
       "      <td>0.042192</td>\n",
       "      <td>-0.012025</td>\n",
       "      <td>-0.013526</td>\n",
       "      <td>...</td>\n",
       "      <td>1.762794</td>\n",
       "      <td>5.277947</td>\n",
       "      <td>-0.007453</td>\n",
       "      <td>2.086886</td>\n",
       "      <td>3.203218e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.874876e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347306</th>\n",
       "      <td>4.901173</td>\n",
       "      <td>-0.933148</td>\n",
       "      <td>1.723408</td>\n",
       "      <td>0.010480</td>\n",
       "      <td>-1.524683</td>\n",
       "      <td>-0.357989</td>\n",
       "      <td>-0.012035</td>\n",
       "      <td>0.042213</td>\n",
       "      <td>-0.012043</td>\n",
       "      <td>0.008260</td>\n",
       "      <td>...</td>\n",
       "      <td>1.995012</td>\n",
       "      <td>5.272712</td>\n",
       "      <td>-0.053025</td>\n",
       "      <td>2.015685</td>\n",
       "      <td>2.536000e+03</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347307</th>\n",
       "      <td>4.980108</td>\n",
       "      <td>1.045975</td>\n",
       "      <td>1.409241</td>\n",
       "      <td>0.009201</td>\n",
       "      <td>0.595034</td>\n",
       "      <td>-0.236573</td>\n",
       "      <td>-0.011982</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>-0.011978</td>\n",
       "      <td>0.006874</td>\n",
       "      <td>...</td>\n",
       "      <td>1.711573</td>\n",
       "      <td>5.271878</td>\n",
       "      <td>-0.119189</td>\n",
       "      <td>2.222986</td>\n",
       "      <td>1.458822e+08</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347308</th>\n",
       "      <td>5.101314</td>\n",
       "      <td>-1.286053</td>\n",
       "      <td>2.095083</td>\n",
       "      <td>-0.004993</td>\n",
       "      <td>-0.773370</td>\n",
       "      <td>-1.085315</td>\n",
       "      <td>-0.012028</td>\n",
       "      <td>0.042215</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>0.011983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.887533</td>\n",
       "      <td>5.282514</td>\n",
       "      <td>-0.046848</td>\n",
       "      <td>2.366267</td>\n",
       "      <td>1.206200e+04</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347309</th>\n",
       "      <td>5.130153</td>\n",
       "      <td>1.279023</td>\n",
       "      <td>1.660115</td>\n",
       "      <td>-0.015142</td>\n",
       "      <td>1.094487</td>\n",
       "      <td>-1.196779</td>\n",
       "      <td>-0.011906</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.011880</td>\n",
       "      <td>0.023192</td>\n",
       "      <td>...</td>\n",
       "      <td>1.531612</td>\n",
       "      <td>5.271203</td>\n",
       "      <td>0.015756</td>\n",
       "      <td>1.983706</td>\n",
       "      <td>1.035064e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.732486e-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347310</th>\n",
       "      <td>5.159062</td>\n",
       "      <td>0.987594</td>\n",
       "      <td>1.158321</td>\n",
       "      <td>-0.013144</td>\n",
       "      <td>-0.399648</td>\n",
       "      <td>-1.226735</td>\n",
       "      <td>-0.011678</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.011639</td>\n",
       "      <td>-0.009933</td>\n",
       "      <td>...</td>\n",
       "      <td>2.444736</td>\n",
       "      <td>5.272225</td>\n",
       "      <td>0.195686</td>\n",
       "      <td>2.155930</td>\n",
       "      <td>1.021215e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.291208e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347311</th>\n",
       "      <td>5.219036</td>\n",
       "      <td>1.291102</td>\n",
       "      <td>1.607623</td>\n",
       "      <td>0.022731</td>\n",
       "      <td>1.780569</td>\n",
       "      <td>-0.111562</td>\n",
       "      <td>-0.012004</td>\n",
       "      <td>0.042189</td>\n",
       "      <td>-0.012003</td>\n",
       "      <td>-0.012375</td>\n",
       "      <td>...</td>\n",
       "      <td>2.057160</td>\n",
       "      <td>5.271173</td>\n",
       "      <td>0.106284</td>\n",
       "      <td>2.203803</td>\n",
       "      <td>3.341791e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.691619e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347312</th>\n",
       "      <td>5.363985</td>\n",
       "      <td>1.277278</td>\n",
       "      <td>1.996141</td>\n",
       "      <td>0.010979</td>\n",
       "      <td>0.945110</td>\n",
       "      <td>-0.674692</td>\n",
       "      <td>-0.012009</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>-0.012005</td>\n",
       "      <td>-0.023130</td>\n",
       "      <td>...</td>\n",
       "      <td>2.552730</td>\n",
       "      <td>5.271055</td>\n",
       "      <td>0.130997</td>\n",
       "      <td>1.666330</td>\n",
       "      <td>4.680117e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.968556e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347313</th>\n",
       "      <td>5.683409</td>\n",
       "      <td>0.490914</td>\n",
       "      <td>2.215300</td>\n",
       "      <td>0.013999</td>\n",
       "      <td>0.517185</td>\n",
       "      <td>2.104769</td>\n",
       "      <td>-0.011993</td>\n",
       "      <td>0.042197</td>\n",
       "      <td>-0.011992</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>...</td>\n",
       "      <td>2.106366</td>\n",
       "      <td>5.277033</td>\n",
       "      <td>0.146947</td>\n",
       "      <td>2.015297</td>\n",
       "      <td>7.326258e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.592745e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347314</th>\n",
       "      <td>6.102160</td>\n",
       "      <td>1.218031</td>\n",
       "      <td>1.912647</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.429370</td>\n",
       "      <td>-0.298835</td>\n",
       "      <td>-0.011745</td>\n",
       "      <td>0.042218</td>\n",
       "      <td>-0.011708</td>\n",
       "      <td>-0.013340</td>\n",
       "      <td>...</td>\n",
       "      <td>1.751379</td>\n",
       "      <td>5.272793</td>\n",
       "      <td>-0.029386</td>\n",
       "      <td>2.369718</td>\n",
       "      <td>3.218419e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.560661e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347315</th>\n",
       "      <td>6.294287</td>\n",
       "      <td>0.740536</td>\n",
       "      <td>1.750744</td>\n",
       "      <td>0.006878</td>\n",
       "      <td>0.160956</td>\n",
       "      <td>-0.330470</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.042109</td>\n",
       "      <td>-0.012056</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>...</td>\n",
       "      <td>1.103246</td>\n",
       "      <td>5.270519</td>\n",
       "      <td>-0.154954</td>\n",
       "      <td>2.014074</td>\n",
       "      <td>4.139668e+07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347316</th>\n",
       "      <td>6.624885</td>\n",
       "      <td>0.947272</td>\n",
       "      <td>1.964411</td>\n",
       "      <td>-6.669213</td>\n",
       "      <td>1.857085</td>\n",
       "      <td>-4.677280</td>\n",
       "      <td>-0.012114</td>\n",
       "      <td>-23.701450</td>\n",
       "      <td>-0.012080</td>\n",
       "      <td>-0.008578</td>\n",
       "      <td>...</td>\n",
       "      <td>1.602102</td>\n",
       "      <td>5.274765</td>\n",
       "      <td>-0.132008</td>\n",
       "      <td>1.773431</td>\n",
       "      <td>1.379795e+08</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.916703e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347317</th>\n",
       "      <td>6.685095</td>\n",
       "      <td>1.052954</td>\n",
       "      <td>1.716926</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.231415</td>\n",
       "      <td>0.685805</td>\n",
       "      <td>-0.012023</td>\n",
       "      <td>0.042052</td>\n",
       "      <td>-0.012045</td>\n",
       "      <td>-0.006985</td>\n",
       "      <td>...</td>\n",
       "      <td>2.606178</td>\n",
       "      <td>5.270617</td>\n",
       "      <td>-0.147710</td>\n",
       "      <td>2.399954</td>\n",
       "      <td>8.194058e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347318</th>\n",
       "      <td>7.848846</td>\n",
       "      <td>0.807176</td>\n",
       "      <td>2.093281</td>\n",
       "      <td>0.021128</td>\n",
       "      <td>0.114179</td>\n",
       "      <td>-1.186913</td>\n",
       "      <td>-0.012028</td>\n",
       "      <td>0.042202</td>\n",
       "      <td>-0.012034</td>\n",
       "      <td>-0.041588</td>\n",
       "      <td>...</td>\n",
       "      <td>2.154728</td>\n",
       "      <td>5.270906</td>\n",
       "      <td>0.026964</td>\n",
       "      <td>2.341309</td>\n",
       "      <td>1.090487e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>347319 rows  108 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        BB_gamma_hel  B_CosTBTO  B_CosTBz  B_DeltaT      B_R2  B_TagVPvalue  \\\n",
       "0          -1.588105  -0.566125  2.060708  0.006763 -1.170962      1.273394   \n",
       "1          -1.583511   0.326212  1.965546  0.000565 -0.139086     -0.987356   \n",
       "2          -1.582909  -1.306332  2.205411  0.021708 -1.382867     -0.158151   \n",
       "3          -1.581905  -0.050490  1.774217 -0.000200  0.090453     -1.093808   \n",
       "4          -1.575761  -0.710063  1.977314  0.008272 -1.013066      0.622739   \n",
       "5          -1.574043   0.709838  1.887089  0.022262 -0.304458      0.978461   \n",
       "6          -1.573475  -0.185380  2.025063  0.011435 -0.784249      1.865040   \n",
       "7          -1.569883  -1.009027  2.177724  0.011549 -0.807890      1.904731   \n",
       "8          -1.569604  -0.014816  2.004494  0.047200 -0.350407     -0.201612   \n",
       "9          -1.569481  -1.954512  2.171281 -0.006076 -1.450575     -1.119552   \n",
       "10         -1.569182   1.151153  1.900612  0.012091  0.203661     -1.112279   \n",
       "11         -1.568435  -0.565598  1.992011  0.028741  0.130616     -0.521075   \n",
       "12         -1.567715   1.239812  1.800736  0.015411  1.816450      0.289027   \n",
       "13         -1.567501  -0.476724  1.985347  0.010538 -2.046603      0.424110   \n",
       "14         -1.566319   0.031546  1.854112  0.044732 -1.219332     -0.360032   \n",
       "15         -1.564832  -1.970770  2.149373  0.027442  0.407642     -0.513775   \n",
       "16         -1.563681  -0.660002  1.922876  0.001845 -0.536715     -0.790237   \n",
       "17         -1.563292   0.557182  2.007596 -0.016845  0.846171     -0.835290   \n",
       "18         -1.562395  -1.927134  2.147949  0.027548  0.407642     -0.513775   \n",
       "19         -1.561559  -1.420410  1.757970  0.003209 -0.599403      0.007394   \n",
       "20         -1.560382   0.954287  1.845459  0.010007  1.672864      1.046969   \n",
       "21         -1.560270   0.883145  2.025181  0.011548  0.610251     -0.459426   \n",
       "22         -1.560108   1.292310  1.941228  0.022710  1.175870     -0.532663   \n",
       "23         -1.556034  -0.581448  1.814682  0.011301  0.705363      1.948656   \n",
       "24         -1.556008  -1.944766  1.977108  0.005731 -1.118724     -1.038394   \n",
       "25         -1.555477   0.947163  1.692409  0.003611  0.469500     -0.688393   \n",
       "26         -1.554658   1.301108  2.126952  0.007212  0.142358      1.879190   \n",
       "27         -1.554489   0.541929  2.085999  0.025837  2.523899      2.095510   \n",
       "28         -1.554356  -1.141241  2.066996  0.191681 -0.732644      0.585884   \n",
       "29         -1.551773  -1.472757  1.820266 -0.002964 -0.404230     -0.735953   \n",
       "...              ...        ...       ...       ...       ...           ...   \n",
       "347289      3.854896  -1.692997  1.480798  0.046164 -2.198134     -0.796348   \n",
       "347290      3.887383   0.105915  1.584645  0.020289 -1.000228     -0.487211   \n",
       "347291      3.892479  -0.065853  2.061924  0.010526 -0.292694      0.887248   \n",
       "347292      3.902444  -1.064211  1.677861  0.003299 -1.331728     -0.797308   \n",
       "347293      3.903558  -1.626939  1.651184  0.043389 -0.376686     -0.132686   \n",
       "347294      3.952121   1.299437  1.937808  0.027862  1.435039      2.141457   \n",
       "347295      4.034023   0.098159  1.722211 -0.002962 -0.002591     -0.868576   \n",
       "347296      4.037626   0.818402  1.499229  0.009324  0.215447     -0.210065   \n",
       "347297      4.229581   1.300406  1.705838  0.010989  0.944568     -0.055836   \n",
       "347298      4.432310  -0.510520  1.354849  0.015130 -0.724999      0.369340   \n",
       "347299      4.489255   0.564778  1.635464  0.025745 -0.457566      0.508801   \n",
       "347300      4.516870   1.312192  2.100788  0.007455  1.441639      0.661746   \n",
       "347301      4.542540   1.298028  1.920674  0.012971  1.157315      0.068808   \n",
       "347302      4.598580   1.247436  1.782253  0.010198  1.954320      0.381041   \n",
       "347303      4.670085   0.955770  1.571868  0.003274 -0.085037      0.696914   \n",
       "347304      4.671551  -1.677279  1.777513  0.014126 -1.745042     -1.086039   \n",
       "347305      4.731845  -0.942247  1.973351  0.011722 -1.738874      0.286266   \n",
       "347306      4.901173  -0.933148  1.723408  0.010480 -1.524683     -0.357989   \n",
       "347307      4.980108   1.045975  1.409241  0.009201  0.595034     -0.236573   \n",
       "347308      5.101314  -1.286053  2.095083 -0.004993 -0.773370     -1.085315   \n",
       "347309      5.130153   1.279023  1.660115 -0.015142  1.094487     -1.196779   \n",
       "347310      5.159062   0.987594  1.158321 -0.013144 -0.399648     -1.226735   \n",
       "347311      5.219036   1.291102  1.607623  0.022731  1.780569     -0.111562   \n",
       "347312      5.363985   1.277278  1.996141  0.010979  0.945110     -0.674692   \n",
       "347313      5.683409   0.490914  2.215300  0.013999  0.517185      2.104769   \n",
       "347314      6.102160   1.218031  1.912647  0.009165  0.429370     -0.298835   \n",
       "347315      6.294287   0.740536  1.750744  0.006878  0.160956     -0.330470   \n",
       "347316      6.624885   0.947272  1.964411 -6.669213  1.857085     -4.677280   \n",
       "347317      6.685095   1.052954  1.716926  0.011800  0.231415      0.685805   \n",
       "347318      7.848846   0.807176  2.093281  0.021128  0.114179     -1.186913   \n",
       "\n",
       "        B_TagVex   B_TagVey  B_TagVez   B_TagVx      ...       \\\n",
       "0      -0.012045   0.042164 -0.012057 -0.005894      ...        \n",
       "1      -0.012026   0.042042 -0.012054  0.023391      ...        \n",
       "2      -0.011995   0.042215 -0.011983 -0.031475      ...        \n",
       "3      -0.011944   0.042215 -0.011924 -0.002600      ...        \n",
       "4      -0.012038   0.042200 -0.012050 -0.000999      ...        \n",
       "5      -0.012024   0.042213 -0.012022 -0.001381      ...        \n",
       "6      -0.012011   0.042205 -0.012009  0.003061      ...        \n",
       "7      -0.012035   0.042192 -0.012047 -0.019559      ...        \n",
       "8      -0.012022   0.042195 -0.012024 -0.017455      ...        \n",
       "9      -0.012044   0.042205 -0.012056 -0.001675      ...        \n",
       "10     -0.012012   0.042202 -0.012007  0.003261      ...        \n",
       "11     -0.012043   0.042117 -0.012053 -0.004857      ...        \n",
       "12     -0.012019   0.042169 -0.012031  0.000831      ...        \n",
       "13     -0.012028   0.042176 -0.012036 -0.003214      ...        \n",
       "14     -0.012039   0.042200 -0.012048  0.032568      ...        \n",
       "15     -0.012014   0.042215 -0.012010 -0.032058      ...        \n",
       "16     -0.012004   0.042208 -0.011999  0.000494      ...        \n",
       "17     -0.012024   0.042202 -0.012029  0.027877      ...        \n",
       "18     -0.012014   0.042215 -0.012010 -0.032058      ...        \n",
       "19     -0.012037   0.042138 -0.012042  0.012916      ...        \n",
       "20     -0.012023   0.042210 -0.012027  0.001565      ...        \n",
       "21     -0.012037   0.042189 -0.012046 -0.007102      ...        \n",
       "22     -0.012003   0.042213 -0.011994 -0.008904      ...        \n",
       "23     -0.012019   0.042215 -0.012017 -0.011938      ...        \n",
       "24     -0.012031   0.042210 -0.012036  0.029229      ...        \n",
       "25     -0.011810   0.042215 -0.011778 -0.004983      ...        \n",
       "26     -0.012004   0.042215 -0.011998 -0.007538      ...        \n",
       "27     -0.011718   0.042218 -0.011681 -0.007442      ...        \n",
       "28     -0.012009   0.042208 -0.012006 -0.013884      ...        \n",
       "29     -0.012035   0.042184 -0.012041 -0.004175      ...        \n",
       "...          ...        ...       ...       ...      ...        \n",
       "347289 -0.012041   0.042187 -0.012053  0.002444      ...        \n",
       "347290 -0.011995   0.042215 -0.011987  0.004836      ...        \n",
       "347291 -0.012030   0.042176 -0.012041 -0.006805      ...        \n",
       "347292 -0.012034   0.042182 -0.012043  0.012150      ...        \n",
       "347293 -0.012047   0.042213 -0.012059 -0.006949      ...        \n",
       "347294 -0.012023   0.042200 -0.012031 -0.011890      ...        \n",
       "347295 -0.011880   0.042215 -0.011856  0.026800      ...        \n",
       "347296 -0.012030   0.042153 -0.012043 -0.034153      ...        \n",
       "347297 -0.012023   0.042164 -0.012034 -0.020875      ...        \n",
       "347298 -0.012030   0.042195 -0.012032  0.026033      ...        \n",
       "347299 -0.011854   0.042218 -0.011825 -0.017161      ...        \n",
       "347300 -0.012019   0.042202 -0.012012 -0.014997      ...        \n",
       "347301 -0.012034   0.042184 -0.012036 -0.006348      ...        \n",
       "347302 -0.011935   0.042215 -0.011918 -0.015006      ...        \n",
       "347303 -0.011995   0.042200 -0.011993  0.011673      ...        \n",
       "347304 -0.012018   0.042215 -0.012014 -0.000708      ...        \n",
       "347305 -0.012025   0.042192 -0.012025 -0.013526      ...        \n",
       "347306 -0.012035   0.042213 -0.012043  0.008260      ...        \n",
       "347307 -0.011982   0.042197 -0.011978  0.006874      ...        \n",
       "347308 -0.012028   0.042215 -0.012030  0.011983      ...        \n",
       "347309 -0.011906   0.042218 -0.011880  0.023192      ...        \n",
       "347310 -0.011678   0.042218 -0.011639 -0.009933      ...        \n",
       "347311 -0.012004   0.042189 -0.012003 -0.012375      ...        \n",
       "347312 -0.012009   0.042200 -0.012005 -0.023130      ...        \n",
       "347313 -0.011993   0.042197 -0.011992  0.002635      ...        \n",
       "347314 -0.011745   0.042218 -0.011708 -0.013340      ...        \n",
       "347315 -0.012037   0.042109 -0.012056 -0.000693      ...        \n",
       "347316 -0.012114 -23.701450 -0.012080 -0.008578      ...        \n",
       "347317 -0.012023   0.042052 -0.012045 -0.006985      ...        \n",
       "347318 -0.012028   0.042202 -0.012034 -0.041588      ...        \n",
       "\n",
       "        B_useCMSFrame_bodaughterSumOf_bopz_bc_bc       mbc    deltae  \\\n",
       "0                                       0.024435  5.277649 -0.002986   \n",
       "1                                      -0.396096  5.280044  0.024862   \n",
       "2                                       1.046946  5.279857 -0.106811   \n",
       "3                                      -1.155154  5.272421 -0.122822   \n",
       "4                                      -0.647386  5.281581  0.055017   \n",
       "5                                      -0.462639  5.273503 -0.166270   \n",
       "6                                      -0.210416  5.275576  0.008971   \n",
       "7                                       0.878339  5.280567  0.179423   \n",
       "8                                      -0.578034  5.274578 -0.108732   \n",
       "9                                       0.627990  5.277041  0.085580   \n",
       "10                                     -1.322810  5.282043 -0.135296   \n",
       "11                                      0.929582  5.274243 -0.003875   \n",
       "12                                      0.155492  5.284900 -0.069414   \n",
       "13                                     -0.408360  5.272332 -0.029731   \n",
       "14                                     -0.457383  5.275331  0.013524   \n",
       "15                                     -1.099204  5.284764  0.156094   \n",
       "16                                     -0.295934  5.285688  0.105284   \n",
       "17                                      1.331109  5.273226 -0.052433   \n",
       "18                                     -1.245328  5.283813  0.170637   \n",
       "19                                     -1.559707  5.280505 -0.167871   \n",
       "20                                     -1.808633  5.274035 -0.003709   \n",
       "21                                      0.609037  5.280972  0.126612   \n",
       "22                                     -2.305296  5.275794  0.120669   \n",
       "23                                     -0.360283  5.279708  0.073355   \n",
       "24                                      0.001654  5.280077 -0.038192   \n",
       "25                                      1.511616  5.270384 -0.094270   \n",
       "26                                     -1.513486  5.272954 -0.186648   \n",
       "27                                     -0.821759  5.280763  0.012349   \n",
       "28                                     -0.282872  5.285203 -0.062984   \n",
       "29                                     -0.801543  5.281014 -0.000976   \n",
       "...                                          ...       ...       ...   \n",
       "347289                                  2.252906  5.273190 -0.150184   \n",
       "347290                                  1.465676  5.278460 -0.057058   \n",
       "347291                                  0.817587  5.284071  0.172147   \n",
       "347292                                  1.191764  5.280465  0.003451   \n",
       "347293                                  1.963822  5.270998 -0.039089   \n",
       "347294                                  1.673755  5.275138 -0.043785   \n",
       "347295                                  2.033746  5.270757 -0.173174   \n",
       "347296                                  1.708497  5.271570  0.145039   \n",
       "347297                                  1.514197  5.275967  0.163549   \n",
       "347298                                  0.760395  5.273506 -0.076322   \n",
       "347299                                  1.412109  5.272670  0.110744   \n",
       "347300                                  0.859256  5.270451 -0.126251   \n",
       "347301                                  1.929028  5.275460 -0.017815   \n",
       "347302                                  1.745383  5.275244  0.013244   \n",
       "347303                                  1.823107  5.279059  0.058655   \n",
       "347304                                  2.067389  5.274599 -0.145649   \n",
       "347305                                  1.762794  5.277947 -0.007453   \n",
       "347306                                  1.995012  5.272712 -0.053025   \n",
       "347307                                  1.711573  5.271878 -0.119189   \n",
       "347308                                  0.887533  5.282514 -0.046848   \n",
       "347309                                  1.531612  5.271203  0.015756   \n",
       "347310                                  2.444736  5.272225  0.195686   \n",
       "347311                                  2.057160  5.271173  0.106284   \n",
       "347312                                  2.552730  5.271055  0.130997   \n",
       "347313                                  2.106366  5.277033  0.146947   \n",
       "347314                                  1.751379  5.272793 -0.029386   \n",
       "347315                                  1.103246  5.270519 -0.154954   \n",
       "347316                                  1.602102  5.274765 -0.132008   \n",
       "347317                                  2.606178  5.270617 -0.147710   \n",
       "347318                                  2.154728  5.270906  0.026964   \n",
       "\n",
       "        daughterInvM        evtNum  MCtype  nCands  channel  labels  \\\n",
       "0           1.657435  3.249500e+04     7.0     1.0      0.0     1.0   \n",
       "1           1.672550  4.530700e+04     7.0     1.0      0.0     1.0   \n",
       "2           2.220620  1.386191e+08     0.0     1.0     11.0     0.0   \n",
       "3           2.332373  2.621646e+08     2.0     2.0      7.0     0.0   \n",
       "4           2.082395  7.297000e+03     7.0     1.0      6.0     1.0   \n",
       "5           2.049185  3.964600e+04     7.0     1.0      1.0     1.0   \n",
       "6           1.822210  1.213567e+09     0.0     2.0     11.0     0.0   \n",
       "7           1.994851  2.481396e+08     0.0     2.0      5.0     0.0   \n",
       "8           0.953385  6.900000e+03     7.0     1.0      6.0     1.0   \n",
       "9           1.227704  3.599200e+04     7.0     1.0      0.0     1.0   \n",
       "10          1.479965  9.190214e+08     2.0     2.0      7.0     0.0   \n",
       "11          1.013377  3.161500e+04     7.0     1.0      0.0     1.0   \n",
       "12          1.761511  4.859773e+07     3.0     2.0      2.0     0.0   \n",
       "13          2.272628  3.696101e+07     1.0     1.0      7.0     0.0   \n",
       "14          1.213096  8.118000e+03     7.0     1.0      0.0     1.0   \n",
       "15          0.959191  1.128291e+08     0.0     2.0      0.0     0.0   \n",
       "16          1.741828  2.034600e+04     7.0     1.0      6.0     1.0   \n",
       "17          2.030820  1.680045e+08     2.0     1.0      3.0     0.0   \n",
       "18          0.922816  1.128291e+08     0.0     2.0      0.0     0.0   \n",
       "19          1.622055  2.512200e+04     7.0     1.0      7.0     1.0   \n",
       "20          0.851073  4.347667e+08     0.0     2.0      6.0     0.0   \n",
       "21          2.361797  9.870547e+07     1.0     2.0      6.0     0.0   \n",
       "22          2.280151  2.564789e+08     0.0     2.0      7.0     0.0   \n",
       "23          1.398857  3.519000e+03     7.0     1.0      6.0     1.0   \n",
       "24          1.571976  2.417300e+04     7.0     1.0      6.0     1.0   \n",
       "25          2.044542  2.665656e+08     2.0     1.0      2.0     0.0   \n",
       "26          2.162750  2.496540e+08     0.0     1.0      8.0     0.0   \n",
       "27          0.906070  3.329600e+04     7.0     1.0      7.0     1.0   \n",
       "28          1.370913  4.515000e+03     7.0     2.0      6.0     1.0   \n",
       "29          0.946660  5.807000e+03     7.0     1.0      1.0     1.0   \n",
       "...              ...           ...     ...     ...      ...     ...   \n",
       "347289      1.698309  4.034400e+04     7.0     1.0      1.0     1.0   \n",
       "347290      1.839832  4.434000e+03     7.0     1.0      6.0     1.0   \n",
       "347291      2.218086  1.621740e+08     1.0     2.0      3.0     0.0   \n",
       "347292      1.857218  1.027600e+04     7.0     1.0      0.0     1.0   \n",
       "347293      0.359104  1.164800e+04     7.0     1.0      6.0     1.0   \n",
       "347294      1.302903  8.121799e+08     0.0     1.0      1.0     0.0   \n",
       "347295      1.346038  3.985900e+04     7.0     1.0      6.0     1.0   \n",
       "347296      1.766551  1.113881e+08     0.0     1.0      1.0     0.0   \n",
       "347297      2.183398  7.082510e+07     0.0     2.0      3.0     0.0   \n",
       "347298      1.854061  3.156900e+04     7.0     1.0      6.0     1.0   \n",
       "347299      2.161471  4.122000e+08     0.0     2.0     11.0     0.0   \n",
       "347300      2.235873  1.798806e+08     0.0     2.0      2.0     0.0   \n",
       "347301      1.964402  8.592804e+08     0.0     2.0      7.0     0.0   \n",
       "347302      1.851924  1.017070e+08     1.0     2.0      7.0     0.0   \n",
       "347303      2.169301  6.610593e+08     0.0     2.0      2.0     0.0   \n",
       "347304      2.385571  1.103195e+08     1.0     2.0      7.0     0.0   \n",
       "347305      2.086886  3.203218e+08     1.0     2.0      3.0     0.0   \n",
       "347306      2.015685  2.536000e+03     7.0     1.0      0.0     1.0   \n",
       "347307      2.222986  1.458822e+08     3.0     2.0      8.0     0.0   \n",
       "347308      2.366267  1.206200e+04     7.0     1.0      0.0     1.0   \n",
       "347309      1.983706  1.035064e+09     2.0     2.0     10.0     0.0   \n",
       "347310      2.155930  1.021215e+09     0.0     1.0      1.0     0.0   \n",
       "347311      2.203803  3.341791e+08     0.0     2.0      7.0     0.0   \n",
       "347312      1.666330  4.680117e+08     0.0     2.0      5.0     0.0   \n",
       "347313      2.015297  7.326258e+07     1.0     2.0      2.0     0.0   \n",
       "347314      2.369718  3.218419e+08     0.0     1.0      1.0     0.0   \n",
       "347315      2.014074  4.139668e+07     3.0     2.0      2.0     0.0   \n",
       "347316      1.773431  1.379795e+08     1.0     1.0      6.0     0.0   \n",
       "347317      2.399954  8.194058e+07     0.0     2.0      2.0     0.0   \n",
       "347318      2.341309  1.090487e+09     0.0     2.0      2.0     0.0   \n",
       "\n",
       "               probs  \n",
       "0       1.000000e+00  \n",
       "1       1.000000e+00  \n",
       "2       9.827306e-01  \n",
       "3       1.000000e+00  \n",
       "4       1.000000e+00  \n",
       "5       1.000000e+00  \n",
       "6       1.000000e+00  \n",
       "7       1.000000e+00  \n",
       "8       1.065946e-10  \n",
       "9       9.526831e-01  \n",
       "10      9.994755e-01  \n",
       "11      1.000000e+00  \n",
       "12      0.000000e+00  \n",
       "13      1.275826e-33  \n",
       "14      9.961402e-01  \n",
       "15      9.999176e-01  \n",
       "16      9.068767e-01  \n",
       "17      3.129100e-03  \n",
       "18      1.000000e+00  \n",
       "19      9.999944e-01  \n",
       "20      1.000000e+00  \n",
       "21      1.000000e+00  \n",
       "22      1.000000e+00  \n",
       "23      1.000000e+00  \n",
       "24      3.955720e-33  \n",
       "25      9.902784e-01  \n",
       "26      1.000000e+00  \n",
       "27      9.997016e-01  \n",
       "28      1.000000e+00  \n",
       "29      0.000000e+00  \n",
       "...              ...  \n",
       "347289  1.000000e+00  \n",
       "347290  9.946324e-01  \n",
       "347291  8.464706e-17  \n",
       "347292  1.000000e+00  \n",
       "347293  2.609681e-16  \n",
       "347294  1.000000e+00  \n",
       "347295  1.000000e+00  \n",
       "347296  9.960432e-01  \n",
       "347297  1.000000e+00  \n",
       "347298  0.000000e+00  \n",
       "347299  1.000000e+00  \n",
       "347300  9.977423e-01  \n",
       "347301  1.000000e+00  \n",
       "347302  1.000000e+00  \n",
       "347303  9.997661e-01  \n",
       "347304  1.000000e+00  \n",
       "347305  3.874876e-18  \n",
       "347306  0.000000e+00  \n",
       "347307  1.000000e+00  \n",
       "347308  0.000000e+00  \n",
       "347309  5.732486e-26  \n",
       "347310  9.291208e-01  \n",
       "347311  2.691619e-23  \n",
       "347312  9.968556e-01  \n",
       "347313  3.592745e-13  \n",
       "347314  8.560661e-10  \n",
       "347315  1.000000e+00  \n",
       "347316  9.916703e-01  \n",
       "347317  1.000000e+00  \n",
       "347318  0.000000e+00  \n",
       "\n",
       "[347319 rows x 108 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test.to_hdf(os.path.join(directories.checkpoints, '{}_preds.h5'.format(os.path.basename(directories.test))), key = 'df', format='t', data_columns=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
